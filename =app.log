2023-04-26 11:42:12,303 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] Starting SpringBootSecurityJwtMongodbApplication using Java 17.0.6 on wks-012 with PID 102147 (/home/inferyx/git/SpringApplicationWithSecurity/target/classes started by inferyx in /home/inferyx/git/SpringApplicationWithSecurity)
2023-04-26 11:42:12,312 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] No active profile set, falling back to 1 default profile: "default"
2023-04-26 11:42:18,449 INFO org.apache.catalina.core.StandardService [restartedMain] Starting service [Tomcat]
2023-04-26 11:42:18,450 INFO org.apache.catalina.core.StandardEngine [restartedMain] Starting Servlet engine: [Apache Tomcat/9.0.65]
2023-04-26 11:42:18,700 INFO org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] [restartedMain] Initializing Spring embedded WebApplicationContext
2023-04-26 11:42:19,040 INFO org.mongodb.driver.client [restartedMain] MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync|spring-boot", "version": "4.6.1"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "5.15.0-69-generic"}, "platform": "Java/Eclipse Adoptium/17.0.6+10"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.Jep395RecordCodecProvider@269fb271]}, clusterSettings={hosts=[localhost:27017], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='30000 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, sendBufferSize=0}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, sendBufferSize=0}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=JAVA_LEGACY, serverApi=null, autoEncryptionSettings=null, contextProvider=null}
2023-04-26 11:42:19,926 INFO org.mongodb.driver.connection [cluster-rtt-ClusterId{value='6448c0c2adee62176f29075a', description='null'}-localhost:27017] Opened connection [connectionId{localValue:2, serverValue:43}] to localhost:27017
2023-04-26 11:42:19,926 INFO org.mongodb.driver.connection [cluster-ClusterId{value='6448c0c2adee62176f29075a', description='null'}-localhost:27017] Opened connection [connectionId{localValue:1, serverValue:42}] to localhost:27017
2023-04-26 11:42:19,928 INFO org.mongodb.driver.cluster [cluster-ClusterId{value='6448c0c2adee62176f29075a', description='null'}-localhost:27017] Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=6, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=843594220}
2023-04-26 11:42:20,770 INFO org.hibernate.jpa.internal.util.LogHelper [restartedMain] HHH000204: Processing PersistenceUnitInfo [name: default]
2023-04-26 11:42:20,949 INFO org.hibernate.Version [restartedMain] HHH000412: Hibernate ORM core version 5.6.10.Final
2023-04-26 11:42:21,412 INFO org.hibernate.annotations.common.Version [restartedMain] HCANN000001: Hibernate Commons Annotations {5.1.2.Final}
2023-04-26 11:42:21,671 INFO com.zaxxer.hikari.HikariDataSource [restartedMain] HikariPool-1 - Starting...
2023-04-26 11:42:21,952 INFO com.zaxxer.hikari.HikariDataSource [restartedMain] HikariPool-1 - Start completed.
2023-04-26 11:42:22,020 INFO org.hibernate.dialect.Dialect [restartedMain] HHH000400: Using dialect: org.hibernate.dialect.MySQL5Dialect
2023-04-26 11:42:23,375 INFO org.hibernate.engine.transaction.jta.platform.internal.JtaPlatformInitiator [restartedMain] HHH000490: Using JtaPlatform implementation: [org.hibernate.engine.transaction.jta.platform.internal.NoJtaPlatform]
2023-04-26 11:42:25,539 WARN org.apache.spark.util.Utils [restartedMain] Your hostname, wks-012 resolves to a loopback address: 127.0.1.1; using 192.168.1.125 instead (on interface wlp6s0)
2023-04-26 11:42:25,541 WARN org.apache.spark.util.Utils [restartedMain] Set SPARK_LOCAL_IP if you need to bind to another address
2023-04-26 11:42:25,882 INFO org.apache.spark.SparkContext [restartedMain] Running Spark version 2.4.5
2023-04-26 11:42:26,614 WARN org.apache.hadoop.util.NativeCodeLoader [restartedMain] Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2023-04-26 11:42:27,413 INFO org.apache.spark.SparkContext [restartedMain] Submitted application: MyAppName
2023-04-26 11:42:28,014 INFO org.apache.spark.SecurityManager [restartedMain] Changing view acls to: inferyx
2023-04-26 11:42:28,016 INFO org.apache.spark.SecurityManager [restartedMain] Changing modify acls to: inferyx
2023-04-26 11:42:28,018 INFO org.apache.spark.SecurityManager [restartedMain] Changing view acls groups to: 
2023-04-26 11:42:28,020 INFO org.apache.spark.SecurityManager [restartedMain] Changing modify acls groups to: 
2023-04-26 11:42:28,021 INFO org.apache.spark.SecurityManager [restartedMain] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(inferyx); groups with view permissions: Set(); users  with modify permissions: Set(inferyx); groups with modify permissions: Set()
2023-04-26 11:42:29,269 INFO org.apache.spark.util.Utils [restartedMain] Successfully started service 'sparkDriver' on port 45037.
2023-04-26 11:42:29,467 INFO org.apache.spark.SparkEnv [restartedMain] Registering MapOutputTracker
2023-04-26 11:42:29,625 INFO org.apache.spark.SparkEnv [restartedMain] Registering BlockManagerMaster
2023-04-26 11:42:29,672 INFO org.apache.spark.storage.BlockManagerMasterEndpoint [restartedMain] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2023-04-26 11:42:29,673 INFO org.apache.spark.storage.BlockManagerMasterEndpoint [restartedMain] BlockManagerMasterEndpoint up
2023-04-26 11:42:29,726 INFO org.apache.spark.storage.DiskBlockManager [restartedMain] Created local directory at /tmp/blockmgr-96035a0c-5baa-485f-9981-d59e9c549a93
2023-04-26 11:42:29,817 INFO org.apache.spark.storage.memory.MemoryStore [restartedMain] MemoryStore started with capacity 998.4 MB
2023-04-26 11:42:29,900 INFO org.apache.spark.SparkEnv [restartedMain] Registering OutputCommitCoordinator
2023-04-26 11:42:30,132 INFO org.spark_project.jetty.util.log [restartedMain] Logging initialized @22364ms
2023-04-26 11:42:30,496 INFO org.spark_project.jetty.server.Server [restartedMain] jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2023-04-26 11:42:30,521 INFO org.spark_project.jetty.server.Server [restartedMain] Started @22754ms
2023-04-26 11:42:30,578 INFO org.spark_project.jetty.server.AbstractConnector [restartedMain] Started ServerConnector@2dfbf2f2{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2023-04-26 11:42:30,579 INFO org.apache.spark.util.Utils [restartedMain] Successfully started service 'SparkUI' on port 4040.
2023-04-26 11:42:30,620 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@1cc31945{/jobs,null,AVAILABLE,@Spark}
2023-04-26 11:42:30,622 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@3601df28{/jobs/json,null,AVAILABLE,@Spark}
2023-04-26 11:42:30,624 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@61ead456{/jobs/job,null,AVAILABLE,@Spark}
2023-04-26 11:42:30,628 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@38bfc05{/jobs/job/json,null,AVAILABLE,@Spark}
2023-04-26 11:42:30,631 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@34f4d046{/stages,null,AVAILABLE,@Spark}
2023-04-26 11:42:30,632 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@6b4ee677{/stages/json,null,AVAILABLE,@Spark}
2023-04-26 11:42:30,633 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@c840281{/stages/stage,null,AVAILABLE,@Spark}
2023-04-26 11:42:30,636 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@3f126d73{/stages/stage/json,null,AVAILABLE,@Spark}
2023-04-26 11:42:30,639 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@443d9e41{/stages/pool,null,AVAILABLE,@Spark}
2023-04-26 11:42:30,641 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@2abdfb56{/stages/pool/json,null,AVAILABLE,@Spark}
2023-04-26 11:42:30,643 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@c43cc35{/storage,null,AVAILABLE,@Spark}
2023-04-26 11:42:30,644 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@2096513b{/storage/json,null,AVAILABLE,@Spark}
2023-04-26 11:42:30,646 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@8f10cfe{/storage/rdd,null,AVAILABLE,@Spark}
2023-04-26 11:42:30,648 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@4d5331fd{/storage/rdd/json,null,AVAILABLE,@Spark}
2023-04-26 11:42:30,650 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@5fe37e06{/environment,null,AVAILABLE,@Spark}
2023-04-26 11:42:30,651 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@4d6ef348{/environment/json,null,AVAILABLE,@Spark}
2023-04-26 11:42:30,653 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@4df890e7{/executors,null,AVAILABLE,@Spark}
2023-04-26 11:42:30,654 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@19f09c7c{/executors/json,null,AVAILABLE,@Spark}
2023-04-26 11:42:30,656 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@5d27d20a{/executors/threadDump,null,AVAILABLE,@Spark}
2023-04-26 11:42:30,658 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@67e99e55{/executors/threadDump/json,null,AVAILABLE,@Spark}
2023-04-26 11:42:30,670 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@6e1f45e7{/static,null,AVAILABLE,@Spark}
2023-04-26 11:42:30,672 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@4f2a7f1c{/,null,AVAILABLE,@Spark}
2023-04-26 11:42:30,674 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@75f05f16{/api,null,AVAILABLE,@Spark}
2023-04-26 11:42:30,677 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@44a04dbc{/jobs/job/kill,null,AVAILABLE,@Spark}
2023-04-26 11:42:30,679 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@7b6cfe80{/stages/stage/kill,null,AVAILABLE,@Spark}
2023-04-26 11:42:30,696 INFO org.apache.spark.ui.SparkUI [restartedMain] Bound SparkUI to 0.0.0.0, and started at http://192.168.1.125:4040
2023-04-26 11:42:31,014 INFO org.apache.spark.executor.Executor [restartedMain] Starting executor ID driver on host localhost
2023-04-26 11:42:31,065 INFO org.apache.spark.util.Utils [restartedMain] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45759.
2023-04-26 11:42:31,074 INFO org.apache.spark.network.netty.NettyBlockTransferService [restartedMain] Server created on 192.168.1.125:45759
2023-04-26 11:42:31,077 INFO org.apache.spark.storage.BlockManager [restartedMain] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2023-04-26 11:42:31,142 INFO org.apache.spark.storage.BlockManagerMaster [restartedMain] Registering BlockManager BlockManagerId(driver, 192.168.1.125, 45759, None)
2023-04-26 11:42:31,147 INFO org.apache.spark.storage.BlockManagerMasterEndpoint [dispatcher-event-loop-2] Registering block manager 192.168.1.125:45759 with 998.4 MB RAM, BlockManagerId(driver, 192.168.1.125, 45759, None)
2023-04-26 11:42:31,187 INFO org.apache.spark.storage.BlockManagerMaster [restartedMain] Registered BlockManager BlockManagerId(driver, 192.168.1.125, 45759, None)
2023-04-26 11:42:31,188 INFO org.apache.spark.storage.BlockManager [restartedMain] Initialized BlockManager: BlockManagerId(driver, 192.168.1.125, 45759, None)
2023-04-26 11:42:31,417 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@3dd278bc{/metrics/json,null,AVAILABLE,@Spark}
2023-04-26 11:42:35,946 INFO org.apache.kafka.clients.consumer.ConsumerConfig [restartedMain] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-book-group-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = book-group
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2023-04-26 11:42:36,140 INFO org.apache.kafka.common.utils.AppInfoParser [restartedMain] Kafka version: 3.1.1
2023-04-26 11:42:36,140 INFO org.apache.kafka.common.utils.AppInfoParser [restartedMain] Kafka commitId: 97671528ba54a138
2023-04-26 11:42:36,141 INFO org.apache.kafka.common.utils.AppInfoParser [restartedMain] Kafka startTimeMs: 1682489556136
2023-04-26 11:42:36,147 INFO org.apache.kafka.clients.consumer.KafkaConsumer [restartedMain] [Consumer clientId=consumer-book-group-1, groupId=book-group] Subscribed to topic(s): my-topic
2023-04-26 11:42:36,291 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] Started SpringBootSecurityJwtMongodbApplication in 25.283 seconds (JVM running for 28.523)
2023-04-26 11:42:36,296 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] This is a info message
2023-04-26 11:42:36,296 WARN com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] This is a warn message
2023-04-26 11:42:37,432 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-1, groupId=book-group] Resetting the last seen epoch of partition my-topic-0 to 0 since the associated topicId changed from null to QqwJ2XdJQbi2BDXQbzT51g
2023-04-26 11:42:37,436 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-1, groupId=book-group] Cluster ID: 6CnV1ouTQjGsP-r9Rd2WRA
2023-04-26 11:42:37,438 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-1, groupId=book-group] Discovered group coordinator wks-012:9092 (id: 2147483647 rack: null)
2023-04-26 11:42:37,441 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-1, groupId=book-group] (Re-)joining group
2023-04-26 11:42:37,486 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-1, groupId=book-group] Request joining group due to: need to re-join with the given member-id
2023-04-26 11:42:37,487 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-1, groupId=book-group] (Re-)joining group
2023-04-26 11:42:37,492 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-1, groupId=book-group] Successfully joined group with generation Generation{generationId=131, memberId='consumer-book-group-1-5d588ee0-de10-4b93-ae76-ab2d11043439', protocol='range'}
2023-04-26 11:42:37,498 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-1, groupId=book-group] Finished assignment for group at generation 131: {consumer-book-group-1-5d588ee0-de10-4b93-ae76-ab2d11043439=Assignment(partitions=[my-topic-0])}
2023-04-26 11:42:37,511 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-1, groupId=book-group] Successfully synced group in generation Generation{generationId=131, memberId='consumer-book-group-1-5d588ee0-de10-4b93-ae76-ab2d11043439', protocol='range'}
2023-04-26 11:42:37,513 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-1, groupId=book-group] Notifying assignor about the new Assignment(partitions=[my-topic-0])
2023-04-26 11:42:37,519 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-1, groupId=book-group] Adding newly assigned partitions: my-topic-0
2023-04-26 11:42:37,541 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-1, groupId=book-group] Setting offset for partition my-topic-0 to the committed offset FetchPosition{offset=414, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[wks-012:9092 (id: 0 rack: null)], epoch=0}}
2023-04-26 11:42:50,223 INFO org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] [http-nio-8080-exec-1] Initializing Spring DispatcherServlet 'dispatcherServlet'
2023-04-26 11:42:54,147 INFO org.springdoc.api.AbstractOpenApiResource [http-nio-8080-exec-8] Init duration for springdoc-openapi is: 1239 ms
2023-04-26 11:43:27,534 ERROR com.example.spring.jwt.mongodb.security.jwt.AuthEntryPointJwt [http-nio-8080-exec-7] Unauthorized error: Full authentication is required to access this resource
2023-04-26 11:44:04,926 INFO org.mongodb.driver.connection [http-nio-8080-exec-8] Opened connection [connectionId{localValue:3, serverValue:44}] to localhost:27017
2023-04-26 11:44:33,024 INFO org.apache.kafka.clients.producer.ProducerConfig [http-nio-8080-exec-10] ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.springframework.kafka.support.serializer.JsonSerializer

2023-04-26 11:44:33,065 INFO org.apache.kafka.clients.producer.KafkaProducer [http-nio-8080-exec-10] [Producer clientId=producer-1] Instantiated an idempotent producer.
2023-04-26 11:44:33,095 INFO org.apache.kafka.common.utils.AppInfoParser [http-nio-8080-exec-10] Kafka version: 3.1.1
2023-04-26 11:44:33,096 INFO org.apache.kafka.common.utils.AppInfoParser [http-nio-8080-exec-10] Kafka commitId: 97671528ba54a138
2023-04-26 11:44:33,097 INFO org.apache.kafka.common.utils.AppInfoParser [http-nio-8080-exec-10] Kafka startTimeMs: 1682489673095
2023-04-26 11:44:33,111 INFO org.apache.kafka.clients.Metadata [kafka-producer-network-thread | producer-1] [Producer clientId=producer-1] Resetting the last seen epoch of partition my-topic-0 to 0 since the associated topicId changed from null to QqwJ2XdJQbi2BDXQbzT51g
2023-04-26 11:44:33,112 INFO org.apache.kafka.clients.Metadata [kafka-producer-network-thread | producer-1] [Producer clientId=producer-1] Cluster ID: 6CnV1ouTQjGsP-r9Rd2WRA
2023-04-26 11:44:33,391 INFO org.apache.kafka.clients.producer.internals.TransactionManager [kafka-producer-network-thread | producer-1] [Producer clientId=producer-1] ProducerId set to 3000 with epoch 0
2023-04-26 11:47:29,194 INFO org.apache.catalina.core.StandardService [Thread-4] Stopping service [Tomcat]
2023-04-26 11:47:29,197 INFO org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] [Thread-4] Destroying Spring FrameworkServlet 'dispatcherServlet'
2023-04-26 11:47:29,209 WARN org.apache.catalina.loader.WebappClassLoaderBase [Thread-4] The web application [ROOT] appears to have started a thread named [BufferPoolPruner-1-thread-1] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/jdk.internal.misc.Unsafe.park(Native Method)
 java.base@17.0.6/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:252)
 java.base@17.0.6/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1672)
 java.base@17.0.6/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182)
 java.base@17.0.6/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1062)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1122)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
 java.base@17.0.6/java.lang.Thread.run(Thread.java:833)
2023-04-26 11:47:29,210 WARN org.apache.catalina.loader.WebappClassLoaderBase [Thread-4] The web application [ROOT] appears to have started a thread named [cluster-ClusterId{value='6448c0c2adee62176f29075a', description='null'}-localhost:27017] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/jdk.internal.misc.Unsafe.park(Native Method)
 java.base@17.0.6/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:252)
 java.base@17.0.6/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1672)
 app//com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.waitForSignalOrTimeout(DefaultServerMonitor.java:300)
 app//com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.waitForNext(DefaultServerMonitor.java:281)
 app//com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:179)
 java.base@17.0.6/java.lang.Thread.run(Thread.java:833)
2023-04-26 11:47:29,211 WARN org.apache.catalina.loader.WebappClassLoaderBase [Thread-4] The web application [ROOT] appears to have started a thread named [cluster-rtt-ClusterId{value='6448c0c2adee62176f29075a', description='null'}-localhost:27017] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/java.lang.Thread.sleep(Native Method)
 app//com.mongodb.internal.connection.DefaultServerMonitor.waitForNext(DefaultServerMonitor.java:443)
 app//com.mongodb.internal.connection.DefaultServerMonitor.access$1500(DefaultServerMonitor.java:64)
 app//com.mongodb.internal.connection.DefaultServerMonitor$RoundTripTimeRunnable.run(DefaultServerMonitor.java:415)
 java.base@17.0.6/java.lang.Thread.run(Thread.java:833)
2023-04-26 11:47:29,212 WARN org.apache.catalina.loader.WebappClassLoaderBase [Thread-4] The web application [ROOT] appears to have started a thread named [MaintenanceTimer-2-thread-1] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/jdk.internal.misc.Unsafe.park(Native Method)
 java.base@17.0.6/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:252)
 java.base@17.0.6/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1672)
 java.base@17.0.6/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182)
 java.base@17.0.6/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1062)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1122)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
 java.base@17.0.6/java.lang.Thread.run(Thread.java:833)
2023-04-26 11:47:29,214 WARN org.apache.catalina.loader.WebappClassLoaderBase [Thread-4] The web application [ROOT] appears to have started a thread named [kafka-producer-network-thread | producer-1] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/sun.nio.ch.EPoll.wait(Native Method)
 java.base@17.0.6/sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:118)
 java.base@17.0.6/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:129)
 java.base@17.0.6/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:141)
 app//org.apache.kafka.common.network.Selector.select(Selector.java:873)
 app//org.apache.kafka.common.network.Selector.poll(Selector.java:465)
 app//org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:560)
 app//org.apache.kafka.clients.producer.internals.Sender.runOnce(Sender.java:328)
 app//org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:243)
 java.base@17.0.6/java.lang.Thread.run(Thread.java:833)
2023-04-26 11:47:29,225 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-1, groupId=book-group] Revoke previously assigned partitions my-topic-0
2023-04-26 11:47:29,227 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-1, groupId=book-group] Member consumer-book-group-1-5d588ee0-de10-4b93-ae76-ab2d11043439 sending LeaveGroup request to coordinator wks-012:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2023-04-26 11:47:29,232 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-1, groupId=book-group] Resetting generation due to: consumer pro-actively leaving the group
2023-04-26 11:47:29,232 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-1, groupId=book-group] Request joining group due to: consumer pro-actively leaving the group
2023-04-26 11:47:29,233 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-1, groupId=book-group] Unsubscribed all topics or patterns and assigned partitions
2023-04-26 11:47:29,234 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-1, groupId=book-group] Resetting generation due to: consumer pro-actively leaving the group
2023-04-26 11:47:29,234 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-1, groupId=book-group] Request joining group due to: consumer pro-actively leaving the group
2023-04-26 11:47:29,248 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Metrics scheduler closed
2023-04-26 11:47:29,253 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-04-26 11:47:29,253 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Metrics reporters closed
2023-04-26 11:47:29,280 INFO org.apache.kafka.common.utils.AppInfoParser [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] App info kafka.consumer for consumer-book-group-1 unregistered
2023-04-26 11:47:29,292 INFO org.apache.kafka.clients.producer.KafkaProducer [Thread-4] [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
2023-04-26 11:47:29,316 INFO org.apache.kafka.common.metrics.Metrics [Thread-4] Metrics scheduler closed
2023-04-26 11:47:29,317 INFO org.apache.kafka.common.metrics.Metrics [Thread-4] Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-04-26 11:47:29,317 INFO org.apache.kafka.common.metrics.Metrics [Thread-4] Metrics reporters closed
2023-04-26 11:47:29,317 INFO org.apache.kafka.common.utils.AppInfoParser [Thread-4] App info kafka.producer for producer-1 unregistered
2023-04-26 11:47:29,360 INFO org.spark_project.jetty.server.AbstractConnector [Thread-4] Stopped Spark@2dfbf2f2{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2023-04-26 11:47:29,364 INFO org.apache.spark.ui.SparkUI [Thread-4] Stopped Spark web UI at http://192.168.1.125:4040
2023-04-26 11:47:29,438 INFO org.apache.spark.MapOutputTrackerMasterEndpoint [dispatcher-event-loop-0] MapOutputTrackerMasterEndpoint stopped!
2023-04-26 11:47:29,552 INFO org.apache.spark.storage.memory.MemoryStore [Thread-4] MemoryStore cleared
2023-04-26 11:47:29,553 INFO org.apache.spark.storage.BlockManager [Thread-4] BlockManager stopped
2023-04-26 11:47:29,628 INFO org.apache.spark.storage.BlockManagerMaster [Thread-4] BlockManagerMaster stopped
2023-04-26 11:47:29,710 INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [dispatcher-event-loop-1] OutputCommitCoordinator stopped!
2023-04-26 11:47:29,811 INFO org.apache.spark.SparkContext [Thread-4] Successfully stopped SparkContext
2023-04-26 11:47:29,812 INFO org.apache.spark.SparkContext [Thread-4] SparkContext already stopped.
2023-04-26 11:47:29,828 INFO com.zaxxer.hikari.HikariDataSource [Thread-4] HikariPool-1 - Shutdown initiated...
2023-04-26 11:47:29,846 INFO com.zaxxer.hikari.HikariDataSource [Thread-4] HikariPool-1 - Shutdown completed.
2023-04-26 11:47:30,427 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] Starting SpringBootSecurityJwtMongodbApplication using Java 17.0.6 on wks-012 with PID 102147 (/home/inferyx/git/SpringApplicationWithSecurity/target/classes started by inferyx in /home/inferyx/git/SpringApplicationWithSecurity)
2023-04-26 11:47:30,427 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] No active profile set, falling back to 1 default profile: "default"
2023-04-26 11:47:31,346 INFO org.apache.catalina.core.StandardService [restartedMain] Starting service [Tomcat]
2023-04-26 11:47:31,346 INFO org.apache.catalina.core.StandardEngine [restartedMain] Starting Servlet engine: [Apache Tomcat/9.0.65]
2023-04-26 11:47:31,389 INFO org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] [restartedMain] Initializing Spring embedded WebApplicationContext
2023-04-26 11:47:31,471 INFO org.mongodb.driver.client [restartedMain] MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync|spring-boot", "version": "4.6.1"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "5.15.0-69-generic"}, "platform": "Java/Eclipse Adoptium/17.0.6+10"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.Jep395RecordCodecProvider@269fb271]}, clusterSettings={hosts=[localhost:27017], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='30000 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, sendBufferSize=0}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, sendBufferSize=0}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=JAVA_LEGACY, serverApi=null, autoEncryptionSettings=null, contextProvider=null}
2023-04-26 11:47:31,474 INFO org.mongodb.driver.connection [cluster-rtt-ClusterId{value='6448c1fbadee62176f29075b', description='null'}-localhost:27017] Opened connection [connectionId{localValue:5, serverValue:46}] to localhost:27017
2023-04-26 11:47:31,481 INFO org.mongodb.driver.connection [cluster-ClusterId{value='6448c1fbadee62176f29075b', description='null'}-localhost:27017] Opened connection [connectionId{localValue:4, serverValue:45}] to localhost:27017
2023-04-26 11:47:31,482 INFO org.mongodb.driver.cluster [cluster-ClusterId{value='6448c1fbadee62176f29075b', description='null'}-localhost:27017] Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=6, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=2490111}
2023-04-26 11:47:31,737 INFO org.hibernate.jpa.internal.util.LogHelper [restartedMain] HHH000204: Processing PersistenceUnitInfo [name: default]
2023-04-26 11:47:31,753 INFO com.zaxxer.hikari.HikariDataSource [restartedMain] HikariPool-2 - Starting...
2023-04-26 11:47:31,764 INFO com.zaxxer.hikari.HikariDataSource [restartedMain] HikariPool-2 - Start completed.
2023-04-26 11:47:31,765 INFO org.hibernate.dialect.Dialect [restartedMain] HHH000400: Using dialect: org.hibernate.dialect.MySQL5Dialect
2023-04-26 11:47:31,884 INFO org.hibernate.engine.transaction.jta.platform.internal.JtaPlatformInitiator [restartedMain] HHH000490: Using JtaPlatform implementation: [org.hibernate.engine.transaction.jta.platform.internal.NoJtaPlatform]
2023-04-26 11:47:32,168 INFO org.apache.spark.SparkContext [restartedMain] Running Spark version 2.4.5
2023-04-26 11:47:32,169 INFO org.apache.spark.SparkContext [restartedMain] Submitted application: MyAppName
2023-04-26 11:47:32,172 INFO org.apache.spark.SecurityManager [restartedMain] Changing view acls to: inferyx
2023-04-26 11:47:32,173 INFO org.apache.spark.SecurityManager [restartedMain] Changing modify acls to: inferyx
2023-04-26 11:47:32,173 INFO org.apache.spark.SecurityManager [restartedMain] Changing view acls groups to: 
2023-04-26 11:47:32,174 INFO org.apache.spark.SecurityManager [restartedMain] Changing modify acls groups to: 
2023-04-26 11:47:32,174 INFO org.apache.spark.SecurityManager [restartedMain] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(inferyx); groups with view permissions: Set(); users  with modify permissions: Set(inferyx); groups with modify permissions: Set()
2023-04-26 11:47:32,237 INFO org.apache.spark.util.Utils [restartedMain] Successfully started service 'sparkDriver' on port 42199.
2023-04-26 11:47:32,242 INFO org.apache.spark.SparkEnv [restartedMain] Registering MapOutputTracker
2023-04-26 11:47:32,243 INFO org.apache.spark.SparkEnv [restartedMain] Registering BlockManagerMaster
2023-04-26 11:47:32,244 INFO org.apache.spark.storage.BlockManagerMasterEndpoint [restartedMain] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2023-04-26 11:47:32,244 INFO org.apache.spark.storage.BlockManagerMasterEndpoint [restartedMain] BlockManagerMasterEndpoint up
2023-04-26 11:47:32,246 INFO org.apache.spark.storage.DiskBlockManager [restartedMain] Created local directory at /tmp/blockmgr-2e5ee95c-c4a5-40df-9dee-e768482a9209
2023-04-26 11:47:32,247 INFO org.apache.spark.storage.memory.MemoryStore [restartedMain] MemoryStore started with capacity 998.4 MB
2023-04-26 11:47:32,250 INFO org.apache.spark.SparkEnv [restartedMain] Registering OutputCommitCoordinator
2023-04-26 11:47:32,261 INFO org.spark_project.jetty.server.Server [restartedMain] jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2023-04-26 11:47:32,265 INFO org.spark_project.jetty.server.Server [restartedMain] Started @324498ms
2023-04-26 11:47:32,267 INFO org.spark_project.jetty.server.AbstractConnector [restartedMain] Started ServerConnector@35df1986{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2023-04-26 11:47:32,268 INFO org.apache.spark.util.Utils [restartedMain] Successfully started service 'SparkUI' on port 4040.
2023-04-26 11:47:32,269 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@4dd69c3a{/jobs,null,AVAILABLE,@Spark}
2023-04-26 11:47:32,271 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@7933bc7{/jobs/json,null,AVAILABLE,@Spark}
2023-04-26 11:47:32,272 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@52dd2f2e{/jobs/job,null,AVAILABLE,@Spark}
2023-04-26 11:47:32,273 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@1f830fda{/jobs/job/json,null,AVAILABLE,@Spark}
2023-04-26 11:47:32,274 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@66cd675c{/stages,null,AVAILABLE,@Spark}
2023-04-26 11:47:32,275 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@781e3541{/stages/json,null,AVAILABLE,@Spark}
2023-04-26 11:47:32,276 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@4381c133{/stages/stage,null,AVAILABLE,@Spark}
2023-04-26 11:47:32,278 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@44fbeb46{/stages/stage/json,null,AVAILABLE,@Spark}
2023-04-26 11:47:32,279 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@49818302{/stages/pool,null,AVAILABLE,@Spark}
2023-04-26 11:47:32,280 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@40bc034e{/stages/pool/json,null,AVAILABLE,@Spark}
2023-04-26 11:47:32,281 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@d79ca33{/storage,null,AVAILABLE,@Spark}
2023-04-26 11:47:32,282 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@16c3a101{/storage/json,null,AVAILABLE,@Spark}
2023-04-26 11:47:32,283 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@531fb948{/storage/rdd,null,AVAILABLE,@Spark}
2023-04-26 11:47:32,284 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@73997616{/storage/rdd/json,null,AVAILABLE,@Spark}
2023-04-26 11:47:32,284 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@30f21204{/environment,null,AVAILABLE,@Spark}
2023-04-26 11:47:32,285 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@3bb266b2{/environment/json,null,AVAILABLE,@Spark}
2023-04-26 11:47:32,286 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@15494d75{/executors,null,AVAILABLE,@Spark}
2023-04-26 11:47:32,287 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@2d58a00c{/executors/json,null,AVAILABLE,@Spark}
2023-04-26 11:47:32,288 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@d1af591{/executors/threadDump,null,AVAILABLE,@Spark}
2023-04-26 11:47:32,289 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@25358576{/executors/threadDump/json,null,AVAILABLE,@Spark}
2023-04-26 11:47:32,291 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@7a485640{/static,null,AVAILABLE,@Spark}
2023-04-26 11:47:32,293 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@12f8d1c5{/,null,AVAILABLE,@Spark}
2023-04-26 11:47:32,294 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@48941876{/api,null,AVAILABLE,@Spark}
2023-04-26 11:47:32,295 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@79c19e01{/jobs/job/kill,null,AVAILABLE,@Spark}
2023-04-26 11:47:32,297 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@2d631e05{/stages/stage/kill,null,AVAILABLE,@Spark}
2023-04-26 11:47:32,297 INFO org.apache.spark.ui.SparkUI [restartedMain] Bound SparkUI to 0.0.0.0, and started at http://192.168.1.125:4040
2023-04-26 11:47:32,335 INFO org.apache.spark.executor.Executor [restartedMain] Starting executor ID driver on host localhost
2023-04-26 11:47:32,342 INFO org.apache.spark.util.Utils [restartedMain] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44061.
2023-04-26 11:47:32,343 INFO org.apache.spark.network.netty.NettyBlockTransferService [restartedMain] Server created on 192.168.1.125:44061
2023-04-26 11:47:32,343 INFO org.apache.spark.storage.BlockManager [restartedMain] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2023-04-26 11:47:32,344 INFO org.apache.spark.storage.BlockManagerMaster [restartedMain] Registering BlockManager BlockManagerId(driver, 192.168.1.125, 44061, None)
2023-04-26 11:47:32,345 INFO org.apache.spark.storage.BlockManagerMasterEndpoint [dispatcher-event-loop-2] Registering block manager 192.168.1.125:44061 with 998.4 MB RAM, BlockManagerId(driver, 192.168.1.125, 44061, None)
2023-04-26 11:47:32,346 INFO org.apache.spark.storage.BlockManagerMaster [restartedMain] Registered BlockManager BlockManagerId(driver, 192.168.1.125, 44061, None)
2023-04-26 11:47:32,347 INFO org.apache.spark.storage.BlockManager [restartedMain] Initialized BlockManager: BlockManagerId(driver, 192.168.1.125, 44061, None)
2023-04-26 11:47:32,349 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@494dbc07{/metrics/json,null,AVAILABLE,@Spark}
2023-04-26 11:47:34,017 INFO org.apache.kafka.clients.consumer.ConsumerConfig [restartedMain] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-book-group-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = book-group
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2023-04-26 11:47:34,024 INFO org.apache.kafka.common.utils.AppInfoParser [restartedMain] Kafka version: 3.1.1
2023-04-26 11:47:34,025 INFO org.apache.kafka.common.utils.AppInfoParser [restartedMain] Kafka commitId: 97671528ba54a138
2023-04-26 11:47:34,025 INFO org.apache.kafka.common.utils.AppInfoParser [restartedMain] Kafka startTimeMs: 1682489854024
2023-04-26 11:47:34,026 INFO org.apache.kafka.clients.consumer.KafkaConsumer [restartedMain] [Consumer clientId=consumer-book-group-2, groupId=book-group] Subscribed to topic(s): my-topic
2023-04-26 11:47:34,037 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-2, groupId=book-group] Resetting the last seen epoch of partition my-topic-0 to 0 since the associated topicId changed from null to QqwJ2XdJQbi2BDXQbzT51g
2023-04-26 11:47:34,038 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-2, groupId=book-group] Cluster ID: 6CnV1ouTQjGsP-r9Rd2WRA
2023-04-26 11:47:34,038 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-2, groupId=book-group] Discovered group coordinator wks-012:9092 (id: 2147483647 rack: null)
2023-04-26 11:47:34,041 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-2, groupId=book-group] (Re-)joining group
2023-04-26 11:47:34,049 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-2, groupId=book-group] Request joining group due to: need to re-join with the given member-id
2023-04-26 11:47:34,054 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-2, groupId=book-group] (Re-)joining group
2023-04-26 11:47:34,058 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-2, groupId=book-group] Successfully joined group with generation Generation{generationId=133, memberId='consumer-book-group-2-b9aeb80a-6c33-4bbc-9f15-626d497f76c3', protocol='range'}
2023-04-26 11:47:34,058 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-2, groupId=book-group] Finished assignment for group at generation 133: {consumer-book-group-2-b9aeb80a-6c33-4bbc-9f15-626d497f76c3=Assignment(partitions=[my-topic-0])}
2023-04-26 11:47:34,069 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-2, groupId=book-group] Successfully synced group in generation Generation{generationId=133, memberId='consumer-book-group-2-b9aeb80a-6c33-4bbc-9f15-626d497f76c3', protocol='range'}
2023-04-26 11:47:34,070 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-2, groupId=book-group] Notifying assignor about the new Assignment(partitions=[my-topic-0])
2023-04-26 11:47:34,071 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-2, groupId=book-group] Adding newly assigned partitions: my-topic-0
2023-04-26 11:47:34,076 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-2, groupId=book-group] Setting offset for partition my-topic-0 to the committed offset FetchPosition{offset=415, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[wks-012:9092 (id: 0 rack: null)], epoch=0}}
2023-04-26 11:47:34,103 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] Started SpringBootSecurityJwtMongodbApplication in 3.829 seconds (JVM running for 326.336)
2023-04-26 11:47:34,108 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] This is a info message
2023-04-26 11:47:34,108 WARN com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] This is a warn message
2023-04-26 11:47:59,504 INFO org.apache.catalina.core.StandardService [Thread-21] Stopping service [Tomcat]
2023-04-26 11:47:59,508 WARN org.apache.catalina.loader.WebappClassLoaderBase [Thread-21] The web application [ROOT] appears to have started a thread named [cluster-ClusterId{value='6448c1fbadee62176f29075b', description='null'}-localhost:27017] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/jdk.internal.misc.Unsafe.park(Native Method)
 java.base@17.0.6/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:252)
 java.base@17.0.6/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1672)
 app//com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.waitForSignalOrTimeout(DefaultServerMonitor.java:300)
 app//com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.waitForNext(DefaultServerMonitor.java:281)
 app//com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:179)
 java.base@17.0.6/java.lang.Thread.run(Thread.java:833)
2023-04-26 11:47:59,509 WARN org.apache.catalina.loader.WebappClassLoaderBase [Thread-21] The web application [ROOT] appears to have started a thread named [cluster-rtt-ClusterId{value='6448c1fbadee62176f29075b', description='null'}-localhost:27017] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/java.lang.Thread.sleep(Native Method)
 app//com.mongodb.internal.connection.DefaultServerMonitor.waitForNext(DefaultServerMonitor.java:443)
 app//com.mongodb.internal.connection.DefaultServerMonitor.access$1500(DefaultServerMonitor.java:64)
 app//com.mongodb.internal.connection.DefaultServerMonitor$RoundTripTimeRunnable.run(DefaultServerMonitor.java:415)
 java.base@17.0.6/java.lang.Thread.run(Thread.java:833)
2023-04-26 11:47:59,510 WARN org.apache.catalina.loader.WebappClassLoaderBase [Thread-21] The web application [ROOT] appears to have started a thread named [MaintenanceTimer-3-thread-1] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/jdk.internal.misc.Unsafe.park(Native Method)
 java.base@17.0.6/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:252)
 java.base@17.0.6/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1672)
 java.base@17.0.6/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182)
 java.base@17.0.6/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1062)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1122)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
 java.base@17.0.6/java.lang.Thread.run(Thread.java:833)
2023-04-26 11:47:59,513 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-2, groupId=book-group] Revoke previously assigned partitions my-topic-0
2023-04-26 11:47:59,513 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-2, groupId=book-group] Member consumer-book-group-2-b9aeb80a-6c33-4bbc-9f15-626d497f76c3 sending LeaveGroup request to coordinator wks-012:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2023-04-26 11:47:59,516 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-2, groupId=book-group] Resetting generation due to: consumer pro-actively leaving the group
2023-04-26 11:47:59,517 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-2, groupId=book-group] Request joining group due to: consumer pro-actively leaving the group
2023-04-26 11:47:59,517 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-2, groupId=book-group] Unsubscribed all topics or patterns and assigned partitions
2023-04-26 11:47:59,518 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-2, groupId=book-group] Resetting generation due to: consumer pro-actively leaving the group
2023-04-26 11:47:59,518 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-2, groupId=book-group] Request joining group due to: consumer pro-actively leaving the group
2023-04-26 11:47:59,519 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Metrics scheduler closed
2023-04-26 11:47:59,519 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-04-26 11:47:59,520 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Metrics reporters closed
2023-04-26 11:47:59,523 INFO org.apache.kafka.common.utils.AppInfoParser [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] App info kafka.consumer for consumer-book-group-2 unregistered
2023-04-26 11:47:59,528 INFO org.spark_project.jetty.server.AbstractConnector [Thread-21] Stopped Spark@35df1986{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2023-04-26 11:47:59,529 INFO org.apache.spark.ui.SparkUI [Thread-21] Stopped Spark web UI at http://192.168.1.125:4040
2023-04-26 11:47:59,531 INFO org.apache.spark.MapOutputTrackerMasterEndpoint [dispatcher-event-loop-2] MapOutputTrackerMasterEndpoint stopped!
2023-04-26 11:47:59,542 INFO org.apache.spark.storage.memory.MemoryStore [Thread-21] MemoryStore cleared
2023-04-26 11:47:59,542 INFO org.apache.spark.storage.BlockManager [Thread-21] BlockManager stopped
2023-04-26 11:47:59,543 INFO org.apache.spark.storage.BlockManagerMaster [Thread-21] BlockManagerMaster stopped
2023-04-26 11:47:59,544 INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [dispatcher-event-loop-3] OutputCommitCoordinator stopped!
2023-04-26 11:47:59,554 INFO org.apache.spark.SparkContext [Thread-21] Successfully stopped SparkContext
2023-04-26 11:47:59,555 INFO org.apache.spark.SparkContext [Thread-21] SparkContext already stopped.
2023-04-26 11:47:59,562 INFO com.zaxxer.hikari.HikariDataSource [Thread-21] HikariPool-2 - Shutdown initiated...
2023-04-26 11:47:59,566 INFO com.zaxxer.hikari.HikariDataSource [Thread-21] HikariPool-2 - Shutdown completed.
2023-04-26 11:47:59,811 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] Starting SpringBootSecurityJwtMongodbApplication using Java 17.0.6 on wks-012 with PID 102147 (/home/inferyx/git/SpringApplicationWithSecurity/target/classes started by inferyx in /home/inferyx/git/SpringApplicationWithSecurity)
2023-04-26 11:47:59,811 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] No active profile set, falling back to 1 default profile: "default"
2023-04-26 11:48:00,548 INFO org.apache.catalina.core.StandardService [restartedMain] Starting service [Tomcat]
2023-04-26 11:48:00,549 INFO org.apache.catalina.core.StandardEngine [restartedMain] Starting Servlet engine: [Apache Tomcat/9.0.65]
2023-04-26 11:48:00,576 INFO org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] [restartedMain] Initializing Spring embedded WebApplicationContext
2023-04-26 11:48:00,621 INFO org.mongodb.driver.client [restartedMain] MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync|spring-boot", "version": "4.6.1"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "5.15.0-69-generic"}, "platform": "Java/Eclipse Adoptium/17.0.6+10"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.Jep395RecordCodecProvider@269fb271]}, clusterSettings={hosts=[localhost:27017], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='30000 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, sendBufferSize=0}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, sendBufferSize=0}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=JAVA_LEGACY, serverApi=null, autoEncryptionSettings=null, contextProvider=null}
2023-04-26 11:48:00,656 INFO org.mongodb.driver.connection [cluster-ClusterId{value='6448c218adee62176f29075c', description='null'}-localhost:27017] Opened connection [connectionId{localValue:6, serverValue:48}] to localhost:27017
2023-04-26 11:48:00,657 INFO org.mongodb.driver.cluster [cluster-ClusterId{value='6448c218adee62176f29075c', description='null'}-localhost:27017] Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=6, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=32268287}
2023-04-26 11:48:00,657 INFO org.mongodb.driver.connection [cluster-rtt-ClusterId{value='6448c218adee62176f29075c', description='null'}-localhost:27017] Opened connection [connectionId{localValue:7, serverValue:47}] to localhost:27017
2023-04-26 11:48:00,852 INFO org.hibernate.jpa.internal.util.LogHelper [restartedMain] HHH000204: Processing PersistenceUnitInfo [name: default]
2023-04-26 11:48:00,865 INFO com.zaxxer.hikari.HikariDataSource [restartedMain] HikariPool-3 - Starting...
2023-04-26 11:48:00,871 INFO com.zaxxer.hikari.HikariDataSource [restartedMain] HikariPool-3 - Start completed.
2023-04-26 11:48:00,872 INFO org.hibernate.dialect.Dialect [restartedMain] HHH000400: Using dialect: org.hibernate.dialect.MySQL5Dialect
2023-04-26 11:48:00,980 INFO org.hibernate.engine.transaction.jta.platform.internal.JtaPlatformInitiator [restartedMain] HHH000490: Using JtaPlatform implementation: [org.hibernate.engine.transaction.jta.platform.internal.NoJtaPlatform]
2023-04-26 11:48:01,275 INFO org.apache.spark.SparkContext [restartedMain] Running Spark version 2.4.5
2023-04-26 11:48:01,276 INFO org.apache.spark.SparkContext [restartedMain] Submitted application: MyAppName
2023-04-26 11:48:01,279 INFO org.apache.spark.SecurityManager [restartedMain] Changing view acls to: inferyx
2023-04-26 11:48:01,279 INFO org.apache.spark.SecurityManager [restartedMain] Changing modify acls to: inferyx
2023-04-26 11:48:01,280 INFO org.apache.spark.SecurityManager [restartedMain] Changing view acls groups to: 
2023-04-26 11:48:01,280 INFO org.apache.spark.SecurityManager [restartedMain] Changing modify acls groups to: 
2023-04-26 11:48:01,281 INFO org.apache.spark.SecurityManager [restartedMain] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(inferyx); groups with view permissions: Set(); users  with modify permissions: Set(inferyx); groups with modify permissions: Set()
2023-04-26 11:48:01,349 INFO org.apache.spark.util.Utils [restartedMain] Successfully started service 'sparkDriver' on port 39283.
2023-04-26 11:48:01,355 INFO org.apache.spark.SparkEnv [restartedMain] Registering MapOutputTracker
2023-04-26 11:48:01,357 INFO org.apache.spark.SparkEnv [restartedMain] Registering BlockManagerMaster
2023-04-26 11:48:01,358 INFO org.apache.spark.storage.BlockManagerMasterEndpoint [restartedMain] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2023-04-26 11:48:01,358 INFO org.apache.spark.storage.BlockManagerMasterEndpoint [restartedMain] BlockManagerMasterEndpoint up
2023-04-26 11:48:01,359 INFO org.apache.spark.storage.DiskBlockManager [restartedMain] Created local directory at /tmp/blockmgr-39b9aad9-7785-41ba-8cea-73edfbd6c313
2023-04-26 11:48:01,360 INFO org.apache.spark.storage.memory.MemoryStore [restartedMain] MemoryStore started with capacity 998.4 MB
2023-04-26 11:48:01,364 INFO org.apache.spark.SparkEnv [restartedMain] Registering OutputCommitCoordinator
2023-04-26 11:48:01,374 INFO org.spark_project.jetty.server.Server [restartedMain] jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2023-04-26 11:48:01,376 INFO org.spark_project.jetty.server.Server [restartedMain] Started @353609ms
2023-04-26 11:48:01,378 INFO org.spark_project.jetty.server.AbstractConnector [restartedMain] Started ServerConnector@2317ddb8{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2023-04-26 11:48:01,379 INFO org.apache.spark.util.Utils [restartedMain] Successfully started service 'SparkUI' on port 4040.
2023-04-26 11:48:01,380 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@2982d4e0{/jobs,null,AVAILABLE,@Spark}
2023-04-26 11:48:01,381 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@5cfaba45{/jobs/json,null,AVAILABLE,@Spark}
2023-04-26 11:48:01,382 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@530b45da{/jobs/job,null,AVAILABLE,@Spark}
2023-04-26 11:48:01,383 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@4e6ca81e{/jobs/job/json,null,AVAILABLE,@Spark}
2023-04-26 11:48:01,384 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@71f640e7{/stages,null,AVAILABLE,@Spark}
2023-04-26 11:48:01,385 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@78d7f40e{/stages/json,null,AVAILABLE,@Spark}
2023-04-26 11:48:01,387 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@71fb4b5b{/stages/stage,null,AVAILABLE,@Spark}
2023-04-26 11:48:01,388 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@5a7c3122{/stages/stage/json,null,AVAILABLE,@Spark}
2023-04-26 11:48:01,390 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@194ea3f2{/stages/pool,null,AVAILABLE,@Spark}
2023-04-26 11:48:01,391 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@5efe5351{/stages/pool/json,null,AVAILABLE,@Spark}
2023-04-26 11:48:01,391 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@75f47a46{/storage,null,AVAILABLE,@Spark}
2023-04-26 11:48:01,392 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@4e3aa0fb{/storage/json,null,AVAILABLE,@Spark}
2023-04-26 11:48:01,393 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@2207cbfd{/storage/rdd,null,AVAILABLE,@Spark}
2023-04-26 11:48:01,396 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@e4f7037{/storage/rdd/json,null,AVAILABLE,@Spark}
2023-04-26 11:48:01,398 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@58bf1df2{/environment,null,AVAILABLE,@Spark}
2023-04-26 11:48:01,400 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@43194366{/environment/json,null,AVAILABLE,@Spark}
2023-04-26 11:48:01,401 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@5f5d30db{/executors,null,AVAILABLE,@Spark}
2023-04-26 11:48:01,402 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@2ae63dcf{/executors/json,null,AVAILABLE,@Spark}
2023-04-26 11:48:01,403 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@a96e5ec{/executors/threadDump,null,AVAILABLE,@Spark}
2023-04-26 11:48:01,404 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@31d95ab{/executors/threadDump/json,null,AVAILABLE,@Spark}
2023-04-26 11:48:01,406 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@3570b6fb{/static,null,AVAILABLE,@Spark}
2023-04-26 11:48:01,407 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@2e63fddf{/,null,AVAILABLE,@Spark}
2023-04-26 11:48:01,408 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@1367c47a{/api,null,AVAILABLE,@Spark}
2023-04-26 11:48:01,410 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@4ccf1d72{/jobs/job/kill,null,AVAILABLE,@Spark}
2023-04-26 11:48:01,411 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@7d739c31{/stages/stage/kill,null,AVAILABLE,@Spark}
2023-04-26 11:48:01,411 INFO org.apache.spark.ui.SparkUI [restartedMain] Bound SparkUI to 0.0.0.0, and started at http://192.168.1.125:4040
2023-04-26 11:48:01,456 INFO org.apache.spark.executor.Executor [restartedMain] Starting executor ID driver on host localhost
2023-04-26 11:48:01,465 INFO org.apache.spark.util.Utils [restartedMain] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33337.
2023-04-26 11:48:01,468 INFO org.apache.spark.network.netty.NettyBlockTransferService [restartedMain] Server created on 192.168.1.125:33337
2023-04-26 11:48:01,468 INFO org.apache.spark.storage.BlockManager [restartedMain] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2023-04-26 11:48:01,469 INFO org.apache.spark.storage.BlockManagerMaster [restartedMain] Registering BlockManager BlockManagerId(driver, 192.168.1.125, 33337, None)
2023-04-26 11:48:01,470 INFO org.apache.spark.storage.BlockManagerMasterEndpoint [dispatcher-event-loop-3] Registering block manager 192.168.1.125:33337 with 998.4 MB RAM, BlockManagerId(driver, 192.168.1.125, 33337, None)
2023-04-26 11:48:01,471 INFO org.apache.spark.storage.BlockManagerMaster [restartedMain] Registered BlockManager BlockManagerId(driver, 192.168.1.125, 33337, None)
2023-04-26 11:48:01,471 INFO org.apache.spark.storage.BlockManager [restartedMain] Initialized BlockManager: BlockManagerId(driver, 192.168.1.125, 33337, None)
2023-04-26 11:48:01,472 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@6d7eb6eb{/metrics/json,null,AVAILABLE,@Spark}
2023-04-26 11:48:02,923 INFO org.apache.kafka.clients.consumer.ConsumerConfig [restartedMain] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-book-group-3
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = book-group
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2023-04-26 11:48:02,930 INFO org.apache.kafka.common.utils.AppInfoParser [restartedMain] Kafka version: 3.1.1
2023-04-26 11:48:02,930 INFO org.apache.kafka.common.utils.AppInfoParser [restartedMain] Kafka commitId: 97671528ba54a138
2023-04-26 11:48:02,930 INFO org.apache.kafka.common.utils.AppInfoParser [restartedMain] Kafka startTimeMs: 1682489882930
2023-04-26 11:48:02,931 INFO org.apache.kafka.clients.consumer.KafkaConsumer [restartedMain] [Consumer clientId=consumer-book-group-3, groupId=book-group] Subscribed to topic(s): my-topic
2023-04-26 11:48:02,936 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-3, groupId=book-group] Resetting the last seen epoch of partition my-topic-0 to 0 since the associated topicId changed from null to QqwJ2XdJQbi2BDXQbzT51g
2023-04-26 11:48:02,937 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-3, groupId=book-group] Cluster ID: 6CnV1ouTQjGsP-r9Rd2WRA
2023-04-26 11:48:02,939 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-3, groupId=book-group] Discovered group coordinator wks-012:9092 (id: 2147483647 rack: null)
2023-04-26 11:48:02,940 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-3, groupId=book-group] (Re-)joining group
2023-04-26 11:48:02,945 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-3, groupId=book-group] Request joining group due to: need to re-join with the given member-id
2023-04-26 11:48:02,946 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-3, groupId=book-group] (Re-)joining group
2023-04-26 11:48:02,948 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] Started SpringBootSecurityJwtMongodbApplication in 3.193 seconds (JVM running for 355.18)
2023-04-26 11:48:02,948 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-3, groupId=book-group] Successfully joined group with generation Generation{generationId=135, memberId='consumer-book-group-3-9509cc31-3830-43fe-a842-ce10f80e473c', protocol='range'}
2023-04-26 11:48:02,949 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-3, groupId=book-group] Finished assignment for group at generation 135: {consumer-book-group-3-9509cc31-3830-43fe-a842-ce10f80e473c=Assignment(partitions=[my-topic-0])}
2023-04-26 11:48:02,953 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-3, groupId=book-group] Successfully synced group in generation Generation{generationId=135, memberId='consumer-book-group-3-9509cc31-3830-43fe-a842-ce10f80e473c', protocol='range'}
2023-04-26 11:48:02,954 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-3, groupId=book-group] Notifying assignor about the new Assignment(partitions=[my-topic-0])
2023-04-26 11:48:02,954 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-3, groupId=book-group] Adding newly assigned partitions: my-topic-0
2023-04-26 11:48:02,956 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] This is a info message
2023-04-26 11:48:02,956 WARN com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] This is a warn message
2023-04-26 11:48:02,963 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-3, groupId=book-group] Setting offset for partition my-topic-0 to the committed offset FetchPosition{offset=415, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[wks-012:9092 (id: 0 rack: null)], epoch=0}}
2023-04-26 11:48:28,411 INFO org.apache.catalina.core.StandardService [Thread-37] Stopping service [Tomcat]
2023-04-26 11:48:28,418 WARN org.apache.catalina.loader.WebappClassLoaderBase [Thread-37] The web application [ROOT] appears to have started a thread named [cluster-ClusterId{value='6448c218adee62176f29075c', description='null'}-localhost:27017] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/jdk.internal.misc.Unsafe.park(Native Method)
 java.base@17.0.6/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:252)
 java.base@17.0.6/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1672)
 app//com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.waitForSignalOrTimeout(DefaultServerMonitor.java:300)
 app//com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.waitForNext(DefaultServerMonitor.java:281)
 app//com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:179)
 java.base@17.0.6/java.lang.Thread.run(Thread.java:833)
2023-04-26 11:48:28,423 WARN org.apache.catalina.loader.WebappClassLoaderBase [Thread-37] The web application [ROOT] appears to have started a thread named [cluster-rtt-ClusterId{value='6448c218adee62176f29075c', description='null'}-localhost:27017] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/java.lang.Thread.sleep(Native Method)
 app//com.mongodb.internal.connection.DefaultServerMonitor.waitForNext(DefaultServerMonitor.java:443)
 app//com.mongodb.internal.connection.DefaultServerMonitor.access$1500(DefaultServerMonitor.java:64)
 app//com.mongodb.internal.connection.DefaultServerMonitor$RoundTripTimeRunnable.run(DefaultServerMonitor.java:415)
 java.base@17.0.6/java.lang.Thread.run(Thread.java:833)
2023-04-26 11:48:28,424 WARN org.apache.catalina.loader.WebappClassLoaderBase [Thread-37] The web application [ROOT] appears to have started a thread named [MaintenanceTimer-4-thread-1] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/jdk.internal.misc.Unsafe.park(Native Method)
 java.base@17.0.6/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:252)
 java.base@17.0.6/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1672)
 java.base@17.0.6/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182)
 java.base@17.0.6/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1062)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1122)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
 java.base@17.0.6/java.lang.Thread.run(Thread.java:833)
2023-04-26 11:48:28,428 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-3, groupId=book-group] Revoke previously assigned partitions my-topic-0
2023-04-26 11:48:28,429 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-3, groupId=book-group] Member consumer-book-group-3-9509cc31-3830-43fe-a842-ce10f80e473c sending LeaveGroup request to coordinator wks-012:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2023-04-26 11:48:28,435 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-3, groupId=book-group] Resetting generation due to: consumer pro-actively leaving the group
2023-04-26 11:48:28,436 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-3, groupId=book-group] Request joining group due to: consumer pro-actively leaving the group
2023-04-26 11:48:28,436 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-3, groupId=book-group] Unsubscribed all topics or patterns and assigned partitions
2023-04-26 11:48:28,436 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-3, groupId=book-group] Resetting generation due to: consumer pro-actively leaving the group
2023-04-26 11:48:28,437 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-3, groupId=book-group] Request joining group due to: consumer pro-actively leaving the group
2023-04-26 11:48:28,438 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Metrics scheduler closed
2023-04-26 11:48:28,438 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-04-26 11:48:28,439 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Metrics reporters closed
2023-04-26 11:48:28,445 INFO org.apache.kafka.common.utils.AppInfoParser [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] App info kafka.consumer for consumer-book-group-3 unregistered
2023-04-26 11:48:28,455 INFO org.spark_project.jetty.server.AbstractConnector [Thread-37] Stopped Spark@2317ddb8{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2023-04-26 11:48:28,458 INFO org.apache.spark.ui.SparkUI [Thread-37] Stopped Spark web UI at http://192.168.1.125:4040
2023-04-26 11:48:28,462 INFO org.apache.spark.MapOutputTrackerMasterEndpoint [dispatcher-event-loop-3] MapOutputTrackerMasterEndpoint stopped!
2023-04-26 11:48:28,500 INFO org.apache.spark.storage.memory.MemoryStore [Thread-37] MemoryStore cleared
2023-04-26 11:48:28,501 INFO org.apache.spark.storage.BlockManager [Thread-37] BlockManager stopped
2023-04-26 11:48:28,501 INFO org.apache.spark.storage.BlockManagerMaster [Thread-37] BlockManagerMaster stopped
2023-04-26 11:48:28,502 INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [dispatcher-event-loop-2] OutputCommitCoordinator stopped!
2023-04-26 11:48:28,516 INFO org.apache.spark.SparkContext [Thread-37] Successfully stopped SparkContext
2023-04-26 11:48:28,516 INFO org.apache.spark.SparkContext [Thread-37] SparkContext already stopped.
2023-04-26 11:48:28,518 INFO com.zaxxer.hikari.HikariDataSource [Thread-37] HikariPool-3 - Shutdown initiated...
2023-04-26 11:48:28,523 INFO com.zaxxer.hikari.HikariDataSource [Thread-37] HikariPool-3 - Shutdown completed.
2023-04-26 11:48:28,788 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] Starting SpringBootSecurityJwtMongodbApplication using Java 17.0.6 on wks-012 with PID 102147 (/home/inferyx/git/SpringApplicationWithSecurity/target/classes started by inferyx in /home/inferyx/git/SpringApplicationWithSecurity)
2023-04-26 11:48:28,788 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] No active profile set, falling back to 1 default profile: "default"
2023-04-26 11:48:29,534 INFO org.apache.catalina.core.StandardService [restartedMain] Starting service [Tomcat]
2023-04-26 11:48:29,534 INFO org.apache.catalina.core.StandardEngine [restartedMain] Starting Servlet engine: [Apache Tomcat/9.0.65]
2023-04-26 11:48:29,560 INFO org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] [restartedMain] Initializing Spring embedded WebApplicationContext
2023-04-26 11:48:29,638 INFO org.mongodb.driver.client [restartedMain] MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync|spring-boot", "version": "4.6.1"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "5.15.0-69-generic"}, "platform": "Java/Eclipse Adoptium/17.0.6+10"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.Jep395RecordCodecProvider@269fb271]}, clusterSettings={hosts=[localhost:27017], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='30000 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, sendBufferSize=0}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, sendBufferSize=0}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=JAVA_LEGACY, serverApi=null, autoEncryptionSettings=null, contextProvider=null}
2023-04-26 11:48:29,640 INFO org.mongodb.driver.connection [cluster-rtt-ClusterId{value='6448c235adee62176f29075d', description='null'}-localhost:27017] Opened connection [connectionId{localValue:9, serverValue:49}] to localhost:27017
2023-04-26 11:48:29,647 INFO org.mongodb.driver.connection [cluster-ClusterId{value='6448c235adee62176f29075d', description='null'}-localhost:27017] Opened connection [connectionId{localValue:8, serverValue:50}] to localhost:27017
2023-04-26 11:48:29,648 INFO org.mongodb.driver.cluster [cluster-ClusterId{value='6448c235adee62176f29075d', description='null'}-localhost:27017] Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=6, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=8222265}
2023-04-26 11:48:29,937 INFO org.hibernate.jpa.internal.util.LogHelper [restartedMain] HHH000204: Processing PersistenceUnitInfo [name: default]
2023-04-26 11:48:29,962 INFO com.zaxxer.hikari.HikariDataSource [restartedMain] HikariPool-4 - Starting...
2023-04-26 11:48:29,968 INFO com.zaxxer.hikari.HikariDataSource [restartedMain] HikariPool-4 - Start completed.
2023-04-26 11:48:29,969 INFO org.hibernate.dialect.Dialect [restartedMain] HHH000400: Using dialect: org.hibernate.dialect.MySQL5Dialect
2023-04-26 11:48:30,160 INFO org.hibernate.engine.transaction.jta.platform.internal.JtaPlatformInitiator [restartedMain] HHH000490: Using JtaPlatform implementation: [org.hibernate.engine.transaction.jta.platform.internal.NoJtaPlatform]
2023-04-26 11:48:30,536 INFO org.apache.spark.SparkContext [restartedMain] Running Spark version 2.4.5
2023-04-26 11:48:30,538 INFO org.apache.spark.SparkContext [restartedMain] Submitted application: MyAppName
2023-04-26 11:48:30,542 INFO org.apache.spark.SecurityManager [restartedMain] Changing view acls to: inferyx
2023-04-26 11:48:30,543 INFO org.apache.spark.SecurityManager [restartedMain] Changing modify acls to: inferyx
2023-04-26 11:48:30,544 INFO org.apache.spark.SecurityManager [restartedMain] Changing view acls groups to: 
2023-04-26 11:48:30,544 INFO org.apache.spark.SecurityManager [restartedMain] Changing modify acls groups to: 
2023-04-26 11:48:30,544 INFO org.apache.spark.SecurityManager [restartedMain] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(inferyx); groups with view permissions: Set(); users  with modify permissions: Set(inferyx); groups with modify permissions: Set()
2023-04-26 11:48:30,604 INFO org.apache.spark.util.Utils [restartedMain] Successfully started service 'sparkDriver' on port 46635.
2023-04-26 11:48:30,610 INFO org.apache.spark.SparkEnv [restartedMain] Registering MapOutputTracker
2023-04-26 11:48:30,612 INFO org.apache.spark.SparkEnv [restartedMain] Registering BlockManagerMaster
2023-04-26 11:48:30,613 INFO org.apache.spark.storage.BlockManagerMasterEndpoint [restartedMain] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2023-04-26 11:48:30,613 INFO org.apache.spark.storage.BlockManagerMasterEndpoint [restartedMain] BlockManagerMasterEndpoint up
2023-04-26 11:48:30,614 INFO org.apache.spark.storage.DiskBlockManager [restartedMain] Created local directory at /tmp/blockmgr-c49aacd1-a35f-49cf-bc99-ef4570b4857e
2023-04-26 11:48:30,615 INFO org.apache.spark.storage.memory.MemoryStore [restartedMain] MemoryStore started with capacity 998.4 MB
2023-04-26 11:48:30,618 INFO org.apache.spark.SparkEnv [restartedMain] Registering OutputCommitCoordinator
2023-04-26 11:48:30,626 INFO org.spark_project.jetty.server.Server [restartedMain] jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2023-04-26 11:48:30,636 INFO org.spark_project.jetty.server.Server [restartedMain] Started @382869ms
2023-04-26 11:48:30,637 INFO org.spark_project.jetty.server.AbstractConnector [restartedMain] Started ServerConnector@5d94c86c{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2023-04-26 11:48:30,638 INFO org.apache.spark.util.Utils [restartedMain] Successfully started service 'SparkUI' on port 4040.
2023-04-26 11:48:30,639 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@58ebaa44{/jobs,null,AVAILABLE,@Spark}
2023-04-26 11:48:30,640 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@e459b4d{/jobs/json,null,AVAILABLE,@Spark}
2023-04-26 11:48:30,641 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@68e95542{/jobs/job,null,AVAILABLE,@Spark}
2023-04-26 11:48:30,642 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@13c5379b{/jobs/job/json,null,AVAILABLE,@Spark}
2023-04-26 11:48:30,644 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@16f9062d{/stages,null,AVAILABLE,@Spark}
2023-04-26 11:48:30,647 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@3444784c{/stages/json,null,AVAILABLE,@Spark}
2023-04-26 11:48:30,648 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@16a568c3{/stages/stage,null,AVAILABLE,@Spark}
2023-04-26 11:48:30,650 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@2ce6fc63{/stages/stage/json,null,AVAILABLE,@Spark}
2023-04-26 11:48:30,651 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@72705fbc{/stages/pool,null,AVAILABLE,@Spark}
2023-04-26 11:48:30,652 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@6561cfa6{/stages/pool/json,null,AVAILABLE,@Spark}
2023-04-26 11:48:30,653 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@58bf40b9{/storage,null,AVAILABLE,@Spark}
2023-04-26 11:48:30,655 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@43a1ba29{/storage/json,null,AVAILABLE,@Spark}
2023-04-26 11:48:30,656 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@33dc905e{/storage/rdd,null,AVAILABLE,@Spark}
2023-04-26 11:48:30,657 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@582b757c{/storage/rdd/json,null,AVAILABLE,@Spark}
2023-04-26 11:48:30,658 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@7ab35c48{/environment,null,AVAILABLE,@Spark}
2023-04-26 11:48:30,659 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@340dc1c5{/environment/json,null,AVAILABLE,@Spark}
2023-04-26 11:48:30,660 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@6384f97a{/executors,null,AVAILABLE,@Spark}
2023-04-26 11:48:30,661 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@6087f0e3{/executors/json,null,AVAILABLE,@Spark}
2023-04-26 11:48:30,661 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@b6b1a7{/executors/threadDump,null,AVAILABLE,@Spark}
2023-04-26 11:48:30,662 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@5aef5b9d{/executors/threadDump/json,null,AVAILABLE,@Spark}
2023-04-26 11:48:30,663 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@1e643693{/static,null,AVAILABLE,@Spark}
2023-04-26 11:48:30,664 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@413bcd54{/,null,AVAILABLE,@Spark}
2023-04-26 11:48:30,665 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@74b5675d{/api,null,AVAILABLE,@Spark}
2023-04-26 11:48:30,667 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@6bc99d4c{/jobs/job/kill,null,AVAILABLE,@Spark}
2023-04-26 11:48:30,668 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@7487bc0c{/stages/stage/kill,null,AVAILABLE,@Spark}
2023-04-26 11:48:30,668 INFO org.apache.spark.ui.SparkUI [restartedMain] Bound SparkUI to 0.0.0.0, and started at http://192.168.1.125:4040
2023-04-26 11:48:30,705 INFO org.apache.spark.executor.Executor [restartedMain] Starting executor ID driver on host localhost
2023-04-26 11:48:30,713 INFO org.apache.spark.util.Utils [restartedMain] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46789.
2023-04-26 11:48:30,714 INFO org.apache.spark.network.netty.NettyBlockTransferService [restartedMain] Server created on 192.168.1.125:46789
2023-04-26 11:48:30,714 INFO org.apache.spark.storage.BlockManager [restartedMain] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2023-04-26 11:48:30,714 INFO org.apache.spark.storage.BlockManagerMaster [restartedMain] Registering BlockManager BlockManagerId(driver, 192.168.1.125, 46789, None)
2023-04-26 11:48:30,715 INFO org.apache.spark.storage.BlockManagerMasterEndpoint [dispatcher-event-loop-3] Registering block manager 192.168.1.125:46789 with 998.4 MB RAM, BlockManagerId(driver, 192.168.1.125, 46789, None)
2023-04-26 11:48:30,717 INFO org.apache.spark.storage.BlockManagerMaster [restartedMain] Registered BlockManager BlockManagerId(driver, 192.168.1.125, 46789, None)
2023-04-26 11:48:30,717 INFO org.apache.spark.storage.BlockManager [restartedMain] Initialized BlockManager: BlockManagerId(driver, 192.168.1.125, 46789, None)
2023-04-26 11:48:30,719 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@2d13bb48{/metrics/json,null,AVAILABLE,@Spark}
2023-04-26 11:48:32,492 INFO org.apache.kafka.clients.consumer.ConsumerConfig [restartedMain] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-book-group-4
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = book-group
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2023-04-26 11:48:32,496 INFO org.apache.kafka.common.utils.AppInfoParser [restartedMain] Kafka version: 3.1.1
2023-04-26 11:48:32,497 INFO org.apache.kafka.common.utils.AppInfoParser [restartedMain] Kafka commitId: 97671528ba54a138
2023-04-26 11:48:32,497 INFO org.apache.kafka.common.utils.AppInfoParser [restartedMain] Kafka startTimeMs: 1682489912496
2023-04-26 11:48:32,498 INFO org.apache.kafka.clients.consumer.KafkaConsumer [restartedMain] [Consumer clientId=consumer-book-group-4, groupId=book-group] Subscribed to topic(s): my-topic
2023-04-26 11:48:32,504 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-4, groupId=book-group] Resetting the last seen epoch of partition my-topic-0 to 0 since the associated topicId changed from null to QqwJ2XdJQbi2BDXQbzT51g
2023-04-26 11:48:32,505 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-4, groupId=book-group] Cluster ID: 6CnV1ouTQjGsP-r9Rd2WRA
2023-04-26 11:48:32,506 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-4, groupId=book-group] Discovered group coordinator wks-012:9092 (id: 2147483647 rack: null)
2023-04-26 11:48:32,507 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-4, groupId=book-group] (Re-)joining group
2023-04-26 11:48:32,513 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] Started SpringBootSecurityJwtMongodbApplication in 3.791 seconds (JVM running for 384.746)
2023-04-26 11:48:32,514 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-4, groupId=book-group] Request joining group due to: need to re-join with the given member-id
2023-04-26 11:48:32,514 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-4, groupId=book-group] (Re-)joining group
2023-04-26 11:48:32,518 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] This is a info message
2023-04-26 11:48:32,518 WARN com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] This is a warn message
2023-04-26 11:48:32,520 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-4, groupId=book-group] Successfully joined group with generation Generation{generationId=137, memberId='consumer-book-group-4-700fef6a-07f9-4334-adb5-4d46a53e5324', protocol='range'}
2023-04-26 11:48:32,521 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-4, groupId=book-group] Finished assignment for group at generation 137: {consumer-book-group-4-700fef6a-07f9-4334-adb5-4d46a53e5324=Assignment(partitions=[my-topic-0])}
2023-04-26 11:48:32,531 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-4, groupId=book-group] Successfully synced group in generation Generation{generationId=137, memberId='consumer-book-group-4-700fef6a-07f9-4334-adb5-4d46a53e5324', protocol='range'}
2023-04-26 11:48:32,532 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-4, groupId=book-group] Notifying assignor about the new Assignment(partitions=[my-topic-0])
2023-04-26 11:48:32,532 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-4, groupId=book-group] Adding newly assigned partitions: my-topic-0
2023-04-26 11:48:32,535 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-4, groupId=book-group] Setting offset for partition my-topic-0 to the committed offset FetchPosition{offset=415, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[wks-012:9092 (id: 0 rack: null)], epoch=0}}
2023-04-26 11:49:21,075 INFO org.apache.catalina.core.StandardService [Thread-53] Stopping service [Tomcat]
2023-04-26 11:49:21,084 WARN org.apache.catalina.loader.WebappClassLoaderBase [Thread-53] The web application [ROOT] appears to have started a thread named [cluster-ClusterId{value='6448c235adee62176f29075d', description='null'}-localhost:27017] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/jdk.internal.misc.Unsafe.park(Native Method)
 java.base@17.0.6/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:252)
 java.base@17.0.6/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1672)
 app//com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.waitForSignalOrTimeout(DefaultServerMonitor.java:300)
 app//com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.waitForNext(DefaultServerMonitor.java:281)
 app//com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:179)
 java.base@17.0.6/java.lang.Thread.run(Thread.java:833)
2023-04-26 11:49:21,085 WARN org.apache.catalina.loader.WebappClassLoaderBase [Thread-53] The web application [ROOT] appears to have started a thread named [cluster-rtt-ClusterId{value='6448c235adee62176f29075d', description='null'}-localhost:27017] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/java.lang.Thread.sleep(Native Method)
 app//com.mongodb.internal.connection.DefaultServerMonitor.waitForNext(DefaultServerMonitor.java:443)
 app//com.mongodb.internal.connection.DefaultServerMonitor.access$1500(DefaultServerMonitor.java:64)
 app//com.mongodb.internal.connection.DefaultServerMonitor$RoundTripTimeRunnable.run(DefaultServerMonitor.java:415)
 java.base@17.0.6/java.lang.Thread.run(Thread.java:833)
2023-04-26 11:49:21,086 WARN org.apache.catalina.loader.WebappClassLoaderBase [Thread-53] The web application [ROOT] appears to have started a thread named [MaintenanceTimer-5-thread-1] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/jdk.internal.misc.Unsafe.park(Native Method)
 java.base@17.0.6/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:252)
 java.base@17.0.6/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1672)
 java.base@17.0.6/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182)
 java.base@17.0.6/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1062)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1122)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
 java.base@17.0.6/java.lang.Thread.run(Thread.java:833)
2023-04-26 11:49:21,097 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-4, groupId=book-group] Revoke previously assigned partitions my-topic-0
2023-04-26 11:49:21,098 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-4, groupId=book-group] Member consumer-book-group-4-700fef6a-07f9-4334-adb5-4d46a53e5324 sending LeaveGroup request to coordinator wks-012:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2023-04-26 11:49:21,101 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-4, groupId=book-group] Resetting generation due to: consumer pro-actively leaving the group
2023-04-26 11:49:21,102 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-4, groupId=book-group] Request joining group due to: consumer pro-actively leaving the group
2023-04-26 11:49:21,102 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-4, groupId=book-group] Unsubscribed all topics or patterns and assigned partitions
2023-04-26 11:49:21,103 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-4, groupId=book-group] Resetting generation due to: consumer pro-actively leaving the group
2023-04-26 11:49:21,103 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-4, groupId=book-group] Request joining group due to: consumer pro-actively leaving the group
2023-04-26 11:49:21,104 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Metrics scheduler closed
2023-04-26 11:49:21,104 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-04-26 11:49:21,104 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Metrics reporters closed
2023-04-26 11:49:21,107 INFO org.apache.kafka.common.utils.AppInfoParser [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] App info kafka.consumer for consumer-book-group-4 unregistered
2023-04-26 11:49:21,121 INFO org.spark_project.jetty.server.AbstractConnector [Thread-53] Stopped Spark@5d94c86c{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2023-04-26 11:49:21,123 INFO org.apache.spark.ui.SparkUI [Thread-53] Stopped Spark web UI at http://192.168.1.125:4040
2023-04-26 11:49:21,126 INFO org.apache.spark.MapOutputTrackerMasterEndpoint [dispatcher-event-loop-3] MapOutputTrackerMasterEndpoint stopped!
2023-04-26 11:49:21,143 INFO org.apache.spark.storage.memory.MemoryStore [Thread-53] MemoryStore cleared
2023-04-26 11:49:21,144 INFO org.apache.spark.storage.BlockManager [Thread-53] BlockManager stopped
2023-04-26 11:49:21,144 INFO org.apache.spark.storage.BlockManagerMaster [Thread-53] BlockManagerMaster stopped
2023-04-26 11:49:21,145 INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [dispatcher-event-loop-2] OutputCommitCoordinator stopped!
2023-04-26 11:49:21,153 INFO org.apache.spark.SparkContext [Thread-53] Successfully stopped SparkContext
2023-04-26 11:49:21,153 INFO org.apache.spark.SparkContext [Thread-53] SparkContext already stopped.
2023-04-26 11:49:21,156 INFO com.zaxxer.hikari.HikariDataSource [Thread-53] HikariPool-4 - Shutdown initiated...
2023-04-26 11:49:21,159 INFO com.zaxxer.hikari.HikariDataSource [Thread-53] HikariPool-4 - Shutdown completed.
2023-04-26 11:49:21,510 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] Starting SpringBootSecurityJwtMongodbApplication using Java 17.0.6 on wks-012 with PID 102147 (/home/inferyx/git/SpringApplicationWithSecurity/target/classes started by inferyx in /home/inferyx/git/SpringApplicationWithSecurity)
2023-04-26 11:49:21,511 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] No active profile set, falling back to 1 default profile: "default"
2023-04-26 11:49:22,349 INFO org.apache.catalina.core.StandardService [restartedMain] Starting service [Tomcat]
2023-04-26 11:49:22,349 INFO org.apache.catalina.core.StandardEngine [restartedMain] Starting Servlet engine: [Apache Tomcat/9.0.65]
2023-04-26 11:49:22,379 INFO org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] [restartedMain] Initializing Spring embedded WebApplicationContext
2023-04-26 11:49:22,434 INFO org.mongodb.driver.client [restartedMain] MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync|spring-boot", "version": "4.6.1"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "5.15.0-69-generic"}, "platform": "Java/Eclipse Adoptium/17.0.6+10"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.Jep395RecordCodecProvider@269fb271]}, clusterSettings={hosts=[localhost:27017], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='30000 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, sendBufferSize=0}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, sendBufferSize=0}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=JAVA_LEGACY, serverApi=null, autoEncryptionSettings=null, contextProvider=null}
2023-04-26 11:49:22,436 INFO org.mongodb.driver.connection [cluster-ClusterId{value='6448c26aadee62176f29075e', description='null'}-localhost:27017] Opened connection [connectionId{localValue:10, serverValue:51}] to localhost:27017
2023-04-26 11:49:22,436 INFO org.mongodb.driver.cluster [cluster-ClusterId{value='6448c26aadee62176f29075e', description='null'}-localhost:27017] Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=6, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1562409}
2023-04-26 11:49:22,437 INFO org.mongodb.driver.connection [cluster-rtt-ClusterId{value='6448c26aadee62176f29075e', description='null'}-localhost:27017] Opened connection [connectionId{localValue:11, serverValue:52}] to localhost:27017
2023-04-26 11:49:22,656 INFO org.hibernate.jpa.internal.util.LogHelper [restartedMain] HHH000204: Processing PersistenceUnitInfo [name: default]
2023-04-26 11:49:22,674 INFO com.zaxxer.hikari.HikariDataSource [restartedMain] HikariPool-5 - Starting...
2023-04-26 11:49:22,699 INFO com.zaxxer.hikari.HikariDataSource [restartedMain] HikariPool-5 - Start completed.
2023-04-26 11:49:22,700 INFO org.hibernate.dialect.Dialect [restartedMain] HHH000400: Using dialect: org.hibernate.dialect.MySQL5Dialect
2023-04-26 11:49:22,880 INFO org.hibernate.engine.transaction.jta.platform.internal.JtaPlatformInitiator [restartedMain] HHH000490: Using JtaPlatform implementation: [org.hibernate.engine.transaction.jta.platform.internal.NoJtaPlatform]
2023-04-26 11:49:23,245 INFO org.apache.spark.SparkContext [restartedMain] Running Spark version 2.4.5
2023-04-26 11:49:23,248 INFO org.apache.spark.SparkContext [restartedMain] Submitted application: MyAppName
2023-04-26 11:49:23,250 INFO org.apache.spark.SecurityManager [restartedMain] Changing view acls to: inferyx
2023-04-26 11:49:23,251 INFO org.apache.spark.SecurityManager [restartedMain] Changing modify acls to: inferyx
2023-04-26 11:49:23,251 INFO org.apache.spark.SecurityManager [restartedMain] Changing view acls groups to: 
2023-04-26 11:49:23,252 INFO org.apache.spark.SecurityManager [restartedMain] Changing modify acls groups to: 
2023-04-26 11:49:23,252 INFO org.apache.spark.SecurityManager [restartedMain] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(inferyx); groups with view permissions: Set(); users  with modify permissions: Set(inferyx); groups with modify permissions: Set()
2023-04-26 11:49:23,336 INFO org.apache.spark.util.Utils [restartedMain] Successfully started service 'sparkDriver' on port 45505.
2023-04-26 11:49:23,343 INFO org.apache.spark.SparkEnv [restartedMain] Registering MapOutputTracker
2023-04-26 11:49:23,344 INFO org.apache.spark.SparkEnv [restartedMain] Registering BlockManagerMaster
2023-04-26 11:49:23,345 INFO org.apache.spark.storage.BlockManagerMasterEndpoint [restartedMain] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2023-04-26 11:49:23,346 INFO org.apache.spark.storage.BlockManagerMasterEndpoint [restartedMain] BlockManagerMasterEndpoint up
2023-04-26 11:49:23,347 INFO org.apache.spark.storage.DiskBlockManager [restartedMain] Created local directory at /tmp/blockmgr-1098ad2c-6946-4ec6-80e8-e29c678b59e8
2023-04-26 11:49:23,348 INFO org.apache.spark.storage.memory.MemoryStore [restartedMain] MemoryStore started with capacity 998.4 MB
2023-04-26 11:49:23,352 INFO org.apache.spark.SparkEnv [restartedMain] Registering OutputCommitCoordinator
2023-04-26 11:49:23,362 INFO org.spark_project.jetty.server.Server [restartedMain] jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2023-04-26 11:49:23,365 INFO org.spark_project.jetty.server.Server [restartedMain] Started @435598ms
2023-04-26 11:49:23,366 INFO org.spark_project.jetty.server.AbstractConnector [restartedMain] Started ServerConnector@7f090bd0{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2023-04-26 11:49:23,367 INFO org.apache.spark.util.Utils [restartedMain] Successfully started service 'SparkUI' on port 4040.
2023-04-26 11:49:23,368 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@5adadacd{/jobs,null,AVAILABLE,@Spark}
2023-04-26 11:49:23,369 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@2fddecb2{/jobs/json,null,AVAILABLE,@Spark}
2023-04-26 11:49:23,370 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@6c2749b6{/jobs/job,null,AVAILABLE,@Spark}
2023-04-26 11:49:23,376 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@5bc79869{/jobs/job/json,null,AVAILABLE,@Spark}
2023-04-26 11:49:23,378 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@57473d6e{/stages,null,AVAILABLE,@Spark}
2023-04-26 11:49:23,379 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@173ff348{/stages/json,null,AVAILABLE,@Spark}
2023-04-26 11:49:23,380 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@410372e0{/stages/stage,null,AVAILABLE,@Spark}
2023-04-26 11:49:23,381 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@26e0be79{/stages/stage/json,null,AVAILABLE,@Spark}
2023-04-26 11:49:23,382 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@48225b86{/stages/pool,null,AVAILABLE,@Spark}
2023-04-26 11:49:23,383 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@20ef4479{/stages/pool/json,null,AVAILABLE,@Spark}
2023-04-26 11:49:23,388 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@7aa1a6f6{/storage,null,AVAILABLE,@Spark}
2023-04-26 11:49:23,391 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@5991dcad{/storage/json,null,AVAILABLE,@Spark}
2023-04-26 11:49:23,393 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@3355272{/storage/rdd,null,AVAILABLE,@Spark}
2023-04-26 11:49:23,394 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@59a302e1{/storage/rdd/json,null,AVAILABLE,@Spark}
2023-04-26 11:49:23,403 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@2ec29935{/environment,null,AVAILABLE,@Spark}
2023-04-26 11:49:23,407 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@79e15cf4{/environment/json,null,AVAILABLE,@Spark}
2023-04-26 11:49:23,408 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@506e3778{/executors,null,AVAILABLE,@Spark}
2023-04-26 11:49:23,409 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@111b839e{/executors/json,null,AVAILABLE,@Spark}
2023-04-26 11:49:23,411 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@2b2aabb9{/executors/threadDump,null,AVAILABLE,@Spark}
2023-04-26 11:49:23,412 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@585f44d4{/executors/threadDump/json,null,AVAILABLE,@Spark}
2023-04-26 11:49:23,414 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@79ab7d50{/static,null,AVAILABLE,@Spark}
2023-04-26 11:49:23,415 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@34e11688{/,null,AVAILABLE,@Spark}
2023-04-26 11:49:23,416 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@4a6814ce{/api,null,AVAILABLE,@Spark}
2023-04-26 11:49:23,417 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@4ad03118{/jobs/job/kill,null,AVAILABLE,@Spark}
2023-04-26 11:49:23,418 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@76392b9c{/stages/stage/kill,null,AVAILABLE,@Spark}
2023-04-26 11:49:23,419 INFO org.apache.spark.ui.SparkUI [restartedMain] Bound SparkUI to 0.0.0.0, and started at http://192.168.1.125:4040
2023-04-26 11:49:23,457 INFO org.apache.spark.executor.Executor [restartedMain] Starting executor ID driver on host localhost
2023-04-26 11:49:23,465 INFO org.apache.spark.util.Utils [restartedMain] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39617.
2023-04-26 11:49:23,466 INFO org.apache.spark.network.netty.NettyBlockTransferService [restartedMain] Server created on 192.168.1.125:39617
2023-04-26 11:49:23,466 INFO org.apache.spark.storage.BlockManager [restartedMain] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2023-04-26 11:49:23,467 INFO org.apache.spark.storage.BlockManagerMaster [restartedMain] Registering BlockManager BlockManagerId(driver, 192.168.1.125, 39617, None)
2023-04-26 11:49:23,468 INFO org.apache.spark.storage.BlockManagerMasterEndpoint [dispatcher-event-loop-2] Registering block manager 192.168.1.125:39617 with 998.4 MB RAM, BlockManagerId(driver, 192.168.1.125, 39617, None)
2023-04-26 11:49:23,468 INFO org.apache.spark.storage.BlockManagerMaster [restartedMain] Registered BlockManager BlockManagerId(driver, 192.168.1.125, 39617, None)
2023-04-26 11:49:23,469 INFO org.apache.spark.storage.BlockManager [restartedMain] Initialized BlockManager: BlockManagerId(driver, 192.168.1.125, 39617, None)
2023-04-26 11:49:23,471 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@47d8da6a{/metrics/json,null,AVAILABLE,@Spark}
2023-04-26 11:49:25,150 INFO org.apache.kafka.clients.consumer.ConsumerConfig [restartedMain] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-book-group-5
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = book-group
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2023-04-26 11:49:25,157 INFO org.apache.kafka.common.utils.AppInfoParser [restartedMain] Kafka version: 3.1.1
2023-04-26 11:49:25,157 INFO org.apache.kafka.common.utils.AppInfoParser [restartedMain] Kafka commitId: 97671528ba54a138
2023-04-26 11:49:25,157 INFO org.apache.kafka.common.utils.AppInfoParser [restartedMain] Kafka startTimeMs: 1682489965157
2023-04-26 11:49:25,159 INFO org.apache.kafka.clients.consumer.KafkaConsumer [restartedMain] [Consumer clientId=consumer-book-group-5, groupId=book-group] Subscribed to topic(s): my-topic
2023-04-26 11:49:25,168 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-5, groupId=book-group] Resetting the last seen epoch of partition my-topic-0 to 0 since the associated topicId changed from null to QqwJ2XdJQbi2BDXQbzT51g
2023-04-26 11:49:25,169 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-5, groupId=book-group] Cluster ID: 6CnV1ouTQjGsP-r9Rd2WRA
2023-04-26 11:49:25,169 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-5, groupId=book-group] Discovered group coordinator wks-012:9092 (id: 2147483647 rack: null)
2023-04-26 11:49:25,178 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-5, groupId=book-group] (Re-)joining group
2023-04-26 11:49:25,183 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-5, groupId=book-group] Request joining group due to: need to re-join with the given member-id
2023-04-26 11:49:25,183 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-5, groupId=book-group] (Re-)joining group
2023-04-26 11:49:25,188 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] Started SpringBootSecurityJwtMongodbApplication in 3.771 seconds (JVM running for 437.421)
2023-04-26 11:49:25,194 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-5, groupId=book-group] Successfully joined group with generation Generation{generationId=139, memberId='consumer-book-group-5-96428b7d-20c8-4cf3-88cf-7534bdff645e', protocol='range'}
2023-04-26 11:49:25,194 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-5, groupId=book-group] Finished assignment for group at generation 139: {consumer-book-group-5-96428b7d-20c8-4cf3-88cf-7534bdff645e=Assignment(partitions=[my-topic-0])}
2023-04-26 11:49:25,197 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-5, groupId=book-group] Successfully synced group in generation Generation{generationId=139, memberId='consumer-book-group-5-96428b7d-20c8-4cf3-88cf-7534bdff645e', protocol='range'}
2023-04-26 11:49:25,198 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-5, groupId=book-group] Notifying assignor about the new Assignment(partitions=[my-topic-0])
2023-04-26 11:49:25,198 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] This is a info message
2023-04-26 11:49:25,198 WARN com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] This is a warn message
2023-04-26 11:49:25,198 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-5, groupId=book-group] Adding newly assigned partitions: my-topic-0
2023-04-26 11:49:25,206 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-5, groupId=book-group] Setting offset for partition my-topic-0 to the committed offset FetchPosition{offset=415, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[wks-012:9092 (id: 0 rack: null)], epoch=0}}
2023-04-26 11:49:34,576 INFO org.apache.catalina.core.StandardService [Thread-69] Stopping service [Tomcat]
2023-04-26 11:49:34,589 WARN org.apache.catalina.loader.WebappClassLoaderBase [Thread-69] The web application [ROOT] appears to have started a thread named [cluster-ClusterId{value='6448c26aadee62176f29075e', description='null'}-localhost:27017] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/jdk.internal.misc.Unsafe.park(Native Method)
 java.base@17.0.6/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:252)
 java.base@17.0.6/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1672)
 app//com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.waitForSignalOrTimeout(DefaultServerMonitor.java:300)
 app//com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.waitForNext(DefaultServerMonitor.java:281)
 app//com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:179)
 java.base@17.0.6/java.lang.Thread.run(Thread.java:833)
2023-04-26 11:49:34,592 WARN org.apache.catalina.loader.WebappClassLoaderBase [Thread-69] The web application [ROOT] appears to have started a thread named [cluster-rtt-ClusterId{value='6448c26aadee62176f29075e', description='null'}-localhost:27017] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/java.lang.Thread.sleep(Native Method)
 app//com.mongodb.internal.connection.DefaultServerMonitor.waitForNext(DefaultServerMonitor.java:443)
 app//com.mongodb.internal.connection.DefaultServerMonitor.access$1500(DefaultServerMonitor.java:64)
 app//com.mongodb.internal.connection.DefaultServerMonitor$RoundTripTimeRunnable.run(DefaultServerMonitor.java:415)
 java.base@17.0.6/java.lang.Thread.run(Thread.java:833)
2023-04-26 11:49:34,593 WARN org.apache.catalina.loader.WebappClassLoaderBase [Thread-69] The web application [ROOT] appears to have started a thread named [MaintenanceTimer-6-thread-1] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/jdk.internal.misc.Unsafe.park(Native Method)
 java.base@17.0.6/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:252)
 java.base@17.0.6/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1672)
 java.base@17.0.6/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182)
 java.base@17.0.6/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1062)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1122)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
 java.base@17.0.6/java.lang.Thread.run(Thread.java:833)
2023-04-26 11:49:34,599 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-5, groupId=book-group] Revoke previously assigned partitions my-topic-0
2023-04-26 11:49:34,600 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-5, groupId=book-group] Member consumer-book-group-5-96428b7d-20c8-4cf3-88cf-7534bdff645e sending LeaveGroup request to coordinator wks-012:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2023-04-26 11:49:34,609 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-5, groupId=book-group] Resetting generation due to: consumer pro-actively leaving the group
2023-04-26 11:49:34,609 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-5, groupId=book-group] Request joining group due to: consumer pro-actively leaving the group
2023-04-26 11:49:34,610 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-5, groupId=book-group] Unsubscribed all topics or patterns and assigned partitions
2023-04-26 11:49:34,610 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-5, groupId=book-group] Resetting generation due to: consumer pro-actively leaving the group
2023-04-26 11:49:34,610 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-5, groupId=book-group] Request joining group due to: consumer pro-actively leaving the group
2023-04-26 11:49:34,611 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Metrics scheduler closed
2023-04-26 11:49:34,611 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-04-26 11:49:34,612 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Metrics reporters closed
2023-04-26 11:49:34,624 INFO org.apache.kafka.common.utils.AppInfoParser [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] App info kafka.consumer for consumer-book-group-5 unregistered
2023-04-26 11:49:34,645 INFO org.spark_project.jetty.server.AbstractConnector [Thread-69] Stopped Spark@7f090bd0{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2023-04-26 11:49:34,649 INFO org.apache.spark.ui.SparkUI [Thread-69] Stopped Spark web UI at http://192.168.1.125:4040
2023-04-26 11:49:34,655 INFO org.apache.spark.MapOutputTrackerMasterEndpoint [dispatcher-event-loop-1] MapOutputTrackerMasterEndpoint stopped!
2023-04-26 11:49:34,706 INFO org.apache.spark.storage.memory.MemoryStore [Thread-69] MemoryStore cleared
2023-04-26 11:49:34,707 INFO org.apache.spark.storage.BlockManager [Thread-69] BlockManager stopped
2023-04-26 11:49:34,707 INFO org.apache.spark.storage.BlockManagerMaster [Thread-69] BlockManagerMaster stopped
2023-04-26 11:49:34,710 INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [dispatcher-event-loop-3] OutputCommitCoordinator stopped!
2023-04-26 11:49:34,747 INFO org.apache.spark.SparkContext [Thread-69] Successfully stopped SparkContext
2023-04-26 11:49:34,748 INFO org.apache.spark.SparkContext [Thread-69] SparkContext already stopped.
2023-04-26 11:49:34,750 INFO com.zaxxer.hikari.HikariDataSource [Thread-69] HikariPool-5 - Shutdown initiated...
2023-04-26 11:49:34,764 INFO com.zaxxer.hikari.HikariDataSource [Thread-69] HikariPool-5 - Shutdown completed.
2023-04-26 11:49:35,119 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] Starting SpringBootSecurityJwtMongodbApplication using Java 17.0.6 on wks-012 with PID 102147 (/home/inferyx/git/SpringApplicationWithSecurity/target/classes started by inferyx in /home/inferyx/git/SpringApplicationWithSecurity)
2023-04-26 11:49:35,119 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] No active profile set, falling back to 1 default profile: "default"
2023-04-26 11:49:35,815 INFO org.apache.catalina.core.StandardService [restartedMain] Starting service [Tomcat]
2023-04-26 11:49:35,816 INFO org.apache.catalina.core.StandardEngine [restartedMain] Starting Servlet engine: [Apache Tomcat/9.0.65]
2023-04-26 11:49:35,842 INFO org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] [restartedMain] Initializing Spring embedded WebApplicationContext
2023-04-26 11:49:35,889 INFO org.mongodb.driver.client [restartedMain] MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync|spring-boot", "version": "4.6.1"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "5.15.0-69-generic"}, "platform": "Java/Eclipse Adoptium/17.0.6+10"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.Jep395RecordCodecProvider@269fb271]}, clusterSettings={hosts=[localhost:27017], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='30000 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, sendBufferSize=0}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, sendBufferSize=0}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=JAVA_LEGACY, serverApi=null, autoEncryptionSettings=null, contextProvider=null}
2023-04-26 11:49:35,891 INFO org.mongodb.driver.connection [cluster-rtt-ClusterId{value='6448c277adee62176f29075f', description='null'}-localhost:27017] Opened connection [connectionId{localValue:13, serverValue:54}] to localhost:27017
2023-04-26 11:49:35,893 INFO org.mongodb.driver.connection [cluster-ClusterId{value='6448c277adee62176f29075f', description='null'}-localhost:27017] Opened connection [connectionId{localValue:12, serverValue:53}] to localhost:27017
2023-04-26 11:49:35,893 INFO org.mongodb.driver.cluster [cluster-ClusterId{value='6448c277adee62176f29075f', description='null'}-localhost:27017] Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=6, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=2484954}
2023-04-26 11:49:36,113 INFO org.hibernate.jpa.internal.util.LogHelper [restartedMain] HHH000204: Processing PersistenceUnitInfo [name: default]
2023-04-26 11:49:36,133 INFO com.zaxxer.hikari.HikariDataSource [restartedMain] HikariPool-6 - Starting...
2023-04-26 11:49:36,137 INFO com.zaxxer.hikari.HikariDataSource [restartedMain] HikariPool-6 - Start completed.
2023-04-26 11:49:36,138 INFO org.hibernate.dialect.Dialect [restartedMain] HHH000400: Using dialect: org.hibernate.dialect.MySQL5Dialect
2023-04-26 11:49:36,264 INFO org.hibernate.engine.transaction.jta.platform.internal.JtaPlatformInitiator [restartedMain] HHH000490: Using JtaPlatform implementation: [org.hibernate.engine.transaction.jta.platform.internal.NoJtaPlatform]
2023-04-26 11:49:36,557 INFO org.apache.spark.SparkContext [restartedMain] Running Spark version 2.4.5
2023-04-26 11:49:36,558 INFO org.apache.spark.SparkContext [restartedMain] Submitted application: MyAppName
2023-04-26 11:49:36,560 INFO org.apache.spark.SecurityManager [restartedMain] Changing view acls to: inferyx
2023-04-26 11:49:36,560 INFO org.apache.spark.SecurityManager [restartedMain] Changing modify acls to: inferyx
2023-04-26 11:49:36,561 INFO org.apache.spark.SecurityManager [restartedMain] Changing view acls groups to: 
2023-04-26 11:49:36,561 INFO org.apache.spark.SecurityManager [restartedMain] Changing modify acls groups to: 
2023-04-26 11:49:36,561 INFO org.apache.spark.SecurityManager [restartedMain] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(inferyx); groups with view permissions: Set(); users  with modify permissions: Set(inferyx); groups with modify permissions: Set()
2023-04-26 11:49:36,643 INFO org.apache.spark.util.Utils [restartedMain] Successfully started service 'sparkDriver' on port 40271.
2023-04-26 11:49:36,646 INFO org.apache.spark.SparkEnv [restartedMain] Registering MapOutputTracker
2023-04-26 11:49:36,648 INFO org.apache.spark.SparkEnv [restartedMain] Registering BlockManagerMaster
2023-04-26 11:49:36,648 INFO org.apache.spark.storage.BlockManagerMasterEndpoint [restartedMain] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2023-04-26 11:49:36,649 INFO org.apache.spark.storage.BlockManagerMasterEndpoint [restartedMain] BlockManagerMasterEndpoint up
2023-04-26 11:49:36,649 INFO org.apache.spark.storage.DiskBlockManager [restartedMain] Created local directory at /tmp/blockmgr-1770c861-6bc7-4491-b3db-26d428f32f71
2023-04-26 11:49:36,650 INFO org.apache.spark.storage.memory.MemoryStore [restartedMain] MemoryStore started with capacity 998.4 MB
2023-04-26 11:49:36,652 INFO org.apache.spark.SparkEnv [restartedMain] Registering OutputCommitCoordinator
2023-04-26 11:49:36,658 INFO org.spark_project.jetty.server.Server [restartedMain] jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2023-04-26 11:49:36,660 INFO org.spark_project.jetty.server.Server [restartedMain] Started @448893ms
2023-04-26 11:49:36,662 INFO org.spark_project.jetty.server.AbstractConnector [restartedMain] Started ServerConnector@7b630c84{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2023-04-26 11:49:36,662 INFO org.apache.spark.util.Utils [restartedMain] Successfully started service 'SparkUI' on port 4040.
2023-04-26 11:49:36,663 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@6108d83c{/jobs,null,AVAILABLE,@Spark}
2023-04-26 11:49:36,664 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@5793be82{/jobs/json,null,AVAILABLE,@Spark}
2023-04-26 11:49:36,665 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@67992066{/jobs/job,null,AVAILABLE,@Spark}
2023-04-26 11:49:36,666 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@4fd173d{/jobs/job/json,null,AVAILABLE,@Spark}
2023-04-26 11:49:36,667 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@2393f6a6{/stages,null,AVAILABLE,@Spark}
2023-04-26 11:49:36,668 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@693e3bf8{/stages/json,null,AVAILABLE,@Spark}
2023-04-26 11:49:36,668 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@478aaca9{/stages/stage,null,AVAILABLE,@Spark}
2023-04-26 11:49:36,669 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@2ba20f9a{/stages/stage/json,null,AVAILABLE,@Spark}
2023-04-26 11:49:36,670 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@1738fa8e{/stages/pool,null,AVAILABLE,@Spark}
2023-04-26 11:49:36,671 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@fc1e26a{/stages/pool/json,null,AVAILABLE,@Spark}
2023-04-26 11:49:36,672 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@1dbce8df{/storage,null,AVAILABLE,@Spark}
2023-04-26 11:49:36,673 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@26d9fa2d{/storage/json,null,AVAILABLE,@Spark}
2023-04-26 11:49:36,674 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@76e598f2{/storage/rdd,null,AVAILABLE,@Spark}
2023-04-26 11:49:36,675 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@7637e6f6{/storage/rdd/json,null,AVAILABLE,@Spark}
2023-04-26 11:49:36,676 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@cb712ff{/environment,null,AVAILABLE,@Spark}
2023-04-26 11:49:36,677 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@655699d6{/environment/json,null,AVAILABLE,@Spark}
2023-04-26 11:49:36,677 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@3faebf86{/executors,null,AVAILABLE,@Spark}
2023-04-26 11:49:36,678 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@76e7fed3{/executors/json,null,AVAILABLE,@Spark}
2023-04-26 11:49:36,679 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@42f32a1a{/executors/threadDump,null,AVAILABLE,@Spark}
2023-04-26 11:49:36,679 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@13ba83bb{/executors/threadDump/json,null,AVAILABLE,@Spark}
2023-04-26 11:49:36,680 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@59225cc5{/static,null,AVAILABLE,@Spark}
2023-04-26 11:49:36,681 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@10fa3054{/,null,AVAILABLE,@Spark}
2023-04-26 11:49:36,682 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@4b026bab{/api,null,AVAILABLE,@Spark}
2023-04-26 11:49:36,683 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@476d3f16{/jobs/job/kill,null,AVAILABLE,@Spark}
2023-04-26 11:49:36,684 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@6d4df3f{/stages/stage/kill,null,AVAILABLE,@Spark}
2023-04-26 11:49:36,685 INFO org.apache.spark.ui.SparkUI [restartedMain] Bound SparkUI to 0.0.0.0, and started at http://192.168.1.125:4040
2023-04-26 11:49:36,724 INFO org.apache.spark.executor.Executor [restartedMain] Starting executor ID driver on host localhost
2023-04-26 11:49:36,730 INFO org.apache.spark.util.Utils [restartedMain] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42991.
2023-04-26 11:49:36,730 INFO org.apache.spark.network.netty.NettyBlockTransferService [restartedMain] Server created on 192.168.1.125:42991
2023-04-26 11:49:36,731 INFO org.apache.spark.storage.BlockManager [restartedMain] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2023-04-26 11:49:36,732 INFO org.apache.spark.storage.BlockManagerMaster [restartedMain] Registering BlockManager BlockManagerId(driver, 192.168.1.125, 42991, None)
2023-04-26 11:49:36,734 INFO org.apache.spark.storage.BlockManagerMasterEndpoint [dispatcher-event-loop-2] Registering block manager 192.168.1.125:42991 with 998.4 MB RAM, BlockManagerId(driver, 192.168.1.125, 42991, None)
2023-04-26 11:49:36,735 INFO org.apache.spark.storage.BlockManagerMaster [restartedMain] Registered BlockManager BlockManagerId(driver, 192.168.1.125, 42991, None)
2023-04-26 11:49:36,736 INFO org.apache.spark.storage.BlockManager [restartedMain] Initialized BlockManager: BlockManagerId(driver, 192.168.1.125, 42991, None)
2023-04-26 11:49:36,737 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@37749b75{/metrics/json,null,AVAILABLE,@Spark}
2023-04-26 11:49:38,568 INFO org.apache.kafka.clients.consumer.ConsumerConfig [restartedMain] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-book-group-6
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = book-group
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2023-04-26 11:49:38,575 INFO org.apache.kafka.common.utils.AppInfoParser [restartedMain] Kafka version: 3.1.1
2023-04-26 11:49:38,575 INFO org.apache.kafka.common.utils.AppInfoParser [restartedMain] Kafka commitId: 97671528ba54a138
2023-04-26 11:49:38,576 INFO org.apache.kafka.common.utils.AppInfoParser [restartedMain] Kafka startTimeMs: 1682489978575
2023-04-26 11:49:38,577 INFO org.apache.kafka.clients.consumer.KafkaConsumer [restartedMain] [Consumer clientId=consumer-book-group-6, groupId=book-group] Subscribed to topic(s): my-topic
2023-04-26 11:49:38,597 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-6, groupId=book-group] Resetting the last seen epoch of partition my-topic-0 to 0 since the associated topicId changed from null to QqwJ2XdJQbi2BDXQbzT51g
2023-04-26 11:49:38,598 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-6, groupId=book-group] Cluster ID: 6CnV1ouTQjGsP-r9Rd2WRA
2023-04-26 11:49:38,598 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-6, groupId=book-group] Discovered group coordinator wks-012:9092 (id: 2147483647 rack: null)
2023-04-26 11:49:38,605 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-6, groupId=book-group] (Re-)joining group
2023-04-26 11:49:38,613 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-6, groupId=book-group] Request joining group due to: need to re-join with the given member-id
2023-04-26 11:49:38,613 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-6, groupId=book-group] (Re-)joining group
2023-04-26 11:49:38,615 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] Started SpringBootSecurityJwtMongodbApplication in 3.59 seconds (JVM running for 450.848)
2023-04-26 11:49:38,617 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-6, groupId=book-group] Successfully joined group with generation Generation{generationId=141, memberId='consumer-book-group-6-75a33a13-459f-4108-9c03-9aa2388fbbd2', protocol='range'}
2023-04-26 11:49:38,618 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-6, groupId=book-group] Finished assignment for group at generation 141: {consumer-book-group-6-75a33a13-459f-4108-9c03-9aa2388fbbd2=Assignment(partitions=[my-topic-0])}
2023-04-26 11:49:38,621 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] This is a info message
2023-04-26 11:49:38,621 WARN com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] This is a warn message
2023-04-26 11:49:38,623 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-6, groupId=book-group] Successfully synced group in generation Generation{generationId=141, memberId='consumer-book-group-6-75a33a13-459f-4108-9c03-9aa2388fbbd2', protocol='range'}
2023-04-26 11:49:38,624 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-6, groupId=book-group] Notifying assignor about the new Assignment(partitions=[my-topic-0])
2023-04-26 11:49:38,624 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-6, groupId=book-group] Adding newly assigned partitions: my-topic-0
2023-04-26 11:49:38,635 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-6, groupId=book-group] Setting offset for partition my-topic-0 to the committed offset FetchPosition{offset=415, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[wks-012:9092 (id: 0 rack: null)], epoch=0}}
2023-04-26 11:50:09,067 INFO org.apache.catalina.core.StandardService [Thread-86] Stopping service [Tomcat]
2023-04-26 11:50:09,071 WARN org.apache.catalina.loader.WebappClassLoaderBase [Thread-86] The web application [ROOT] appears to have started a thread named [cluster-ClusterId{value='6448c277adee62176f29075f', description='null'}-localhost:27017] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/jdk.internal.misc.Unsafe.park(Native Method)
 java.base@17.0.6/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:252)
 java.base@17.0.6/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1672)
 app//com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.waitForSignalOrTimeout(DefaultServerMonitor.java:300)
 app//com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.waitForNext(DefaultServerMonitor.java:281)
 app//com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:179)
 java.base@17.0.6/java.lang.Thread.run(Thread.java:833)
2023-04-26 11:50:09,071 WARN org.apache.catalina.loader.WebappClassLoaderBase [Thread-86] The web application [ROOT] appears to have started a thread named [cluster-rtt-ClusterId{value='6448c277adee62176f29075f', description='null'}-localhost:27017] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/java.lang.Thread.sleep(Native Method)
 app//com.mongodb.internal.connection.DefaultServerMonitor.waitForNext(DefaultServerMonitor.java:443)
 app//com.mongodb.internal.connection.DefaultServerMonitor.access$1500(DefaultServerMonitor.java:64)
 app//com.mongodb.internal.connection.DefaultServerMonitor$RoundTripTimeRunnable.run(DefaultServerMonitor.java:415)
 java.base@17.0.6/java.lang.Thread.run(Thread.java:833)
2023-04-26 11:50:09,074 WARN org.apache.catalina.loader.WebappClassLoaderBase [Thread-86] The web application [ROOT] appears to have started a thread named [MaintenanceTimer-7-thread-1] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/jdk.internal.misc.Unsafe.park(Native Method)
 java.base@17.0.6/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:252)
 java.base@17.0.6/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1672)
 java.base@17.0.6/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182)
 java.base@17.0.6/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1062)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1122)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
 java.base@17.0.6/java.lang.Thread.run(Thread.java:833)
2023-04-26 11:50:09,080 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-6, groupId=book-group] Revoke previously assigned partitions my-topic-0
2023-04-26 11:50:09,080 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-6, groupId=book-group] Member consumer-book-group-6-75a33a13-459f-4108-9c03-9aa2388fbbd2 sending LeaveGroup request to coordinator wks-012:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2023-04-26 11:50:09,081 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-6, groupId=book-group] Resetting generation due to: consumer pro-actively leaving the group
2023-04-26 11:50:09,081 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-6, groupId=book-group] Request joining group due to: consumer pro-actively leaving the group
2023-04-26 11:50:09,082 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-6, groupId=book-group] Unsubscribed all topics or patterns and assigned partitions
2023-04-26 11:50:09,082 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-6, groupId=book-group] Resetting generation due to: consumer pro-actively leaving the group
2023-04-26 11:50:09,082 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-6, groupId=book-group] Request joining group due to: consumer pro-actively leaving the group
2023-04-26 11:50:09,086 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Metrics scheduler closed
2023-04-26 11:50:09,086 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-04-26 11:50:09,087 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Metrics reporters closed
2023-04-26 11:50:09,096 INFO org.apache.kafka.common.utils.AppInfoParser [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] App info kafka.consumer for consumer-book-group-6 unregistered
2023-04-26 11:50:09,104 INFO org.spark_project.jetty.server.AbstractConnector [Thread-86] Stopped Spark@7b630c84{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2023-04-26 11:50:09,106 INFO org.apache.spark.ui.SparkUI [Thread-86] Stopped Spark web UI at http://192.168.1.125:4040
2023-04-26 11:50:09,114 INFO org.apache.spark.MapOutputTrackerMasterEndpoint [dispatcher-event-loop-2] MapOutputTrackerMasterEndpoint stopped!
2023-04-26 11:50:09,138 INFO org.apache.spark.storage.memory.MemoryStore [Thread-86] MemoryStore cleared
2023-04-26 11:50:09,138 INFO org.apache.spark.storage.BlockManager [Thread-86] BlockManager stopped
2023-04-26 11:50:09,139 INFO org.apache.spark.storage.BlockManagerMaster [Thread-86] BlockManagerMaster stopped
2023-04-26 11:50:09,140 INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [dispatcher-event-loop-3] OutputCommitCoordinator stopped!
2023-04-26 11:50:09,161 INFO org.apache.spark.SparkContext [Thread-86] Successfully stopped SparkContext
2023-04-26 11:50:09,161 INFO org.apache.spark.SparkContext [Thread-86] SparkContext already stopped.
2023-04-26 11:50:09,163 INFO com.zaxxer.hikari.HikariDataSource [Thread-86] HikariPool-6 - Shutdown initiated...
2023-04-26 11:50:09,171 INFO com.zaxxer.hikari.HikariDataSource [Thread-86] HikariPool-6 - Shutdown completed.
2023-04-26 11:50:09,464 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] Starting SpringBootSecurityJwtMongodbApplication using Java 17.0.6 on wks-012 with PID 102147 (/home/inferyx/git/SpringApplicationWithSecurity/target/classes started by inferyx in /home/inferyx/git/SpringApplicationWithSecurity)
2023-04-26 11:50:09,464 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] No active profile set, falling back to 1 default profile: "default"
2023-04-26 11:50:10,210 INFO org.apache.catalina.core.StandardService [restartedMain] Starting service [Tomcat]
2023-04-26 11:50:10,210 INFO org.apache.catalina.core.StandardEngine [restartedMain] Starting Servlet engine: [Apache Tomcat/9.0.65]
2023-04-26 11:50:10,244 INFO org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] [restartedMain] Initializing Spring embedded WebApplicationContext
2023-04-26 11:50:10,292 INFO org.mongodb.driver.client [restartedMain] MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync|spring-boot", "version": "4.6.1"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "5.15.0-69-generic"}, "platform": "Java/Eclipse Adoptium/17.0.6+10"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.Jep395RecordCodecProvider@269fb271]}, clusterSettings={hosts=[localhost:27017], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='30000 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, sendBufferSize=0}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, sendBufferSize=0}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=JAVA_LEGACY, serverApi=null, autoEncryptionSettings=null, contextProvider=null}
2023-04-26 11:50:10,312 INFO org.mongodb.driver.connection [cluster-ClusterId{value='6448c29aadee62176f290760', description='null'}-localhost:27017] Opened connection [connectionId{localValue:14, serverValue:55}] to localhost:27017
2023-04-26 11:50:10,312 INFO org.mongodb.driver.cluster [cluster-ClusterId{value='6448c29aadee62176f290760', description='null'}-localhost:27017] Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=6, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=18638645}
2023-04-26 11:50:10,317 INFO org.mongodb.driver.connection [cluster-rtt-ClusterId{value='6448c29aadee62176f290760', description='null'}-localhost:27017] Opened connection [connectionId{localValue:15, serverValue:56}] to localhost:27017
2023-04-26 11:50:10,637 INFO org.hibernate.jpa.internal.util.LogHelper [restartedMain] HHH000204: Processing PersistenceUnitInfo [name: default]
2023-04-26 11:50:10,658 INFO com.zaxxer.hikari.HikariDataSource [restartedMain] HikariPool-7 - Starting...
2023-04-26 11:50:10,664 INFO com.zaxxer.hikari.HikariDataSource [restartedMain] HikariPool-7 - Start completed.
2023-04-26 11:50:10,665 INFO org.hibernate.dialect.Dialect [restartedMain] HHH000400: Using dialect: org.hibernate.dialect.MySQL5Dialect
2023-04-26 11:50:10,771 INFO org.hibernate.engine.transaction.jta.platform.internal.JtaPlatformInitiator [restartedMain] HHH000490: Using JtaPlatform implementation: [org.hibernate.engine.transaction.jta.platform.internal.NoJtaPlatform]
2023-04-26 11:50:11,067 INFO org.apache.spark.SparkContext [restartedMain] Running Spark version 2.4.5
2023-04-26 11:50:11,067 INFO org.apache.spark.SparkContext [restartedMain] Submitted application: MyAppName
2023-04-26 11:50:11,069 INFO org.apache.spark.SecurityManager [restartedMain] Changing view acls to: inferyx
2023-04-26 11:50:11,070 INFO org.apache.spark.SecurityManager [restartedMain] Changing modify acls to: inferyx
2023-04-26 11:50:11,070 INFO org.apache.spark.SecurityManager [restartedMain] Changing view acls groups to: 
2023-04-26 11:50:11,071 INFO org.apache.spark.SecurityManager [restartedMain] Changing modify acls groups to: 
2023-04-26 11:50:11,071 INFO org.apache.spark.SecurityManager [restartedMain] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(inferyx); groups with view permissions: Set(); users  with modify permissions: Set(inferyx); groups with modify permissions: Set()
2023-04-26 11:50:11,120 INFO org.apache.spark.util.Utils [restartedMain] Successfully started service 'sparkDriver' on port 46345.
2023-04-26 11:50:11,124 INFO org.apache.spark.SparkEnv [restartedMain] Registering MapOutputTracker
2023-04-26 11:50:11,126 INFO org.apache.spark.SparkEnv [restartedMain] Registering BlockManagerMaster
2023-04-26 11:50:11,126 INFO org.apache.spark.storage.BlockManagerMasterEndpoint [restartedMain] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2023-04-26 11:50:11,127 INFO org.apache.spark.storage.BlockManagerMasterEndpoint [restartedMain] BlockManagerMasterEndpoint up
2023-04-26 11:50:11,127 INFO org.apache.spark.storage.DiskBlockManager [restartedMain] Created local directory at /tmp/blockmgr-539ae8b7-e783-4283-b26b-f80d29f83549
2023-04-26 11:50:11,128 INFO org.apache.spark.storage.memory.MemoryStore [restartedMain] MemoryStore started with capacity 998.4 MB
2023-04-26 11:50:11,130 INFO org.apache.spark.SparkEnv [restartedMain] Registering OutputCommitCoordinator
2023-04-26 11:50:11,137 INFO org.spark_project.jetty.server.Server [restartedMain] jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2023-04-26 11:50:11,142 INFO org.spark_project.jetty.server.Server [restartedMain] Started @483375ms
2023-04-26 11:50:11,143 INFO org.spark_project.jetty.server.AbstractConnector [restartedMain] Started ServerConnector@378e2c2a{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2023-04-26 11:50:11,143 INFO org.apache.spark.util.Utils [restartedMain] Successfully started service 'SparkUI' on port 4040.
2023-04-26 11:50:11,145 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@428210df{/jobs,null,AVAILABLE,@Spark}
2023-04-26 11:50:11,146 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@bbf0a8f{/jobs/json,null,AVAILABLE,@Spark}
2023-04-26 11:50:11,147 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@6f16a736{/jobs/job,null,AVAILABLE,@Spark}
2023-04-26 11:50:11,147 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@29bf2982{/jobs/job/json,null,AVAILABLE,@Spark}
2023-04-26 11:50:11,148 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@7be9d082{/stages,null,AVAILABLE,@Spark}
2023-04-26 11:50:11,148 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@8f15602{/stages/json,null,AVAILABLE,@Spark}
2023-04-26 11:50:11,149 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@52846f3d{/stages/stage,null,AVAILABLE,@Spark}
2023-04-26 11:50:11,150 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@227afd6{/stages/stage/json,null,AVAILABLE,@Spark}
2023-04-26 11:50:11,151 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@37ff02d0{/stages/pool,null,AVAILABLE,@Spark}
2023-04-26 11:50:11,151 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@353c136b{/stages/pool/json,null,AVAILABLE,@Spark}
2023-04-26 11:50:11,152 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@44b06f76{/storage,null,AVAILABLE,@Spark}
2023-04-26 11:50:11,153 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@7bd62129{/storage/json,null,AVAILABLE,@Spark}
2023-04-26 11:50:11,154 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@34704501{/storage/rdd,null,AVAILABLE,@Spark}
2023-04-26 11:50:11,154 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@6102afaf{/storage/rdd/json,null,AVAILABLE,@Spark}
2023-04-26 11:50:11,155 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@260ea726{/environment,null,AVAILABLE,@Spark}
2023-04-26 11:50:11,156 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@6c1b15a7{/environment/json,null,AVAILABLE,@Spark}
2023-04-26 11:50:11,157 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@5af51d68{/executors,null,AVAILABLE,@Spark}
2023-04-26 11:50:11,157 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@2e7ebe4e{/executors/json,null,AVAILABLE,@Spark}
2023-04-26 11:50:11,158 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@7d1a0b30{/executors/threadDump,null,AVAILABLE,@Spark}
2023-04-26 11:50:11,159 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@49988f19{/executors/threadDump/json,null,AVAILABLE,@Spark}
2023-04-26 11:50:11,160 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@3b74d51f{/static,null,AVAILABLE,@Spark}
2023-04-26 11:50:11,161 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@60a390f1{/,null,AVAILABLE,@Spark}
2023-04-26 11:50:11,162 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@4dcd545e{/api,null,AVAILABLE,@Spark}
2023-04-26 11:50:11,163 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@5d61038f{/jobs/job/kill,null,AVAILABLE,@Spark}
2023-04-26 11:50:11,164 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@213924ca{/stages/stage/kill,null,AVAILABLE,@Spark}
2023-04-26 11:50:11,164 INFO org.apache.spark.ui.SparkUI [restartedMain] Bound SparkUI to 0.0.0.0, and started at http://192.168.1.125:4040
2023-04-26 11:50:11,195 INFO org.apache.spark.executor.Executor [restartedMain] Starting executor ID driver on host localhost
2023-04-26 11:50:11,198 INFO org.apache.spark.util.Utils [restartedMain] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39693.
2023-04-26 11:50:11,199 INFO org.apache.spark.network.netty.NettyBlockTransferService [restartedMain] Server created on 192.168.1.125:39693
2023-04-26 11:50:11,199 INFO org.apache.spark.storage.BlockManager [restartedMain] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2023-04-26 11:50:11,199 INFO org.apache.spark.storage.BlockManagerMaster [restartedMain] Registering BlockManager BlockManagerId(driver, 192.168.1.125, 39693, None)
2023-04-26 11:50:11,200 INFO org.apache.spark.storage.BlockManagerMasterEndpoint [dispatcher-event-loop-3] Registering block manager 192.168.1.125:39693 with 998.4 MB RAM, BlockManagerId(driver, 192.168.1.125, 39693, None)
2023-04-26 11:50:11,200 INFO org.apache.spark.storage.BlockManagerMaster [restartedMain] Registered BlockManager BlockManagerId(driver, 192.168.1.125, 39693, None)
2023-04-26 11:50:11,200 INFO org.apache.spark.storage.BlockManager [restartedMain] Initialized BlockManager: BlockManagerId(driver, 192.168.1.125, 39693, None)
2023-04-26 11:50:11,201 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@4591e792{/metrics/json,null,AVAILABLE,@Spark}
2023-04-26 11:50:12,828 INFO org.apache.kafka.clients.consumer.ConsumerConfig [restartedMain] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-book-group-7
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = book-group
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2023-04-26 11:50:12,834 INFO org.apache.kafka.common.utils.AppInfoParser [restartedMain] Kafka version: 3.1.1
2023-04-26 11:50:12,835 INFO org.apache.kafka.common.utils.AppInfoParser [restartedMain] Kafka commitId: 97671528ba54a138
2023-04-26 11:50:12,835 INFO org.apache.kafka.common.utils.AppInfoParser [restartedMain] Kafka startTimeMs: 1682490012834
2023-04-26 11:50:12,836 INFO org.apache.kafka.clients.consumer.KafkaConsumer [restartedMain] [Consumer clientId=consumer-book-group-7, groupId=book-group] Subscribed to topic(s): my-topic
2023-04-26 11:50:12,842 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-7, groupId=book-group] Resetting the last seen epoch of partition my-topic-0 to 0 since the associated topicId changed from null to QqwJ2XdJQbi2BDXQbzT51g
2023-04-26 11:50:12,843 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-7, groupId=book-group] Cluster ID: 6CnV1ouTQjGsP-r9Rd2WRA
2023-04-26 11:50:12,844 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-7, groupId=book-group] Discovered group coordinator wks-012:9092 (id: 2147483647 rack: null)
2023-04-26 11:50:12,857 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-7, groupId=book-group] (Re-)joining group
2023-04-26 11:50:12,862 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-7, groupId=book-group] Request joining group due to: need to re-join with the given member-id
2023-04-26 11:50:12,863 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-7, groupId=book-group] (Re-)joining group
2023-04-26 11:50:12,866 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-7, groupId=book-group] Successfully joined group with generation Generation{generationId=143, memberId='consumer-book-group-7-1e9d9a44-6ca1-463e-9315-184ad5b59ada', protocol='range'}
2023-04-26 11:50:12,867 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-7, groupId=book-group] Finished assignment for group at generation 143: {consumer-book-group-7-1e9d9a44-6ca1-463e-9315-184ad5b59ada=Assignment(partitions=[my-topic-0])}
2023-04-26 11:50:12,870 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] Started SpringBootSecurityJwtMongodbApplication in 3.481 seconds (JVM running for 485.103)
2023-04-26 11:50:12,873 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-7, groupId=book-group] Successfully synced group in generation Generation{generationId=143, memberId='consumer-book-group-7-1e9d9a44-6ca1-463e-9315-184ad5b59ada', protocol='range'}
2023-04-26 11:50:12,874 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-7, groupId=book-group] Notifying assignor about the new Assignment(partitions=[my-topic-0])
2023-04-26 11:50:12,875 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] This is a info message
2023-04-26 11:50:12,875 WARN com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] This is a warn message
2023-04-26 11:50:12,875 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-7, groupId=book-group] Adding newly assigned partitions: my-topic-0
2023-04-26 11:50:12,882 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-7, groupId=book-group] Setting offset for partition my-topic-0 to the committed offset FetchPosition{offset=415, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[wks-012:9092 (id: 0 rack: null)], epoch=0}}
2023-04-26 11:50:25,254 INFO org.apache.catalina.core.StandardService [Thread-102] Stopping service [Tomcat]
2023-04-26 11:50:25,257 WARN org.apache.catalina.loader.WebappClassLoaderBase [Thread-102] The web application [ROOT] appears to have started a thread named [cluster-ClusterId{value='6448c29aadee62176f290760', description='null'}-localhost:27017] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/jdk.internal.misc.Unsafe.park(Native Method)
 java.base@17.0.6/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:252)
 java.base@17.0.6/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1672)
 app//com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.waitForSignalOrTimeout(DefaultServerMonitor.java:300)
 app//com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.waitForNext(DefaultServerMonitor.java:281)
 app//com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:179)
 java.base@17.0.6/java.lang.Thread.run(Thread.java:833)
2023-04-26 11:50:25,258 WARN org.apache.catalina.loader.WebappClassLoaderBase [Thread-102] The web application [ROOT] appears to have started a thread named [cluster-rtt-ClusterId{value='6448c29aadee62176f290760', description='null'}-localhost:27017] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/java.lang.Thread.sleep(Native Method)
 app//com.mongodb.internal.connection.DefaultServerMonitor.waitForNext(DefaultServerMonitor.java:443)
 app//com.mongodb.internal.connection.DefaultServerMonitor.access$1500(DefaultServerMonitor.java:64)
 app//com.mongodb.internal.connection.DefaultServerMonitor$RoundTripTimeRunnable.run(DefaultServerMonitor.java:415)
 java.base@17.0.6/java.lang.Thread.run(Thread.java:833)
2023-04-26 11:50:25,259 WARN org.apache.catalina.loader.WebappClassLoaderBase [Thread-102] The web application [ROOT] appears to have started a thread named [MaintenanceTimer-8-thread-1] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/jdk.internal.misc.Unsafe.park(Native Method)
 java.base@17.0.6/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:252)
 java.base@17.0.6/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1672)
 java.base@17.0.6/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182)
 java.base@17.0.6/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1062)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1122)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
 java.base@17.0.6/java.lang.Thread.run(Thread.java:833)
2023-04-26 11:50:25,261 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-7, groupId=book-group] Revoke previously assigned partitions my-topic-0
2023-04-26 11:50:25,262 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-7, groupId=book-group] Member consumer-book-group-7-1e9d9a44-6ca1-463e-9315-184ad5b59ada sending LeaveGroup request to coordinator wks-012:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2023-04-26 11:50:25,262 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-7, groupId=book-group] Resetting generation due to: consumer pro-actively leaving the group
2023-04-26 11:50:25,262 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-7, groupId=book-group] Request joining group due to: consumer pro-actively leaving the group
2023-04-26 11:50:25,263 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-7, groupId=book-group] Unsubscribed all topics or patterns and assigned partitions
2023-04-26 11:50:25,264 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-7, groupId=book-group] Resetting generation due to: consumer pro-actively leaving the group
2023-04-26 11:50:25,265 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-7, groupId=book-group] Request joining group due to: consumer pro-actively leaving the group
2023-04-26 11:50:25,265 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Metrics scheduler closed
2023-04-26 11:50:25,266 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-04-26 11:50:25,266 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Metrics reporters closed
2023-04-26 11:50:25,270 INFO org.apache.kafka.common.utils.AppInfoParser [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] App info kafka.consumer for consumer-book-group-7 unregistered
2023-04-26 11:50:25,277 INFO org.spark_project.jetty.server.AbstractConnector [Thread-102] Stopped Spark@378e2c2a{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2023-04-26 11:50:25,279 INFO org.apache.spark.ui.SparkUI [Thread-102] Stopped Spark web UI at http://192.168.1.125:4040
2023-04-26 11:50:25,283 INFO org.apache.spark.MapOutputTrackerMasterEndpoint [dispatcher-event-loop-3] MapOutputTrackerMasterEndpoint stopped!
2023-04-26 11:50:25,291 INFO org.apache.spark.storage.memory.MemoryStore [Thread-102] MemoryStore cleared
2023-04-26 11:50:25,291 INFO org.apache.spark.storage.BlockManager [Thread-102] BlockManager stopped
2023-04-26 11:50:25,292 INFO org.apache.spark.storage.BlockManagerMaster [Thread-102] BlockManagerMaster stopped
2023-04-26 11:50:25,292 INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [dispatcher-event-loop-3] OutputCommitCoordinator stopped!
2023-04-26 11:50:25,299 INFO org.apache.spark.SparkContext [Thread-102] Successfully stopped SparkContext
2023-04-26 11:50:25,300 INFO org.apache.spark.SparkContext [Thread-102] SparkContext already stopped.
2023-04-26 11:50:25,302 INFO com.zaxxer.hikari.HikariDataSource [Thread-102] HikariPool-7 - Shutdown initiated...
2023-04-26 11:50:25,305 INFO com.zaxxer.hikari.HikariDataSource [Thread-102] HikariPool-7 - Shutdown completed.
2023-04-26 11:50:25,549 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] Starting SpringBootSecurityJwtMongodbApplication using Java 17.0.6 on wks-012 with PID 102147 (/home/inferyx/git/SpringApplicationWithSecurity/target/classes started by inferyx in /home/inferyx/git/SpringApplicationWithSecurity)
2023-04-26 11:50:25,549 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] No active profile set, falling back to 1 default profile: "default"
2023-04-26 11:50:26,366 INFO org.apache.catalina.core.StandardService [restartedMain] Starting service [Tomcat]
2023-04-26 11:50:26,367 INFO org.apache.catalina.core.StandardEngine [restartedMain] Starting Servlet engine: [Apache Tomcat/9.0.65]
2023-04-26 11:50:26,396 INFO org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] [restartedMain] Initializing Spring embedded WebApplicationContext
2023-04-26 11:50:26,469 INFO org.mongodb.driver.client [restartedMain] MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync|spring-boot", "version": "4.6.1"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "5.15.0-69-generic"}, "platform": "Java/Eclipse Adoptium/17.0.6+10"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.Jep395RecordCodecProvider@269fb271]}, clusterSettings={hosts=[localhost:27017], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='30000 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, sendBufferSize=0}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, sendBufferSize=0}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=JAVA_LEGACY, serverApi=null, autoEncryptionSettings=null, contextProvider=null}
2023-04-26 11:50:26,478 INFO org.mongodb.driver.connection [cluster-ClusterId{value='6448c2aaadee62176f290761', description='null'}-localhost:27017] Opened connection [connectionId{localValue:16, serverValue:57}] to localhost:27017
2023-04-26 11:50:26,479 INFO org.mongodb.driver.cluster [cluster-ClusterId{value='6448c2aaadee62176f290761', description='null'}-localhost:27017] Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=6, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1811667}
2023-04-26 11:50:26,500 INFO org.mongodb.driver.connection [cluster-rtt-ClusterId{value='6448c2aaadee62176f290761', description='null'}-localhost:27017] Opened connection [connectionId{localValue:17, serverValue:58}] to localhost:27017
2023-04-26 11:50:26,781 INFO org.hibernate.jpa.internal.util.LogHelper [restartedMain] HHH000204: Processing PersistenceUnitInfo [name: default]
2023-04-26 11:50:26,794 INFO com.zaxxer.hikari.HikariDataSource [restartedMain] HikariPool-8 - Starting...
2023-04-26 11:50:26,798 INFO com.zaxxer.hikari.HikariDataSource [restartedMain] HikariPool-8 - Start completed.
2023-04-26 11:50:26,799 INFO org.hibernate.dialect.Dialect [restartedMain] HHH000400: Using dialect: org.hibernate.dialect.MySQL5Dialect
2023-04-26 11:50:26,885 INFO org.hibernate.engine.transaction.jta.platform.internal.JtaPlatformInitiator [restartedMain] HHH000490: Using JtaPlatform implementation: [org.hibernate.engine.transaction.jta.platform.internal.NoJtaPlatform]
2023-04-26 11:50:27,142 INFO org.apache.spark.SparkContext [restartedMain] Running Spark version 2.4.5
2023-04-26 11:50:27,143 INFO org.apache.spark.SparkContext [restartedMain] Submitted application: MyAppName
2023-04-26 11:50:27,145 INFO org.apache.spark.SecurityManager [restartedMain] Changing view acls to: inferyx
2023-04-26 11:50:27,145 INFO org.apache.spark.SecurityManager [restartedMain] Changing modify acls to: inferyx
2023-04-26 11:50:27,146 INFO org.apache.spark.SecurityManager [restartedMain] Changing view acls groups to: 
2023-04-26 11:50:27,146 INFO org.apache.spark.SecurityManager [restartedMain] Changing modify acls groups to: 
2023-04-26 11:50:27,146 INFO org.apache.spark.SecurityManager [restartedMain] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(inferyx); groups with view permissions: Set(); users  with modify permissions: Set(inferyx); groups with modify permissions: Set()
2023-04-26 11:50:27,203 INFO org.apache.spark.util.Utils [restartedMain] Successfully started service 'sparkDriver' on port 37817.
2023-04-26 11:50:27,205 INFO org.apache.spark.SparkEnv [restartedMain] Registering MapOutputTracker
2023-04-26 11:50:27,207 INFO org.apache.spark.SparkEnv [restartedMain] Registering BlockManagerMaster
2023-04-26 11:50:27,207 INFO org.apache.spark.storage.BlockManagerMasterEndpoint [restartedMain] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2023-04-26 11:50:27,207 INFO org.apache.spark.storage.BlockManagerMasterEndpoint [restartedMain] BlockManagerMasterEndpoint up
2023-04-26 11:50:27,208 INFO org.apache.spark.storage.DiskBlockManager [restartedMain] Created local directory at /tmp/blockmgr-a1225896-f77d-4017-b72d-ad7b293c7465
2023-04-26 11:50:27,209 INFO org.apache.spark.storage.memory.MemoryStore [restartedMain] MemoryStore started with capacity 998.4 MB
2023-04-26 11:50:27,211 INFO org.apache.spark.SparkEnv [restartedMain] Registering OutputCommitCoordinator
2023-04-26 11:50:27,216 INFO org.spark_project.jetty.server.Server [restartedMain] jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2023-04-26 11:50:27,218 INFO org.spark_project.jetty.server.Server [restartedMain] Started @499451ms
2023-04-26 11:50:27,219 INFO org.spark_project.jetty.server.AbstractConnector [restartedMain] Started ServerConnector@43540428{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2023-04-26 11:50:27,220 INFO org.apache.spark.util.Utils [restartedMain] Successfully started service 'SparkUI' on port 4040.
2023-04-26 11:50:27,220 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@29034431{/jobs,null,AVAILABLE,@Spark}
2023-04-26 11:50:27,221 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@3dc0e9{/jobs/json,null,AVAILABLE,@Spark}
2023-04-26 11:50:27,221 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@6a06b780{/jobs/job,null,AVAILABLE,@Spark}
2023-04-26 11:50:27,222 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@9908bac{/jobs/job/json,null,AVAILABLE,@Spark}
2023-04-26 11:50:27,222 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@3a817463{/stages,null,AVAILABLE,@Spark}
2023-04-26 11:50:27,222 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@54d5d152{/stages/json,null,AVAILABLE,@Spark}
2023-04-26 11:50:27,223 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@b30c6ff{/stages/stage,null,AVAILABLE,@Spark}
2023-04-26 11:50:27,223 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@6f0f1de{/stages/stage/json,null,AVAILABLE,@Spark}
2023-04-26 11:50:27,223 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@3763af0f{/stages/pool,null,AVAILABLE,@Spark}
2023-04-26 11:50:27,224 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@4d46b7f0{/stages/pool/json,null,AVAILABLE,@Spark}
2023-04-26 11:50:27,225 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@404c8799{/storage,null,AVAILABLE,@Spark}
2023-04-26 11:50:27,226 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@4133f04f{/storage/json,null,AVAILABLE,@Spark}
2023-04-26 11:50:27,226 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@6b0234b1{/storage/rdd,null,AVAILABLE,@Spark}
2023-04-26 11:50:27,227 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@1383f716{/storage/rdd/json,null,AVAILABLE,@Spark}
2023-04-26 11:50:27,227 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@37d7cab0{/environment,null,AVAILABLE,@Spark}
2023-04-26 11:50:27,228 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@69b8f58d{/environment/json,null,AVAILABLE,@Spark}
2023-04-26 11:50:27,229 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@1365f3b7{/executors,null,AVAILABLE,@Spark}
2023-04-26 11:50:27,230 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@1f529d8a{/executors/json,null,AVAILABLE,@Spark}
2023-04-26 11:50:27,230 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@1659469a{/executors/threadDump,null,AVAILABLE,@Spark}
2023-04-26 11:50:27,231 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@3863cd41{/executors/threadDump/json,null,AVAILABLE,@Spark}
2023-04-26 11:50:27,232 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@42779b1{/static,null,AVAILABLE,@Spark}
2023-04-26 11:50:27,233 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@f921a72{/,null,AVAILABLE,@Spark}
2023-04-26 11:50:27,234 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@5323f08c{/api,null,AVAILABLE,@Spark}
2023-04-26 11:50:27,234 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@4a59238d{/jobs/job/kill,null,AVAILABLE,@Spark}
2023-04-26 11:50:27,235 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@5507f13e{/stages/stage/kill,null,AVAILABLE,@Spark}
2023-04-26 11:50:27,235 INFO org.apache.spark.ui.SparkUI [restartedMain] Bound SparkUI to 0.0.0.0, and started at http://192.168.1.125:4040
2023-04-26 11:50:27,264 INFO org.apache.spark.executor.Executor [restartedMain] Starting executor ID driver on host localhost
2023-04-26 11:50:27,270 INFO org.apache.spark.util.Utils [restartedMain] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38199.
2023-04-26 11:50:27,270 INFO org.apache.spark.network.netty.NettyBlockTransferService [restartedMain] Server created on 192.168.1.125:38199
2023-04-26 11:50:27,270 INFO org.apache.spark.storage.BlockManager [restartedMain] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2023-04-26 11:50:27,270 INFO org.apache.spark.storage.BlockManagerMaster [restartedMain] Registering BlockManager BlockManagerId(driver, 192.168.1.125, 38199, None)
2023-04-26 11:50:27,271 INFO org.apache.spark.storage.BlockManagerMasterEndpoint [dispatcher-event-loop-2] Registering block manager 192.168.1.125:38199 with 998.4 MB RAM, BlockManagerId(driver, 192.168.1.125, 38199, None)
2023-04-26 11:50:27,272 INFO org.apache.spark.storage.BlockManagerMaster [restartedMain] Registered BlockManager BlockManagerId(driver, 192.168.1.125, 38199, None)
2023-04-26 11:50:27,272 INFO org.apache.spark.storage.BlockManager [restartedMain] Initialized BlockManager: BlockManagerId(driver, 192.168.1.125, 38199, None)
2023-04-26 11:50:27,274 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@56556847{/metrics/json,null,AVAILABLE,@Spark}
2023-04-26 11:50:28,863 INFO org.apache.kafka.clients.consumer.ConsumerConfig [restartedMain] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-book-group-8
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = book-group
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2023-04-26 11:50:28,870 INFO org.apache.kafka.common.utils.AppInfoParser [restartedMain] Kafka version: 3.1.1
2023-04-26 11:50:28,870 INFO org.apache.kafka.common.utils.AppInfoParser [restartedMain] Kafka commitId: 97671528ba54a138
2023-04-26 11:50:28,870 INFO org.apache.kafka.common.utils.AppInfoParser [restartedMain] Kafka startTimeMs: 1682490028870
2023-04-26 11:50:28,871 INFO org.apache.kafka.clients.consumer.KafkaConsumer [restartedMain] [Consumer clientId=consumer-book-group-8, groupId=book-group] Subscribed to topic(s): my-topic
2023-04-26 11:50:28,877 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-8, groupId=book-group] Resetting the last seen epoch of partition my-topic-0 to 0 since the associated topicId changed from null to QqwJ2XdJQbi2BDXQbzT51g
2023-04-26 11:50:28,878 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-8, groupId=book-group] Cluster ID: 6CnV1ouTQjGsP-r9Rd2WRA
2023-04-26 11:50:28,878 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-8, groupId=book-group] Discovered group coordinator wks-012:9092 (id: 2147483647 rack: null)
2023-04-26 11:50:28,881 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-8, groupId=book-group] (Re-)joining group
2023-04-26 11:50:28,889 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-8, groupId=book-group] Request joining group due to: need to re-join with the given member-id
2023-04-26 11:50:28,890 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-8, groupId=book-group] (Re-)joining group
2023-04-26 11:50:28,893 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-8, groupId=book-group] Successfully joined group with generation Generation{generationId=145, memberId='consumer-book-group-8-b5690281-9793-4dee-b0cf-bed76afe4b48', protocol='range'}
2023-04-26 11:50:28,893 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-8, groupId=book-group] Finished assignment for group at generation 145: {consumer-book-group-8-b5690281-9793-4dee-b0cf-bed76afe4b48=Assignment(partitions=[my-topic-0])}
2023-04-26 11:50:28,897 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-8, groupId=book-group] Successfully synced group in generation Generation{generationId=145, memberId='consumer-book-group-8-b5690281-9793-4dee-b0cf-bed76afe4b48', protocol='range'}
2023-04-26 11:50:28,898 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-8, groupId=book-group] Notifying assignor about the new Assignment(partitions=[my-topic-0])
2023-04-26 11:50:28,898 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-8, groupId=book-group] Adding newly assigned partitions: my-topic-0
2023-04-26 11:50:28,901 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-8, groupId=book-group] Setting offset for partition my-topic-0 to the committed offset FetchPosition{offset=415, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[wks-012:9092 (id: 0 rack: null)], epoch=0}}
2023-04-26 11:50:28,905 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] Started SpringBootSecurityJwtMongodbApplication in 3.425 seconds (JVM running for 501.138)
2023-04-26 11:50:28,911 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] This is a info message
2023-04-26 11:50:28,912 WARN com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] This is a warn message
2023-04-26 11:50:33,250 INFO org.apache.catalina.core.StandardService [Thread-119] Stopping service [Tomcat]
2023-04-26 11:50:33,253 WARN org.apache.catalina.loader.WebappClassLoaderBase [Thread-119] The web application [ROOT] appears to have started a thread named [cluster-ClusterId{value='6448c2aaadee62176f290761', description='null'}-localhost:27017] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/jdk.internal.misc.Unsafe.park(Native Method)
 java.base@17.0.6/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:252)
 java.base@17.0.6/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1672)
 app//com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.waitForSignalOrTimeout(DefaultServerMonitor.java:300)
 app//com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.waitForNext(DefaultServerMonitor.java:281)
 app//com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:179)
 java.base@17.0.6/java.lang.Thread.run(Thread.java:833)
2023-04-26 11:50:33,254 WARN org.apache.catalina.loader.WebappClassLoaderBase [Thread-119] The web application [ROOT] appears to have started a thread named [cluster-rtt-ClusterId{value='6448c2aaadee62176f290761', description='null'}-localhost:27017] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/java.lang.Thread.sleep(Native Method)
 app//com.mongodb.internal.connection.DefaultServerMonitor.waitForNext(DefaultServerMonitor.java:443)
 app//com.mongodb.internal.connection.DefaultServerMonitor.access$1500(DefaultServerMonitor.java:64)
 app//com.mongodb.internal.connection.DefaultServerMonitor$RoundTripTimeRunnable.run(DefaultServerMonitor.java:415)
 java.base@17.0.6/java.lang.Thread.run(Thread.java:833)
2023-04-26 11:50:33,255 WARN org.apache.catalina.loader.WebappClassLoaderBase [Thread-119] The web application [ROOT] appears to have started a thread named [MaintenanceTimer-9-thread-1] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/jdk.internal.misc.Unsafe.park(Native Method)
 java.base@17.0.6/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:252)
 java.base@17.0.6/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1672)
 java.base@17.0.6/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182)
 java.base@17.0.6/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1062)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1122)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
 java.base@17.0.6/java.lang.Thread.run(Thread.java:833)
2023-04-26 11:50:33,262 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-8, groupId=book-group] Revoke previously assigned partitions my-topic-0
2023-04-26 11:50:33,262 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-8, groupId=book-group] Member consumer-book-group-8-b5690281-9793-4dee-b0cf-bed76afe4b48 sending LeaveGroup request to coordinator wks-012:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2023-04-26 11:50:33,263 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-8, groupId=book-group] Resetting generation due to: consumer pro-actively leaving the group
2023-04-26 11:50:33,263 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-8, groupId=book-group] Request joining group due to: consumer pro-actively leaving the group
2023-04-26 11:50:33,263 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-8, groupId=book-group] Unsubscribed all topics or patterns and assigned partitions
2023-04-26 11:50:33,263 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-8, groupId=book-group] Resetting generation due to: consumer pro-actively leaving the group
2023-04-26 11:50:33,264 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-8, groupId=book-group] Request joining group due to: consumer pro-actively leaving the group
2023-04-26 11:50:33,267 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Metrics scheduler closed
2023-04-26 11:50:33,267 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-04-26 11:50:33,267 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Metrics reporters closed
2023-04-26 11:50:33,272 INFO org.apache.kafka.common.utils.AppInfoParser [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] App info kafka.consumer for consumer-book-group-8 unregistered
2023-04-26 11:50:33,277 INFO org.spark_project.jetty.server.AbstractConnector [Thread-119] Stopped Spark@43540428{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2023-04-26 11:50:33,279 INFO org.apache.spark.ui.SparkUI [Thread-119] Stopped Spark web UI at http://192.168.1.125:4040
2023-04-26 11:50:33,286 INFO org.apache.spark.MapOutputTrackerMasterEndpoint [dispatcher-event-loop-1] MapOutputTrackerMasterEndpoint stopped!
2023-04-26 11:50:33,300 INFO org.apache.spark.storage.memory.MemoryStore [Thread-119] MemoryStore cleared
2023-04-26 11:50:33,300 INFO org.apache.spark.storage.BlockManager [Thread-119] BlockManager stopped
2023-04-26 11:50:33,301 INFO org.apache.spark.storage.BlockManagerMaster [Thread-119] BlockManagerMaster stopped
2023-04-26 11:50:33,302 INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [dispatcher-event-loop-3] OutputCommitCoordinator stopped!
2023-04-26 11:50:33,313 INFO org.apache.spark.SparkContext [Thread-119] Successfully stopped SparkContext
2023-04-26 11:50:33,314 INFO org.apache.spark.SparkContext [Thread-119] SparkContext already stopped.
2023-04-26 11:50:33,317 INFO com.zaxxer.hikari.HikariDataSource [Thread-119] HikariPool-8 - Shutdown initiated...
2023-04-26 11:50:33,323 INFO com.zaxxer.hikari.HikariDataSource [Thread-119] HikariPool-8 - Shutdown completed.
2023-04-26 11:50:33,596 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] Starting SpringBootSecurityJwtMongodbApplication using Java 17.0.6 on wks-012 with PID 102147 (/home/inferyx/git/SpringApplicationWithSecurity/target/classes started by inferyx in /home/inferyx/git/SpringApplicationWithSecurity)
2023-04-26 11:50:33,597 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] No active profile set, falling back to 1 default profile: "default"
2023-04-26 11:50:34,364 INFO org.apache.catalina.core.StandardService [restartedMain] Starting service [Tomcat]
2023-04-26 11:50:34,365 INFO org.apache.catalina.core.StandardEngine [restartedMain] Starting Servlet engine: [Apache Tomcat/9.0.65]
2023-04-26 11:50:34,396 INFO org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] [restartedMain] Initializing Spring embedded WebApplicationContext
2023-04-26 11:50:34,443 INFO org.mongodb.driver.client [restartedMain] MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync|spring-boot", "version": "4.6.1"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "5.15.0-69-generic"}, "platform": "Java/Eclipse Adoptium/17.0.6+10"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.Jep395RecordCodecProvider@269fb271]}, clusterSettings={hosts=[localhost:27017], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='30000 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, sendBufferSize=0}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, sendBufferSize=0}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=JAVA_LEGACY, serverApi=null, autoEncryptionSettings=null, contextProvider=null}
2023-04-26 11:50:34,448 INFO org.mongodb.driver.connection [cluster-rtt-ClusterId{value='6448c2b2adee62176f290762', description='null'}-localhost:27017] Opened connection [connectionId{localValue:19, serverValue:60}] to localhost:27017
2023-04-26 11:50:34,446 INFO org.mongodb.driver.connection [cluster-ClusterId{value='6448c2b2adee62176f290762', description='null'}-localhost:27017] Opened connection [connectionId{localValue:18, serverValue:59}] to localhost:27017
2023-04-26 11:50:34,461 INFO org.mongodb.driver.cluster [cluster-ClusterId{value='6448c2b2adee62176f290762', description='null'}-localhost:27017] Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=6, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1804053}
2023-04-26 11:50:34,771 INFO org.hibernate.jpa.internal.util.LogHelper [restartedMain] HHH000204: Processing PersistenceUnitInfo [name: default]
2023-04-26 11:50:34,785 INFO com.zaxxer.hikari.HikariDataSource [restartedMain] HikariPool-9 - Starting...
2023-04-26 11:50:34,791 INFO com.zaxxer.hikari.HikariDataSource [restartedMain] HikariPool-9 - Start completed.
2023-04-26 11:50:34,792 INFO org.hibernate.dialect.Dialect [restartedMain] HHH000400: Using dialect: org.hibernate.dialect.MySQL5Dialect
2023-04-26 11:50:34,915 INFO org.hibernate.engine.transaction.jta.platform.internal.JtaPlatformInitiator [restartedMain] HHH000490: Using JtaPlatform implementation: [org.hibernate.engine.transaction.jta.platform.internal.NoJtaPlatform]
2023-04-26 11:50:35,168 INFO org.apache.spark.SparkContext [restartedMain] Running Spark version 2.4.5
2023-04-26 11:50:35,169 INFO org.apache.spark.SparkContext [restartedMain] Submitted application: MyAppName
2023-04-26 11:50:35,171 INFO org.apache.spark.SecurityManager [restartedMain] Changing view acls to: inferyx
2023-04-26 11:50:35,172 INFO org.apache.spark.SecurityManager [restartedMain] Changing modify acls to: inferyx
2023-04-26 11:50:35,172 INFO org.apache.spark.SecurityManager [restartedMain] Changing view acls groups to: 
2023-04-26 11:50:35,173 INFO org.apache.spark.SecurityManager [restartedMain] Changing modify acls groups to: 
2023-04-26 11:50:35,174 INFO org.apache.spark.SecurityManager [restartedMain] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(inferyx); groups with view permissions: Set(); users  with modify permissions: Set(inferyx); groups with modify permissions: Set()
2023-04-26 11:50:35,226 INFO org.apache.spark.util.Utils [restartedMain] Successfully started service 'sparkDriver' on port 37027.
2023-04-26 11:50:35,230 INFO org.apache.spark.SparkEnv [restartedMain] Registering MapOutputTracker
2023-04-26 11:50:35,232 INFO org.apache.spark.SparkEnv [restartedMain] Registering BlockManagerMaster
2023-04-26 11:50:35,232 INFO org.apache.spark.storage.BlockManagerMasterEndpoint [restartedMain] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2023-04-26 11:50:35,232 INFO org.apache.spark.storage.BlockManagerMasterEndpoint [restartedMain] BlockManagerMasterEndpoint up
2023-04-26 11:50:35,233 INFO org.apache.spark.storage.DiskBlockManager [restartedMain] Created local directory at /tmp/blockmgr-b30ab857-d40d-49a4-b42d-02fbdfe37ade
2023-04-26 11:50:35,234 INFO org.apache.spark.storage.memory.MemoryStore [restartedMain] MemoryStore started with capacity 998.4 MB
2023-04-26 11:50:35,236 INFO org.apache.spark.SparkEnv [restartedMain] Registering OutputCommitCoordinator
2023-04-26 11:50:35,241 INFO org.spark_project.jetty.server.Server [restartedMain] jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2023-04-26 11:50:35,244 INFO org.spark_project.jetty.server.Server [restartedMain] Started @507477ms
2023-04-26 11:50:35,245 INFO org.spark_project.jetty.server.AbstractConnector [restartedMain] Started ServerConnector@25bd761e{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2023-04-26 11:50:35,245 INFO org.apache.spark.util.Utils [restartedMain] Successfully started service 'SparkUI' on port 4040.
2023-04-26 11:50:35,246 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@6ea6a987{/jobs,null,AVAILABLE,@Spark}
2023-04-26 11:50:35,246 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@9febd01{/jobs/json,null,AVAILABLE,@Spark}
2023-04-26 11:50:35,247 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@13cbe818{/jobs/job,null,AVAILABLE,@Spark}
2023-04-26 11:50:35,247 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@5d0861f9{/jobs/job/json,null,AVAILABLE,@Spark}
2023-04-26 11:50:35,248 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@639014e{/stages,null,AVAILABLE,@Spark}
2023-04-26 11:50:35,248 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@7ad3a23d{/stages/json,null,AVAILABLE,@Spark}
2023-04-26 11:50:35,249 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@36141e57{/stages/stage,null,AVAILABLE,@Spark}
2023-04-26 11:50:35,249 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@288da6c5{/stages/stage/json,null,AVAILABLE,@Spark}
2023-04-26 11:50:35,250 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@6670e1eb{/stages/pool,null,AVAILABLE,@Spark}
2023-04-26 11:50:35,250 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@1d57552{/stages/pool/json,null,AVAILABLE,@Spark}
2023-04-26 11:50:35,251 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@17252599{/storage,null,AVAILABLE,@Spark}
2023-04-26 11:50:35,253 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@52db0236{/storage/json,null,AVAILABLE,@Spark}
2023-04-26 11:50:35,253 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@33c5de3f{/storage/rdd,null,AVAILABLE,@Spark}
2023-04-26 11:50:35,254 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@199052f0{/storage/rdd/json,null,AVAILABLE,@Spark}
2023-04-26 11:50:35,255 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@2727a5b6{/environment,null,AVAILABLE,@Spark}
2023-04-26 11:50:35,256 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@2de7d9d6{/environment/json,null,AVAILABLE,@Spark}
2023-04-26 11:50:35,256 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@304ece73{/executors,null,AVAILABLE,@Spark}
2023-04-26 11:50:35,258 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@2b520d58{/executors/json,null,AVAILABLE,@Spark}
2023-04-26 11:50:35,259 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@31420791{/executors/threadDump,null,AVAILABLE,@Spark}
2023-04-26 11:50:35,260 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@22785106{/executors/threadDump/json,null,AVAILABLE,@Spark}
2023-04-26 11:50:35,261 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@64832e6c{/static,null,AVAILABLE,@Spark}
2023-04-26 11:50:35,262 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@5f80d769{/,null,AVAILABLE,@Spark}
2023-04-26 11:50:35,263 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@7e5f2d87{/api,null,AVAILABLE,@Spark}
2023-04-26 11:50:35,264 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@63420ed0{/jobs/job/kill,null,AVAILABLE,@Spark}
2023-04-26 11:50:35,265 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@3a20b64e{/stages/stage/kill,null,AVAILABLE,@Spark}
2023-04-26 11:50:35,266 INFO org.apache.spark.ui.SparkUI [restartedMain] Bound SparkUI to 0.0.0.0, and started at http://192.168.1.125:4040
2023-04-26 11:50:35,309 INFO org.apache.spark.executor.Executor [restartedMain] Starting executor ID driver on host localhost
2023-04-26 11:50:35,316 INFO org.apache.spark.util.Utils [restartedMain] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38845.
2023-04-26 11:50:35,317 INFO org.apache.spark.network.netty.NettyBlockTransferService [restartedMain] Server created on 192.168.1.125:38845
2023-04-26 11:50:35,318 INFO org.apache.spark.storage.BlockManager [restartedMain] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2023-04-26 11:50:35,318 INFO org.apache.spark.storage.BlockManagerMaster [restartedMain] Registering BlockManager BlockManagerId(driver, 192.168.1.125, 38845, None)
2023-04-26 11:50:35,319 INFO org.apache.spark.storage.BlockManagerMasterEndpoint [dispatcher-event-loop-2] Registering block manager 192.168.1.125:38845 with 998.4 MB RAM, BlockManagerId(driver, 192.168.1.125, 38845, None)
2023-04-26 11:50:35,320 INFO org.apache.spark.storage.BlockManagerMaster [restartedMain] Registered BlockManager BlockManagerId(driver, 192.168.1.125, 38845, None)
2023-04-26 11:50:35,320 INFO org.apache.spark.storage.BlockManager [restartedMain] Initialized BlockManager: BlockManagerId(driver, 192.168.1.125, 38845, None)
2023-04-26 11:50:35,322 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@4b9247fe{/metrics/json,null,AVAILABLE,@Spark}
2023-04-26 11:50:37,010 INFO org.apache.kafka.clients.consumer.ConsumerConfig [restartedMain] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-book-group-9
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = book-group
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2023-04-26 11:50:37,016 INFO org.apache.kafka.common.utils.AppInfoParser [restartedMain] Kafka version: 3.1.1
2023-04-26 11:50:37,017 INFO org.apache.kafka.common.utils.AppInfoParser [restartedMain] Kafka commitId: 97671528ba54a138
2023-04-26 11:50:37,017 INFO org.apache.kafka.common.utils.AppInfoParser [restartedMain] Kafka startTimeMs: 1682490037016
2023-04-26 11:50:37,017 INFO org.apache.kafka.clients.consumer.KafkaConsumer [restartedMain] [Consumer clientId=consumer-book-group-9, groupId=book-group] Subscribed to topic(s): my-topic
2023-04-26 11:50:37,038 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-9, groupId=book-group] Resetting the last seen epoch of partition my-topic-0 to 0 since the associated topicId changed from null to QqwJ2XdJQbi2BDXQbzT51g
2023-04-26 11:50:37,038 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-9, groupId=book-group] Cluster ID: 6CnV1ouTQjGsP-r9Rd2WRA
2023-04-26 11:50:37,039 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-9, groupId=book-group] Discovered group coordinator wks-012:9092 (id: 2147483647 rack: null)
2023-04-26 11:50:37,045 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] Started SpringBootSecurityJwtMongodbApplication in 3.509 seconds (JVM running for 509.278)
2023-04-26 11:50:37,050 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] This is a info message
2023-04-26 11:50:37,050 WARN com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] This is a warn message
2023-04-26 11:50:37,050 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-9, groupId=book-group] (Re-)joining group
2023-04-26 11:50:37,061 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-9, groupId=book-group] Request joining group due to: need to re-join with the given member-id
2023-04-26 11:50:37,062 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-9, groupId=book-group] (Re-)joining group
2023-04-26 11:50:37,065 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-9, groupId=book-group] Successfully joined group with generation Generation{generationId=147, memberId='consumer-book-group-9-5aabca3c-ddb9-4ab0-8683-e54a1641411d', protocol='range'}
2023-04-26 11:50:37,066 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-9, groupId=book-group] Finished assignment for group at generation 147: {consumer-book-group-9-5aabca3c-ddb9-4ab0-8683-e54a1641411d=Assignment(partitions=[my-topic-0])}
2023-04-26 11:50:37,071 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-9, groupId=book-group] Successfully synced group in generation Generation{generationId=147, memberId='consumer-book-group-9-5aabca3c-ddb9-4ab0-8683-e54a1641411d', protocol='range'}
2023-04-26 11:50:37,071 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-9, groupId=book-group] Notifying assignor about the new Assignment(partitions=[my-topic-0])
2023-04-26 11:50:37,072 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-9, groupId=book-group] Adding newly assigned partitions: my-topic-0
2023-04-26 11:50:37,078 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-9, groupId=book-group] Setting offset for partition my-topic-0 to the committed offset FetchPosition{offset=415, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[wks-012:9092 (id: 0 rack: null)], epoch=0}}
2023-04-26 11:54:04,243 INFO org.apache.catalina.core.StandardService [Thread-135] Stopping service [Tomcat]
2023-04-26 11:54:04,249 WARN org.apache.catalina.loader.WebappClassLoaderBase [Thread-135] The web application [ROOT] appears to have started a thread named [cluster-ClusterId{value='6448c2b2adee62176f290762', description='null'}-localhost:27017] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/jdk.internal.misc.Unsafe.park(Native Method)
 java.base@17.0.6/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:252)
 java.base@17.0.6/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1672)
 app//com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.waitForSignalOrTimeout(DefaultServerMonitor.java:300)
 app//com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.waitForNext(DefaultServerMonitor.java:281)
 app//com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:179)
 java.base@17.0.6/java.lang.Thread.run(Thread.java:833)
2023-04-26 11:54:04,251 WARN org.apache.catalina.loader.WebappClassLoaderBase [Thread-135] The web application [ROOT] appears to have started a thread named [cluster-rtt-ClusterId{value='6448c2b2adee62176f290762', description='null'}-localhost:27017] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/java.lang.Thread.sleep(Native Method)
 app//com.mongodb.internal.connection.DefaultServerMonitor.waitForNext(DefaultServerMonitor.java:443)
 app//com.mongodb.internal.connection.DefaultServerMonitor.access$1500(DefaultServerMonitor.java:64)
 app//com.mongodb.internal.connection.DefaultServerMonitor$RoundTripTimeRunnable.run(DefaultServerMonitor.java:415)
 java.base@17.0.6/java.lang.Thread.run(Thread.java:833)
2023-04-26 11:54:04,255 WARN org.apache.catalina.loader.WebappClassLoaderBase [Thread-135] The web application [ROOT] appears to have started a thread named [MaintenanceTimer-10-thread-1] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/jdk.internal.misc.Unsafe.park(Native Method)
 java.base@17.0.6/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:252)
 java.base@17.0.6/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1672)
 java.base@17.0.6/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182)
 java.base@17.0.6/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1062)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1122)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
 java.base@17.0.6/java.lang.Thread.run(Thread.java:833)
2023-04-26 11:54:04,263 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-9, groupId=book-group] Revoke previously assigned partitions my-topic-0
2023-04-26 11:54:04,263 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-9, groupId=book-group] Member consumer-book-group-9-5aabca3c-ddb9-4ab0-8683-e54a1641411d sending LeaveGroup request to coordinator wks-012:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2023-04-26 11:54:04,268 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-9, groupId=book-group] Resetting generation due to: consumer pro-actively leaving the group
2023-04-26 11:54:04,268 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-9, groupId=book-group] Request joining group due to: consumer pro-actively leaving the group
2023-04-26 11:54:04,268 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-9, groupId=book-group] Unsubscribed all topics or patterns and assigned partitions
2023-04-26 11:54:04,269 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-9, groupId=book-group] Resetting generation due to: consumer pro-actively leaving the group
2023-04-26 11:54:04,269 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-9, groupId=book-group] Request joining group due to: consumer pro-actively leaving the group
2023-04-26 11:54:04,270 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Metrics scheduler closed
2023-04-26 11:54:04,271 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-04-26 11:54:04,271 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Metrics reporters closed
2023-04-26 11:54:04,281 INFO org.apache.kafka.common.utils.AppInfoParser [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] App info kafka.consumer for consumer-book-group-9 unregistered
2023-04-26 11:54:04,295 INFO org.spark_project.jetty.server.AbstractConnector [Thread-135] Stopped Spark@25bd761e{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2023-04-26 11:54:04,297 INFO org.apache.spark.ui.SparkUI [Thread-135] Stopped Spark web UI at http://192.168.1.125:4040
2023-04-26 11:54:04,300 INFO org.apache.spark.MapOutputTrackerMasterEndpoint [dispatcher-event-loop-3] MapOutputTrackerMasterEndpoint stopped!
2023-04-26 11:54:04,320 INFO org.apache.spark.storage.memory.MemoryStore [Thread-135] MemoryStore cleared
2023-04-26 11:54:04,320 INFO org.apache.spark.storage.BlockManager [Thread-135] BlockManager stopped
2023-04-26 11:54:04,321 INFO org.apache.spark.storage.BlockManagerMaster [Thread-135] BlockManagerMaster stopped
2023-04-26 11:54:04,321 INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [dispatcher-event-loop-0] OutputCommitCoordinator stopped!
2023-04-26 11:54:04,393 INFO org.apache.spark.SparkContext [Thread-135] Successfully stopped SparkContext
2023-04-26 11:54:04,393 INFO org.apache.spark.SparkContext [Thread-135] SparkContext already stopped.
2023-04-26 11:54:04,396 INFO com.zaxxer.hikari.HikariDataSource [Thread-135] HikariPool-9 - Shutdown initiated...
2023-04-26 11:54:04,410 INFO com.zaxxer.hikari.HikariDataSource [Thread-135] HikariPool-9 - Shutdown completed.
2023-04-26 11:54:04,791 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] Starting SpringBootSecurityJwtMongodbApplication using Java 17.0.6 on wks-012 with PID 102147 (/home/inferyx/git/SpringApplicationWithSecurity/target/classes started by inferyx in /home/inferyx/git/SpringApplicationWithSecurity)
2023-04-26 11:54:04,791 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] No active profile set, falling back to 1 default profile: "default"
2023-04-26 11:54:05,587 INFO org.apache.catalina.core.StandardService [restartedMain] Starting service [Tomcat]
2023-04-26 11:54:05,587 INFO org.apache.catalina.core.StandardEngine [restartedMain] Starting Servlet engine: [Apache Tomcat/9.0.65]
2023-04-26 11:54:05,618 INFO org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] [restartedMain] Initializing Spring embedded WebApplicationContext
2023-04-26 11:54:05,701 INFO org.mongodb.driver.client [restartedMain] MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync|spring-boot", "version": "4.6.1"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "5.15.0-69-generic"}, "platform": "Java/Eclipse Adoptium/17.0.6+10"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.Jep395RecordCodecProvider@269fb271]}, clusterSettings={hosts=[localhost:27017], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='30000 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, sendBufferSize=0}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, sendBufferSize=0}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=JAVA_LEGACY, serverApi=null, autoEncryptionSettings=null, contextProvider=null}
2023-04-26 11:54:05,702 INFO org.mongodb.driver.connection [cluster-ClusterId{value='6448c385adee62176f290763', description='null'}-localhost:27017] Opened connection [connectionId{localValue:20, serverValue:61}] to localhost:27017
2023-04-26 11:54:05,702 INFO org.mongodb.driver.cluster [cluster-ClusterId{value='6448c385adee62176f290763', description='null'}-localhost:27017] Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=6, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=4805149}
2023-04-26 11:54:05,709 INFO org.mongodb.driver.connection [cluster-rtt-ClusterId{value='6448c385adee62176f290763', description='null'}-localhost:27017] Opened connection [connectionId{localValue:21, serverValue:62}] to localhost:27017
2023-04-26 11:54:06,098 INFO org.hibernate.jpa.internal.util.LogHelper [restartedMain] HHH000204: Processing PersistenceUnitInfo [name: default]
2023-04-26 11:54:06,114 INFO com.zaxxer.hikari.HikariDataSource [restartedMain] HikariPool-10 - Starting...
2023-04-26 11:54:06,118 INFO com.zaxxer.hikari.HikariDataSource [restartedMain] HikariPool-10 - Start completed.
2023-04-26 11:54:06,119 INFO org.hibernate.dialect.Dialect [restartedMain] HHH000400: Using dialect: org.hibernate.dialect.MySQL5Dialect
2023-04-26 11:54:06,202 INFO org.hibernate.engine.transaction.jta.platform.internal.JtaPlatformInitiator [restartedMain] HHH000490: Using JtaPlatform implementation: [org.hibernate.engine.transaction.jta.platform.internal.NoJtaPlatform]
2023-04-26 11:54:06,542 INFO org.apache.spark.SparkContext [restartedMain] Running Spark version 2.4.5
2023-04-26 11:54:06,543 INFO org.apache.spark.SparkContext [restartedMain] Submitted application: MyAppName
2023-04-26 11:54:06,545 INFO org.apache.spark.SecurityManager [restartedMain] Changing view acls to: inferyx
2023-04-26 11:54:06,546 INFO org.apache.spark.SecurityManager [restartedMain] Changing modify acls to: inferyx
2023-04-26 11:54:06,546 INFO org.apache.spark.SecurityManager [restartedMain] Changing view acls groups to: 
2023-04-26 11:54:06,546 INFO org.apache.spark.SecurityManager [restartedMain] Changing modify acls groups to: 
2023-04-26 11:54:06,547 INFO org.apache.spark.SecurityManager [restartedMain] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(inferyx); groups with view permissions: Set(); users  with modify permissions: Set(inferyx); groups with modify permissions: Set()
2023-04-26 11:54:06,621 INFO org.apache.spark.util.Utils [restartedMain] Successfully started service 'sparkDriver' on port 40427.
2023-04-26 11:54:06,630 INFO org.apache.spark.SparkEnv [restartedMain] Registering MapOutputTracker
2023-04-26 11:54:06,633 INFO org.apache.spark.SparkEnv [restartedMain] Registering BlockManagerMaster
2023-04-26 11:54:06,635 INFO org.apache.spark.storage.BlockManagerMasterEndpoint [restartedMain] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2023-04-26 11:54:06,635 INFO org.apache.spark.storage.BlockManagerMasterEndpoint [restartedMain] BlockManagerMasterEndpoint up
2023-04-26 11:54:06,636 INFO org.apache.spark.storage.DiskBlockManager [restartedMain] Created local directory at /tmp/blockmgr-1c49081a-942e-4af3-9c27-682298c875ee
2023-04-26 11:54:06,636 INFO org.apache.spark.storage.memory.MemoryStore [restartedMain] MemoryStore started with capacity 998.4 MB
2023-04-26 11:54:06,638 INFO org.apache.spark.SparkEnv [restartedMain] Registering OutputCommitCoordinator
2023-04-26 11:54:06,642 INFO org.spark_project.jetty.server.Server [restartedMain] jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2023-04-26 11:54:06,644 INFO org.spark_project.jetty.server.Server [restartedMain] Started @718877ms
2023-04-26 11:54:06,645 INFO org.spark_project.jetty.server.AbstractConnector [restartedMain] Started ServerConnector@6f77cdca{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2023-04-26 11:54:06,645 INFO org.apache.spark.util.Utils [restartedMain] Successfully started service 'SparkUI' on port 4040.
2023-04-26 11:54:06,646 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@25e47649{/jobs,null,AVAILABLE,@Spark}
2023-04-26 11:54:06,646 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@7b0f5659{/jobs/json,null,AVAILABLE,@Spark}
2023-04-26 11:54:06,647 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@2fe54606{/jobs/job,null,AVAILABLE,@Spark}
2023-04-26 11:54:06,647 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@7eadc7ad{/jobs/job/json,null,AVAILABLE,@Spark}
2023-04-26 11:54:06,648 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@787da00f{/stages,null,AVAILABLE,@Spark}
2023-04-26 11:54:06,648 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@79c19cc7{/stages/json,null,AVAILABLE,@Spark}
2023-04-26 11:54:06,649 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@7c11208f{/stages/stage,null,AVAILABLE,@Spark}
2023-04-26 11:54:06,650 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@6c91000f{/stages/stage/json,null,AVAILABLE,@Spark}
2023-04-26 11:54:06,650 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@79f9b84a{/stages/pool,null,AVAILABLE,@Spark}
2023-04-26 11:54:06,651 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@c2b2416{/stages/pool/json,null,AVAILABLE,@Spark}
2023-04-26 11:54:06,651 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@794b6844{/storage,null,AVAILABLE,@Spark}
2023-04-26 11:54:06,652 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@1dbe8e1a{/storage/json,null,AVAILABLE,@Spark}
2023-04-26 11:54:06,653 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@29b15e0f{/storage/rdd,null,AVAILABLE,@Spark}
2023-04-26 11:54:06,653 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@7cefc4bb{/storage/rdd/json,null,AVAILABLE,@Spark}
2023-04-26 11:54:06,654 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@5ec48a2a{/environment,null,AVAILABLE,@Spark}
2023-04-26 11:54:06,655 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@37e311b6{/environment/json,null,AVAILABLE,@Spark}
2023-04-26 11:54:06,655 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@14016063{/executors,null,AVAILABLE,@Spark}
2023-04-26 11:54:06,656 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@16c9b444{/executors/json,null,AVAILABLE,@Spark}
2023-04-26 11:54:06,657 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@6c26f96d{/executors/threadDump,null,AVAILABLE,@Spark}
2023-04-26 11:54:06,657 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@43e73bfe{/executors/threadDump/json,null,AVAILABLE,@Spark}
2023-04-26 11:54:06,658 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@5cfa5999{/static,null,AVAILABLE,@Spark}
2023-04-26 11:54:06,658 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@7cabdd6{/,null,AVAILABLE,@Spark}
2023-04-26 11:54:06,659 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@5c7cd1d3{/api,null,AVAILABLE,@Spark}
2023-04-26 11:54:06,660 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@28869ab6{/jobs/job/kill,null,AVAILABLE,@Spark}
2023-04-26 11:54:06,660 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@7a70b67d{/stages/stage/kill,null,AVAILABLE,@Spark}
2023-04-26 11:54:06,660 INFO org.apache.spark.ui.SparkUI [restartedMain] Bound SparkUI to 0.0.0.0, and started at http://192.168.1.125:4040
2023-04-26 11:54:06,687 INFO org.apache.spark.executor.Executor [restartedMain] Starting executor ID driver on host localhost
2023-04-26 11:54:06,693 INFO org.apache.spark.util.Utils [restartedMain] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46375.
2023-04-26 11:54:06,693 INFO org.apache.spark.network.netty.NettyBlockTransferService [restartedMain] Server created on 192.168.1.125:46375
2023-04-26 11:54:06,694 INFO org.apache.spark.storage.BlockManager [restartedMain] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2023-04-26 11:54:06,694 INFO org.apache.spark.storage.BlockManagerMaster [restartedMain] Registering BlockManager BlockManagerId(driver, 192.168.1.125, 46375, None)
2023-04-26 11:54:06,695 INFO org.apache.spark.storage.BlockManagerMasterEndpoint [dispatcher-event-loop-2] Registering block manager 192.168.1.125:46375 with 998.4 MB RAM, BlockManagerId(driver, 192.168.1.125, 46375, None)
2023-04-26 11:54:06,695 INFO org.apache.spark.storage.BlockManagerMaster [restartedMain] Registered BlockManager BlockManagerId(driver, 192.168.1.125, 46375, None)
2023-04-26 11:54:06,695 INFO org.apache.spark.storage.BlockManager [restartedMain] Initialized BlockManager: BlockManagerId(driver, 192.168.1.125, 46375, None)
2023-04-26 11:54:06,697 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@785c5d40{/metrics/json,null,AVAILABLE,@Spark}
2023-04-26 11:54:08,271 INFO org.apache.kafka.clients.consumer.ConsumerConfig [restartedMain] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-book-group-10
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = book-group
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2023-04-26 11:54:08,277 INFO org.apache.kafka.common.utils.AppInfoParser [restartedMain] Kafka version: 3.1.1
2023-04-26 11:54:08,277 INFO org.apache.kafka.common.utils.AppInfoParser [restartedMain] Kafka commitId: 97671528ba54a138
2023-04-26 11:54:08,278 INFO org.apache.kafka.common.utils.AppInfoParser [restartedMain] Kafka startTimeMs: 1682490248277
2023-04-26 11:54:08,278 INFO org.apache.kafka.clients.consumer.KafkaConsumer [restartedMain] [Consumer clientId=consumer-book-group-10, groupId=book-group] Subscribed to topic(s): my-topic
2023-04-26 11:54:08,285 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-10, groupId=book-group] Resetting the last seen epoch of partition my-topic-0 to 0 since the associated topicId changed from null to QqwJ2XdJQbi2BDXQbzT51g
2023-04-26 11:54:08,285 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-10, groupId=book-group] Cluster ID: 6CnV1ouTQjGsP-r9Rd2WRA
2023-04-26 11:54:08,286 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-10, groupId=book-group] Discovered group coordinator wks-012:9092 (id: 2147483647 rack: null)
2023-04-26 11:54:08,288 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-10, groupId=book-group] (Re-)joining group
2023-04-26 11:54:08,298 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-10, groupId=book-group] Request joining group due to: need to re-join with the given member-id
2023-04-26 11:54:08,299 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-10, groupId=book-group] (Re-)joining group
2023-04-26 11:54:08,302 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-10, groupId=book-group] Successfully joined group with generation Generation{generationId=149, memberId='consumer-book-group-10-53cfbb99-8981-47e6-98e9-92f789f11a71', protocol='range'}
2023-04-26 11:54:08,302 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-10, groupId=book-group] Finished assignment for group at generation 149: {consumer-book-group-10-53cfbb99-8981-47e6-98e9-92f789f11a71=Assignment(partitions=[my-topic-0])}
2023-04-26 11:54:08,306 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] Started SpringBootSecurityJwtMongodbApplication in 3.597 seconds (JVM running for 720.539)
2023-04-26 11:54:08,306 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-10, groupId=book-group] Successfully synced group in generation Generation{generationId=149, memberId='consumer-book-group-10-53cfbb99-8981-47e6-98e9-92f789f11a71', protocol='range'}
2023-04-26 11:54:08,307 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-10, groupId=book-group] Notifying assignor about the new Assignment(partitions=[my-topic-0])
2023-04-26 11:54:08,307 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-10, groupId=book-group] Adding newly assigned partitions: my-topic-0
2023-04-26 11:54:08,312 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] This is a info message
2023-04-26 11:54:08,312 WARN com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] This is a warn message
2023-04-26 11:54:08,316 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-10, groupId=book-group] Setting offset for partition my-topic-0 to the committed offset FetchPosition{offset=415, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[wks-012:9092 (id: 0 rack: null)], epoch=0}}
2023-04-26 11:54:27,741 INFO org.apache.catalina.core.StandardService [Thread-152] Stopping service [Tomcat]
2023-04-26 11:54:27,744 WARN org.apache.catalina.loader.WebappClassLoaderBase [Thread-152] The web application [ROOT] appears to have started a thread named [cluster-ClusterId{value='6448c385adee62176f290763', description='null'}-localhost:27017] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/jdk.internal.misc.Unsafe.park(Native Method)
 java.base@17.0.6/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:252)
 java.base@17.0.6/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1672)
 app//com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.waitForSignalOrTimeout(DefaultServerMonitor.java:300)
 app//com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.waitForNext(DefaultServerMonitor.java:281)
 app//com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:179)
 java.base@17.0.6/java.lang.Thread.run(Thread.java:833)
2023-04-26 11:54:27,745 WARN org.apache.catalina.loader.WebappClassLoaderBase [Thread-152] The web application [ROOT] appears to have started a thread named [cluster-rtt-ClusterId{value='6448c385adee62176f290763', description='null'}-localhost:27017] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/java.lang.Thread.sleep(Native Method)
 app//com.mongodb.internal.connection.DefaultServerMonitor.waitForNext(DefaultServerMonitor.java:443)
 app//com.mongodb.internal.connection.DefaultServerMonitor.access$1500(DefaultServerMonitor.java:64)
 app//com.mongodb.internal.connection.DefaultServerMonitor$RoundTripTimeRunnable.run(DefaultServerMonitor.java:415)
 java.base@17.0.6/java.lang.Thread.run(Thread.java:833)
2023-04-26 11:54:27,746 WARN org.apache.catalina.loader.WebappClassLoaderBase [Thread-152] The web application [ROOT] appears to have started a thread named [MaintenanceTimer-11-thread-1] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/jdk.internal.misc.Unsafe.park(Native Method)
 java.base@17.0.6/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:252)
 java.base@17.0.6/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1672)
 java.base@17.0.6/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182)
 java.base@17.0.6/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1062)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1122)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
 java.base@17.0.6/java.lang.Thread.run(Thread.java:833)
2023-04-26 11:54:27,750 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-10, groupId=book-group] Revoke previously assigned partitions my-topic-0
2023-04-26 11:54:27,751 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-10, groupId=book-group] Member consumer-book-group-10-53cfbb99-8981-47e6-98e9-92f789f11a71 sending LeaveGroup request to coordinator wks-012:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2023-04-26 11:54:27,751 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-10, groupId=book-group] Resetting generation due to: consumer pro-actively leaving the group
2023-04-26 11:54:27,751 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-10, groupId=book-group] Request joining group due to: consumer pro-actively leaving the group
2023-04-26 11:54:27,752 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-10, groupId=book-group] Unsubscribed all topics or patterns and assigned partitions
2023-04-26 11:54:27,752 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-10, groupId=book-group] Resetting generation due to: consumer pro-actively leaving the group
2023-04-26 11:54:27,752 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-10, groupId=book-group] Request joining group due to: consumer pro-actively leaving the group
2023-04-26 11:54:27,754 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Metrics scheduler closed
2023-04-26 11:54:27,755 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-04-26 11:54:27,755 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Metrics reporters closed
2023-04-26 11:54:27,760 INFO org.apache.kafka.common.utils.AppInfoParser [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] App info kafka.consumer for consumer-book-group-10 unregistered
2023-04-26 11:54:27,772 INFO org.spark_project.jetty.server.AbstractConnector [Thread-152] Stopped Spark@6f77cdca{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2023-04-26 11:54:27,774 INFO org.apache.spark.ui.SparkUI [Thread-152] Stopped Spark web UI at http://192.168.1.125:4040
2023-04-26 11:54:27,777 INFO org.apache.spark.MapOutputTrackerMasterEndpoint [dispatcher-event-loop-3] MapOutputTrackerMasterEndpoint stopped!
2023-04-26 11:54:27,791 INFO org.apache.spark.storage.memory.MemoryStore [Thread-152] MemoryStore cleared
2023-04-26 11:54:27,791 INFO org.apache.spark.storage.BlockManager [Thread-152] BlockManager stopped
2023-04-26 11:54:27,792 INFO org.apache.spark.storage.BlockManagerMaster [Thread-152] BlockManagerMaster stopped
2023-04-26 11:54:27,792 INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [dispatcher-event-loop-1] OutputCommitCoordinator stopped!
2023-04-26 11:54:27,803 INFO org.apache.spark.SparkContext [Thread-152] Successfully stopped SparkContext
2023-04-26 11:54:27,804 INFO org.apache.spark.SparkContext [Thread-152] SparkContext already stopped.
2023-04-26 11:54:27,806 INFO com.zaxxer.hikari.HikariDataSource [Thread-152] HikariPool-10 - Shutdown initiated...
2023-04-26 11:54:27,809 INFO com.zaxxer.hikari.HikariDataSource [Thread-152] HikariPool-10 - Shutdown completed.
2023-04-26 11:54:28,151 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] Starting SpringBootSecurityJwtMongodbApplication using Java 17.0.6 on wks-012 with PID 102147 (/home/inferyx/git/SpringApplicationWithSecurity/target/classes started by inferyx in /home/inferyx/git/SpringApplicationWithSecurity)
2023-04-26 11:54:28,151 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] No active profile set, falling back to 1 default profile: "default"
2023-04-26 11:54:28,757 INFO org.apache.catalina.core.StandardService [restartedMain] Starting service [Tomcat]
2023-04-26 11:54:28,758 INFO org.apache.catalina.core.StandardEngine [restartedMain] Starting Servlet engine: [Apache Tomcat/9.0.65]
2023-04-26 11:54:28,785 INFO org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] [restartedMain] Initializing Spring embedded WebApplicationContext
2023-04-26 11:54:28,832 INFO org.mongodb.driver.client [restartedMain] MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync|spring-boot", "version": "4.6.1"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "5.15.0-69-generic"}, "platform": "Java/Eclipse Adoptium/17.0.6+10"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.Jep395RecordCodecProvider@269fb271]}, clusterSettings={hosts=[localhost:27017], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='30000 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, sendBufferSize=0}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, sendBufferSize=0}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=JAVA_LEGACY, serverApi=null, autoEncryptionSettings=null, contextProvider=null}
2023-04-26 11:54:28,877 INFO org.mongodb.driver.connection [cluster-ClusterId{value='6448c39cadee62176f290764', description='null'}-localhost:27017] Opened connection [connectionId{localValue:22, serverValue:63}] to localhost:27017
2023-04-26 11:54:28,926 INFO org.mongodb.driver.cluster [cluster-ClusterId{value='6448c39cadee62176f290764', description='null'}-localhost:27017] Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=6, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=43880235}
2023-04-26 11:54:28,927 INFO org.mongodb.driver.connection [cluster-rtt-ClusterId{value='6448c39cadee62176f290764', description='null'}-localhost:27017] Opened connection [connectionId{localValue:23, serverValue:64}] to localhost:27017
2023-04-26 11:54:29,091 INFO org.hibernate.jpa.internal.util.LogHelper [restartedMain] HHH000204: Processing PersistenceUnitInfo [name: default]
2023-04-26 11:54:29,107 INFO com.zaxxer.hikari.HikariDataSource [restartedMain] HikariPool-11 - Starting...
2023-04-26 11:54:29,112 INFO com.zaxxer.hikari.HikariDataSource [restartedMain] HikariPool-11 - Start completed.
2023-04-26 11:54:29,113 INFO org.hibernate.dialect.Dialect [restartedMain] HHH000400: Using dialect: org.hibernate.dialect.MySQL5Dialect
2023-04-26 11:54:29,218 INFO org.hibernate.engine.transaction.jta.platform.internal.JtaPlatformInitiator [restartedMain] HHH000490: Using JtaPlatform implementation: [org.hibernate.engine.transaction.jta.platform.internal.NoJtaPlatform]
2023-04-26 11:54:29,516 INFO org.apache.spark.SparkContext [restartedMain] Running Spark version 2.4.5
2023-04-26 11:54:29,517 INFO org.apache.spark.SparkContext [restartedMain] Submitted application: MyAppName
2023-04-26 11:54:29,519 INFO org.apache.spark.SecurityManager [restartedMain] Changing view acls to: inferyx
2023-04-26 11:54:29,519 INFO org.apache.spark.SecurityManager [restartedMain] Changing modify acls to: inferyx
2023-04-26 11:54:29,520 INFO org.apache.spark.SecurityManager [restartedMain] Changing view acls groups to: 
2023-04-26 11:54:29,520 INFO org.apache.spark.SecurityManager [restartedMain] Changing modify acls groups to: 
2023-04-26 11:54:29,520 INFO org.apache.spark.SecurityManager [restartedMain] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(inferyx); groups with view permissions: Set(); users  with modify permissions: Set(inferyx); groups with modify permissions: Set()
2023-04-26 11:54:29,568 INFO org.apache.spark.util.Utils [restartedMain] Successfully started service 'sparkDriver' on port 34301.
2023-04-26 11:54:29,570 INFO org.apache.spark.SparkEnv [restartedMain] Registering MapOutputTracker
2023-04-26 11:54:29,572 INFO org.apache.spark.SparkEnv [restartedMain] Registering BlockManagerMaster
2023-04-26 11:54:29,573 INFO org.apache.spark.storage.BlockManagerMasterEndpoint [restartedMain] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2023-04-26 11:54:29,573 INFO org.apache.spark.storage.BlockManagerMasterEndpoint [restartedMain] BlockManagerMasterEndpoint up
2023-04-26 11:54:29,574 INFO org.apache.spark.storage.DiskBlockManager [restartedMain] Created local directory at /tmp/blockmgr-0f4261e8-1650-4dd8-b515-28093fad8a44
2023-04-26 11:54:29,576 INFO org.apache.spark.storage.memory.MemoryStore [restartedMain] MemoryStore started with capacity 998.4 MB
2023-04-26 11:54:29,578 INFO org.apache.spark.SparkEnv [restartedMain] Registering OutputCommitCoordinator
2023-04-26 11:54:29,592 INFO org.spark_project.jetty.server.Server [restartedMain] jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2023-04-26 11:54:29,602 INFO org.spark_project.jetty.server.Server [restartedMain] Started @741835ms
2023-04-26 11:54:29,604 INFO org.spark_project.jetty.server.AbstractConnector [restartedMain] Started ServerConnector@10dbc22d{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2023-04-26 11:54:29,604 INFO org.apache.spark.util.Utils [restartedMain] Successfully started service 'SparkUI' on port 4040.
2023-04-26 11:54:29,605 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@269df452{/jobs,null,AVAILABLE,@Spark}
2023-04-26 11:54:29,606 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@3a4750d{/jobs/json,null,AVAILABLE,@Spark}
2023-04-26 11:54:29,606 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@7b6f71d{/jobs/job,null,AVAILABLE,@Spark}
2023-04-26 11:54:29,607 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@49b2976c{/jobs/job/json,null,AVAILABLE,@Spark}
2023-04-26 11:54:29,608 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@c4d56c{/stages,null,AVAILABLE,@Spark}
2023-04-26 11:54:29,613 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@20e8d464{/stages/json,null,AVAILABLE,@Spark}
2023-04-26 11:54:29,614 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@1056d17b{/stages/stage,null,AVAILABLE,@Spark}
2023-04-26 11:54:29,615 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@4d022dea{/stages/stage/json,null,AVAILABLE,@Spark}
2023-04-26 11:54:29,616 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@6bcbfe93{/stages/pool,null,AVAILABLE,@Spark}
2023-04-26 11:54:29,618 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@d665c7a{/stages/pool/json,null,AVAILABLE,@Spark}
2023-04-26 11:54:29,618 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@2b1d89a4{/storage,null,AVAILABLE,@Spark}
2023-04-26 11:54:29,619 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@772c8c50{/storage/json,null,AVAILABLE,@Spark}
2023-04-26 11:54:29,620 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@43a7cb40{/storage/rdd,null,AVAILABLE,@Spark}
2023-04-26 11:54:29,620 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@7c1dcac2{/storage/rdd/json,null,AVAILABLE,@Spark}
2023-04-26 11:54:29,621 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@3b0a3c4c{/environment,null,AVAILABLE,@Spark}
2023-04-26 11:54:29,622 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@28d77971{/environment/json,null,AVAILABLE,@Spark}
2023-04-26 11:54:29,623 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@55575ae0{/executors,null,AVAILABLE,@Spark}
2023-04-26 11:54:29,623 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@478ed152{/executors/json,null,AVAILABLE,@Spark}
2023-04-26 11:54:29,624 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@2d0c6db3{/executors/threadDump,null,AVAILABLE,@Spark}
2023-04-26 11:54:29,625 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@3e038592{/executors/threadDump/json,null,AVAILABLE,@Spark}
2023-04-26 11:54:29,626 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@51ec58e8{/static,null,AVAILABLE,@Spark}
2023-04-26 11:54:29,627 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@60d917c6{/,null,AVAILABLE,@Spark}
2023-04-26 11:54:29,627 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@2ef68f9b{/api,null,AVAILABLE,@Spark}
2023-04-26 11:54:29,628 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@3fe382ad{/jobs/job/kill,null,AVAILABLE,@Spark}
2023-04-26 11:54:29,628 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@172cc5b0{/stages/stage/kill,null,AVAILABLE,@Spark}
2023-04-26 11:54:29,629 INFO org.apache.spark.ui.SparkUI [restartedMain] Bound SparkUI to 0.0.0.0, and started at http://192.168.1.125:4040
2023-04-26 11:54:29,657 INFO org.apache.spark.executor.Executor [restartedMain] Starting executor ID driver on host localhost
2023-04-26 11:54:29,669 INFO org.apache.spark.util.Utils [restartedMain] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35061.
2023-04-26 11:54:29,670 INFO org.apache.spark.network.netty.NettyBlockTransferService [restartedMain] Server created on 192.168.1.125:35061
2023-04-26 11:54:29,670 INFO org.apache.spark.storage.BlockManager [restartedMain] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2023-04-26 11:54:29,671 INFO org.apache.spark.storage.BlockManagerMaster [restartedMain] Registering BlockManager BlockManagerId(driver, 192.168.1.125, 35061, None)
2023-04-26 11:54:29,672 INFO org.apache.spark.storage.BlockManagerMasterEndpoint [dispatcher-event-loop-2] Registering block manager 192.168.1.125:35061 with 998.4 MB RAM, BlockManagerId(driver, 192.168.1.125, 35061, None)
2023-04-26 11:54:29,673 INFO org.apache.spark.storage.BlockManagerMaster [restartedMain] Registered BlockManager BlockManagerId(driver, 192.168.1.125, 35061, None)
2023-04-26 11:54:29,673 INFO org.apache.spark.storage.BlockManager [restartedMain] Initialized BlockManager: BlockManagerId(driver, 192.168.1.125, 35061, None)
2023-04-26 11:54:29,675 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@3551c501{/metrics/json,null,AVAILABLE,@Spark}
2023-04-26 11:54:31,447 INFO org.apache.kafka.clients.consumer.ConsumerConfig [restartedMain] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-book-group-11
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = book-group
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2023-04-26 11:54:31,453 INFO org.apache.kafka.common.utils.AppInfoParser [restartedMain] Kafka version: 3.1.1
2023-04-26 11:54:31,453 INFO org.apache.kafka.common.utils.AppInfoParser [restartedMain] Kafka commitId: 97671528ba54a138
2023-04-26 11:54:31,453 INFO org.apache.kafka.common.utils.AppInfoParser [restartedMain] Kafka startTimeMs: 1682490271453
2023-04-26 11:54:31,454 INFO org.apache.kafka.clients.consumer.KafkaConsumer [restartedMain] [Consumer clientId=consumer-book-group-11, groupId=book-group] Subscribed to topic(s): my-topic
2023-04-26 11:54:31,463 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-11, groupId=book-group] Resetting the last seen epoch of partition my-topic-0 to 0 since the associated topicId changed from null to QqwJ2XdJQbi2BDXQbzT51g
2023-04-26 11:54:31,464 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-11, groupId=book-group] Cluster ID: 6CnV1ouTQjGsP-r9Rd2WRA
2023-04-26 11:54:31,465 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-11, groupId=book-group] Discovered group coordinator wks-012:9092 (id: 2147483647 rack: null)
2023-04-26 11:54:31,467 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-11, groupId=book-group] (Re-)joining group
2023-04-26 11:54:31,471 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-11, groupId=book-group] Request joining group due to: need to re-join with the given member-id
2023-04-26 11:54:31,471 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] Started SpringBootSecurityJwtMongodbApplication in 3.389 seconds (JVM running for 743.704)
2023-04-26 11:54:31,471 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-11, groupId=book-group] (Re-)joining group
2023-04-26 11:54:31,474 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-11, groupId=book-group] Successfully joined group with generation Generation{generationId=151, memberId='consumer-book-group-11-39fdddac-8bd9-45b0-9266-323c3034abc6', protocol='range'}
2023-04-26 11:54:31,476 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] This is a info message
2023-04-26 11:54:31,475 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-11, groupId=book-group] Finished assignment for group at generation 151: {consumer-book-group-11-39fdddac-8bd9-45b0-9266-323c3034abc6=Assignment(partitions=[my-topic-0])}
2023-04-26 11:54:31,476 WARN com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] This is a warn message
2023-04-26 11:54:31,479 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-11, groupId=book-group] Successfully synced group in generation Generation{generationId=151, memberId='consumer-book-group-11-39fdddac-8bd9-45b0-9266-323c3034abc6', protocol='range'}
2023-04-26 11:54:31,480 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-11, groupId=book-group] Notifying assignor about the new Assignment(partitions=[my-topic-0])
2023-04-26 11:54:31,480 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-11, groupId=book-group] Adding newly assigned partitions: my-topic-0
2023-04-26 11:54:31,484 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-11, groupId=book-group] Setting offset for partition my-topic-0 to the committed offset FetchPosition{offset=415, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[wks-012:9092 (id: 0 rack: null)], epoch=0}}
2023-04-26 11:54:39,872 INFO org.apache.catalina.core.StandardService [Thread-169] Stopping service [Tomcat]
2023-04-26 11:54:39,877 WARN org.apache.catalina.loader.WebappClassLoaderBase [Thread-169] The web application [ROOT] appears to have started a thread named [cluster-ClusterId{value='6448c39cadee62176f290764', description='null'}-localhost:27017] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/jdk.internal.misc.Unsafe.park(Native Method)
 java.base@17.0.6/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:252)
 java.base@17.0.6/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1672)
 app//com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.waitForSignalOrTimeout(DefaultServerMonitor.java:300)
 app//com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.waitForNext(DefaultServerMonitor.java:281)
 app//com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:179)
 java.base@17.0.6/java.lang.Thread.run(Thread.java:833)
2023-04-26 11:54:39,884 WARN org.apache.catalina.loader.WebappClassLoaderBase [Thread-169] The web application [ROOT] appears to have started a thread named [cluster-rtt-ClusterId{value='6448c39cadee62176f290764', description='null'}-localhost:27017] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/java.lang.Thread.sleep(Native Method)
 app//com.mongodb.internal.connection.DefaultServerMonitor.waitForNext(DefaultServerMonitor.java:443)
 app//com.mongodb.internal.connection.DefaultServerMonitor.access$1500(DefaultServerMonitor.java:64)
 app//com.mongodb.internal.connection.DefaultServerMonitor$RoundTripTimeRunnable.run(DefaultServerMonitor.java:415)
 java.base@17.0.6/java.lang.Thread.run(Thread.java:833)
2023-04-26 11:54:39,885 WARN org.apache.catalina.loader.WebappClassLoaderBase [Thread-169] The web application [ROOT] appears to have started a thread named [MaintenanceTimer-12-thread-1] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/jdk.internal.misc.Unsafe.park(Native Method)
 java.base@17.0.6/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:252)
 java.base@17.0.6/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1672)
 java.base@17.0.6/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182)
 java.base@17.0.6/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1062)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1122)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
 java.base@17.0.6/java.lang.Thread.run(Thread.java:833)
2023-04-26 11:54:39,889 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-11, groupId=book-group] Revoke previously assigned partitions my-topic-0
2023-04-26 11:54:39,889 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-11, groupId=book-group] Member consumer-book-group-11-39fdddac-8bd9-45b0-9266-323c3034abc6 sending LeaveGroup request to coordinator wks-012:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2023-04-26 11:54:39,896 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-11, groupId=book-group] Resetting generation due to: consumer pro-actively leaving the group
2023-04-26 11:54:39,897 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-11, groupId=book-group] Request joining group due to: consumer pro-actively leaving the group
2023-04-26 11:54:39,897 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-11, groupId=book-group] Unsubscribed all topics or patterns and assigned partitions
2023-04-26 11:54:39,898 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-11, groupId=book-group] Resetting generation due to: consumer pro-actively leaving the group
2023-04-26 11:54:39,898 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-11, groupId=book-group] Request joining group due to: consumer pro-actively leaving the group
2023-04-26 11:54:39,899 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Metrics scheduler closed
2023-04-26 11:54:39,899 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-04-26 11:54:39,899 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Metrics reporters closed
2023-04-26 11:54:39,903 INFO org.apache.kafka.common.utils.AppInfoParser [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] App info kafka.consumer for consumer-book-group-11 unregistered
2023-04-26 11:54:39,911 INFO org.spark_project.jetty.server.AbstractConnector [Thread-169] Stopped Spark@10dbc22d{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2023-04-26 11:54:39,917 INFO org.apache.spark.ui.SparkUI [Thread-169] Stopped Spark web UI at http://192.168.1.125:4040
2023-04-26 11:54:39,921 INFO org.apache.spark.MapOutputTrackerMasterEndpoint [dispatcher-event-loop-2] MapOutputTrackerMasterEndpoint stopped!
2023-04-26 11:54:39,930 INFO org.apache.spark.storage.memory.MemoryStore [Thread-169] MemoryStore cleared
2023-04-26 11:54:39,930 INFO org.apache.spark.storage.BlockManager [Thread-169] BlockManager stopped
2023-04-26 11:54:39,930 INFO org.apache.spark.storage.BlockManagerMaster [Thread-169] BlockManagerMaster stopped
2023-04-26 11:54:39,931 INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [dispatcher-event-loop-3] OutputCommitCoordinator stopped!
2023-04-26 11:54:39,943 INFO org.apache.spark.SparkContext [Thread-169] Successfully stopped SparkContext
2023-04-26 11:54:39,943 INFO org.apache.spark.SparkContext [Thread-169] SparkContext already stopped.
2023-04-26 11:54:39,945 INFO com.zaxxer.hikari.HikariDataSource [Thread-169] HikariPool-11 - Shutdown initiated...
2023-04-26 11:54:39,949 INFO com.zaxxer.hikari.HikariDataSource [Thread-169] HikariPool-11 - Shutdown completed.
2023-04-26 11:54:40,372 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] Starting SpringBootSecurityJwtMongodbApplication using Java 17.0.6 on wks-012 with PID 102147 (/home/inferyx/git/SpringApplicationWithSecurity/target/classes started by inferyx in /home/inferyx/git/SpringApplicationWithSecurity)
2023-04-26 11:54:40,373 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] No active profile set, falling back to 1 default profile: "default"
2023-04-26 11:54:41,209 INFO org.apache.catalina.core.StandardService [restartedMain] Starting service [Tomcat]
2023-04-26 11:54:41,210 INFO org.apache.catalina.core.StandardEngine [restartedMain] Starting Servlet engine: [Apache Tomcat/9.0.65]
2023-04-26 11:54:41,238 INFO org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] [restartedMain] Initializing Spring embedded WebApplicationContext
2023-04-26 11:54:41,290 INFO org.mongodb.driver.client [restartedMain] MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync|spring-boot", "version": "4.6.1"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "5.15.0-69-generic"}, "platform": "Java/Eclipse Adoptium/17.0.6+10"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.Jep395RecordCodecProvider@269fb271]}, clusterSettings={hosts=[localhost:27017], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='30000 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, sendBufferSize=0}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, sendBufferSize=0}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=JAVA_LEGACY, serverApi=null, autoEncryptionSettings=null, contextProvider=null}
2023-04-26 11:54:41,292 INFO org.mongodb.driver.connection [cluster-ClusterId{value='6448c3a9adee62176f290765', description='null'}-localhost:27017] Opened connection [connectionId{localValue:24, serverValue:65}] to localhost:27017
2023-04-26 11:54:41,292 INFO org.mongodb.driver.cluster [cluster-ClusterId{value='6448c3a9adee62176f290765', description='null'}-localhost:27017] Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=6, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1271229}
2023-04-26 11:54:41,402 INFO org.mongodb.driver.connection [cluster-rtt-ClusterId{value='6448c3a9adee62176f290765', description='null'}-localhost:27017] Opened connection [connectionId{localValue:25, serverValue:66}] to localhost:27017
2023-04-26 11:54:41,560 INFO org.hibernate.jpa.internal.util.LogHelper [restartedMain] HHH000204: Processing PersistenceUnitInfo [name: default]
2023-04-26 11:54:41,579 INFO com.zaxxer.hikari.HikariDataSource [restartedMain] HikariPool-12 - Starting...
2023-04-26 11:54:41,585 INFO com.zaxxer.hikari.HikariDataSource [restartedMain] HikariPool-12 - Start completed.
2023-04-26 11:54:41,586 INFO org.hibernate.dialect.Dialect [restartedMain] HHH000400: Using dialect: org.hibernate.dialect.MySQL5Dialect
2023-04-26 11:54:41,666 INFO org.hibernate.engine.transaction.jta.platform.internal.JtaPlatformInitiator [restartedMain] HHH000490: Using JtaPlatform implementation: [org.hibernate.engine.transaction.jta.platform.internal.NoJtaPlatform]
2023-04-26 11:54:42,037 INFO org.apache.spark.SparkContext [restartedMain] Running Spark version 2.4.5
2023-04-26 11:54:42,038 INFO org.apache.spark.SparkContext [restartedMain] Submitted application: MyAppName
2023-04-26 11:54:42,039 INFO org.apache.spark.SecurityManager [restartedMain] Changing view acls to: inferyx
2023-04-26 11:54:42,040 INFO org.apache.spark.SecurityManager [restartedMain] Changing modify acls to: inferyx
2023-04-26 11:54:42,040 INFO org.apache.spark.SecurityManager [restartedMain] Changing view acls groups to: 
2023-04-26 11:54:42,040 INFO org.apache.spark.SecurityManager [restartedMain] Changing modify acls groups to: 
2023-04-26 11:54:42,041 INFO org.apache.spark.SecurityManager [restartedMain] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(inferyx); groups with view permissions: Set(); users  with modify permissions: Set(inferyx); groups with modify permissions: Set()
2023-04-26 11:54:42,105 INFO org.apache.spark.util.Utils [restartedMain] Successfully started service 'sparkDriver' on port 46041.
2023-04-26 11:54:42,111 INFO org.apache.spark.SparkEnv [restartedMain] Registering MapOutputTracker
2023-04-26 11:54:42,113 INFO org.apache.spark.SparkEnv [restartedMain] Registering BlockManagerMaster
2023-04-26 11:54:42,114 INFO org.apache.spark.storage.BlockManagerMasterEndpoint [restartedMain] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2023-04-26 11:54:42,114 INFO org.apache.spark.storage.BlockManagerMasterEndpoint [restartedMain] BlockManagerMasterEndpoint up
2023-04-26 11:54:42,115 INFO org.apache.spark.storage.DiskBlockManager [restartedMain] Created local directory at /tmp/blockmgr-54d8e340-60c5-4d11-85dd-747a1993cc0e
2023-04-26 11:54:42,116 INFO org.apache.spark.storage.memory.MemoryStore [restartedMain] MemoryStore started with capacity 998.4 MB
2023-04-26 11:54:42,118 INFO org.apache.spark.SparkEnv [restartedMain] Registering OutputCommitCoordinator
2023-04-26 11:54:42,121 INFO org.spark_project.jetty.server.Server [restartedMain] jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2023-04-26 11:54:42,124 INFO org.spark_project.jetty.server.Server [restartedMain] Started @754357ms
2023-04-26 11:54:42,125 INFO org.spark_project.jetty.server.AbstractConnector [restartedMain] Started ServerConnector@778870e3{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2023-04-26 11:54:42,126 INFO org.apache.spark.util.Utils [restartedMain] Successfully started service 'SparkUI' on port 4040.
2023-04-26 11:54:42,126 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@70cec427{/jobs,null,AVAILABLE,@Spark}
2023-04-26 11:54:42,127 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@3c4655c4{/jobs/json,null,AVAILABLE,@Spark}
2023-04-26 11:54:42,127 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@2fc0d324{/jobs/job,null,AVAILABLE,@Spark}
2023-04-26 11:54:42,128 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@1bfbb157{/jobs/job/json,null,AVAILABLE,@Spark}
2023-04-26 11:54:42,128 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@1d92a1f3{/stages,null,AVAILABLE,@Spark}
2023-04-26 11:54:42,129 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@13742ff0{/stages/json,null,AVAILABLE,@Spark}
2023-04-26 11:54:42,130 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@3a049749{/stages/stage,null,AVAILABLE,@Spark}
2023-04-26 11:54:42,130 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@7cb3f857{/stages/stage/json,null,AVAILABLE,@Spark}
2023-04-26 11:54:42,131 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@7426530b{/stages/pool,null,AVAILABLE,@Spark}
2023-04-26 11:54:42,132 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@461ff9d0{/stages/pool/json,null,AVAILABLE,@Spark}
2023-04-26 11:54:42,132 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@583f9085{/storage,null,AVAILABLE,@Spark}
2023-04-26 11:54:42,133 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@3badba95{/storage/json,null,AVAILABLE,@Spark}
2023-04-26 11:54:42,133 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@7d5508e0{/storage/rdd,null,AVAILABLE,@Spark}
2023-04-26 11:54:42,134 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@443dafe3{/storage/rdd/json,null,AVAILABLE,@Spark}
2023-04-26 11:54:42,135 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@60a2b459{/environment,null,AVAILABLE,@Spark}
2023-04-26 11:54:42,135 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@368dc3ca{/environment/json,null,AVAILABLE,@Spark}
2023-04-26 11:54:42,136 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@634acff3{/executors,null,AVAILABLE,@Spark}
2023-04-26 11:54:42,136 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@4a2136ce{/executors/json,null,AVAILABLE,@Spark}
2023-04-26 11:54:42,137 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@3fd443af{/executors/threadDump,null,AVAILABLE,@Spark}
2023-04-26 11:54:42,138 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@679f540c{/executors/threadDump/json,null,AVAILABLE,@Spark}
2023-04-26 11:54:42,139 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@529f3a5b{/static,null,AVAILABLE,@Spark}
2023-04-26 11:54:42,139 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@114be180{/,null,AVAILABLE,@Spark}
2023-04-26 11:54:42,140 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@c9f113d{/api,null,AVAILABLE,@Spark}
2023-04-26 11:54:42,140 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@3adba608{/jobs/job/kill,null,AVAILABLE,@Spark}
2023-04-26 11:54:42,141 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@74d20732{/stages/stage/kill,null,AVAILABLE,@Spark}
2023-04-26 11:54:42,141 INFO org.apache.spark.ui.SparkUI [restartedMain] Bound SparkUI to 0.0.0.0, and started at http://192.168.1.125:4040
2023-04-26 11:54:42,171 INFO org.apache.spark.executor.Executor [restartedMain] Starting executor ID driver on host localhost
2023-04-26 11:54:42,178 INFO org.apache.spark.util.Utils [restartedMain] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36763.
2023-04-26 11:54:42,179 INFO org.apache.spark.network.netty.NettyBlockTransferService [restartedMain] Server created on 192.168.1.125:36763
2023-04-26 11:54:42,179 INFO org.apache.spark.storage.BlockManager [restartedMain] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2023-04-26 11:54:42,179 INFO org.apache.spark.storage.BlockManagerMaster [restartedMain] Registering BlockManager BlockManagerId(driver, 192.168.1.125, 36763, None)
2023-04-26 11:54:42,180 INFO org.apache.spark.storage.BlockManagerMasterEndpoint [dispatcher-event-loop-2] Registering block manager 192.168.1.125:36763 with 998.4 MB RAM, BlockManagerId(driver, 192.168.1.125, 36763, None)
2023-04-26 11:54:42,181 INFO org.apache.spark.storage.BlockManagerMaster [restartedMain] Registered BlockManager BlockManagerId(driver, 192.168.1.125, 36763, None)
2023-04-26 11:54:42,181 INFO org.apache.spark.storage.BlockManager [restartedMain] Initialized BlockManager: BlockManagerId(driver, 192.168.1.125, 36763, None)
2023-04-26 11:54:42,183 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@61b0d56c{/metrics/json,null,AVAILABLE,@Spark}
2023-04-26 11:54:43,764 INFO org.apache.kafka.clients.consumer.ConsumerConfig [restartedMain] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-book-group-12
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = book-group
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2023-04-26 11:54:43,770 INFO org.apache.kafka.common.utils.AppInfoParser [restartedMain] Kafka version: 3.1.1
2023-04-26 11:54:43,770 INFO org.apache.kafka.common.utils.AppInfoParser [restartedMain] Kafka commitId: 97671528ba54a138
2023-04-26 11:54:43,771 INFO org.apache.kafka.common.utils.AppInfoParser [restartedMain] Kafka startTimeMs: 1682490283770
2023-04-26 11:54:43,771 INFO org.apache.kafka.clients.consumer.KafkaConsumer [restartedMain] [Consumer clientId=consumer-book-group-12, groupId=book-group] Subscribed to topic(s): my-topic
2023-04-26 11:54:43,779 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-12, groupId=book-group] Resetting the last seen epoch of partition my-topic-0 to 0 since the associated topicId changed from null to QqwJ2XdJQbi2BDXQbzT51g
2023-04-26 11:54:43,779 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-12, groupId=book-group] Cluster ID: 6CnV1ouTQjGsP-r9Rd2WRA
2023-04-26 11:54:43,780 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-12, groupId=book-group] Discovered group coordinator wks-012:9092 (id: 2147483647 rack: null)
2023-04-26 11:54:43,786 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-12, groupId=book-group] (Re-)joining group
2023-04-26 11:54:43,791 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-12, groupId=book-group] Request joining group due to: need to re-join with the given member-id
2023-04-26 11:54:43,791 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-12, groupId=book-group] (Re-)joining group
2023-04-26 11:54:43,794 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-12, groupId=book-group] Successfully joined group with generation Generation{generationId=153, memberId='consumer-book-group-12-d9db6d2a-572a-4930-91a7-6474f199bd1c', protocol='range'}
2023-04-26 11:54:43,794 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-12, groupId=book-group] Finished assignment for group at generation 153: {consumer-book-group-12-d9db6d2a-572a-4930-91a7-6474f199bd1c=Assignment(partitions=[my-topic-0])}
2023-04-26 11:54:43,799 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-12, groupId=book-group] Successfully synced group in generation Generation{generationId=153, memberId='consumer-book-group-12-d9db6d2a-572a-4930-91a7-6474f199bd1c', protocol='range'}
2023-04-26 11:54:43,800 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-12, groupId=book-group] Notifying assignor about the new Assignment(partitions=[my-topic-0])
2023-04-26 11:54:43,800 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-12, groupId=book-group] Adding newly assigned partitions: my-topic-0
2023-04-26 11:54:43,808 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-12, groupId=book-group] Setting offset for partition my-topic-0 to the committed offset FetchPosition{offset=415, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[wks-012:9092 (id: 0 rack: null)], epoch=0}}
2023-04-26 11:54:43,830 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] Started SpringBootSecurityJwtMongodbApplication in 3.565 seconds (JVM running for 756.063)
2023-04-26 11:54:43,835 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] This is a info message
2023-04-26 11:54:43,835 WARN com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] This is a warn message
2023-04-26 11:57:07,791 INFO org.apache.catalina.core.StandardService [Thread-185] Stopping service [Tomcat]
2023-04-26 11:57:07,793 WARN org.apache.catalina.loader.WebappClassLoaderBase [Thread-185] The web application [ROOT] appears to have started a thread named [cluster-ClusterId{value='6448c3a9adee62176f290765', description='null'}-localhost:27017] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/jdk.internal.misc.Unsafe.park(Native Method)
 java.base@17.0.6/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:252)
 java.base@17.0.6/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1672)
 app//com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.waitForSignalOrTimeout(DefaultServerMonitor.java:300)
 app//com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.waitForNext(DefaultServerMonitor.java:281)
 app//com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:179)
 java.base@17.0.6/java.lang.Thread.run(Thread.java:833)
2023-04-26 11:57:07,794 WARN org.apache.catalina.loader.WebappClassLoaderBase [Thread-185] The web application [ROOT] appears to have started a thread named [cluster-rtt-ClusterId{value='6448c3a9adee62176f290765', description='null'}-localhost:27017] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/java.lang.Thread.sleep(Native Method)
 app//com.mongodb.internal.connection.DefaultServerMonitor.waitForNext(DefaultServerMonitor.java:443)
 app//com.mongodb.internal.connection.DefaultServerMonitor.access$1500(DefaultServerMonitor.java:64)
 app//com.mongodb.internal.connection.DefaultServerMonitor$RoundTripTimeRunnable.run(DefaultServerMonitor.java:415)
 java.base@17.0.6/java.lang.Thread.run(Thread.java:833)
2023-04-26 11:57:07,794 WARN org.apache.catalina.loader.WebappClassLoaderBase [Thread-185] The web application [ROOT] appears to have started a thread named [MaintenanceTimer-13-thread-1] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/jdk.internal.misc.Unsafe.park(Native Method)
 java.base@17.0.6/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:252)
 java.base@17.0.6/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1672)
 java.base@17.0.6/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182)
 java.base@17.0.6/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1062)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1122)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
 java.base@17.0.6/java.lang.Thread.run(Thread.java:833)
2023-04-26 11:57:07,796 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-12, groupId=book-group] Revoke previously assigned partitions my-topic-0
2023-04-26 11:57:07,797 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-12, groupId=book-group] Member consumer-book-group-12-d9db6d2a-572a-4930-91a7-6474f199bd1c sending LeaveGroup request to coordinator wks-012:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2023-04-26 11:57:07,797 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-12, groupId=book-group] Resetting generation due to: consumer pro-actively leaving the group
2023-04-26 11:57:07,798 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-12, groupId=book-group] Request joining group due to: consumer pro-actively leaving the group
2023-04-26 11:57:07,798 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-12, groupId=book-group] Unsubscribed all topics or patterns and assigned partitions
2023-04-26 11:57:07,799 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-12, groupId=book-group] Resetting generation due to: consumer pro-actively leaving the group
2023-04-26 11:57:07,799 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-12, groupId=book-group] Request joining group due to: consumer pro-actively leaving the group
2023-04-26 11:57:07,800 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Metrics scheduler closed
2023-04-26 11:57:07,801 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-04-26 11:57:07,801 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Metrics reporters closed
2023-04-26 11:57:07,804 INFO org.apache.kafka.common.utils.AppInfoParser [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] App info kafka.consumer for consumer-book-group-12 unregistered
2023-04-26 11:57:07,814 INFO org.spark_project.jetty.server.AbstractConnector [Thread-185] Stopped Spark@778870e3{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2023-04-26 11:57:07,817 INFO org.apache.spark.ui.SparkUI [Thread-185] Stopped Spark web UI at http://192.168.1.125:4040
2023-04-26 11:57:07,819 INFO org.apache.spark.MapOutputTrackerMasterEndpoint [dispatcher-event-loop-0] MapOutputTrackerMasterEndpoint stopped!
2023-04-26 11:57:07,830 INFO org.apache.spark.storage.memory.MemoryStore [Thread-185] MemoryStore cleared
2023-04-26 11:57:07,830 INFO org.apache.spark.storage.BlockManager [Thread-185] BlockManager stopped
2023-04-26 11:57:07,831 INFO org.apache.spark.storage.BlockManagerMaster [Thread-185] BlockManagerMaster stopped
2023-04-26 11:57:07,832 INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [dispatcher-event-loop-0] OutputCommitCoordinator stopped!
2023-04-26 11:57:07,838 INFO org.apache.spark.SparkContext [Thread-185] Successfully stopped SparkContext
2023-04-26 11:57:07,838 INFO org.apache.spark.SparkContext [Thread-185] SparkContext already stopped.
2023-04-26 11:57:07,839 INFO com.zaxxer.hikari.HikariDataSource [Thread-185] HikariPool-12 - Shutdown initiated...
2023-04-26 11:57:07,842 INFO com.zaxxer.hikari.HikariDataSource [Thread-185] HikariPool-12 - Shutdown completed.
2023-04-26 11:57:08,049 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] Starting SpringBootSecurityJwtMongodbApplication using Java 17.0.6 on wks-012 with PID 102147 (/home/inferyx/git/SpringApplicationWithSecurity/target/classes started by inferyx in /home/inferyx/git/SpringApplicationWithSecurity)
2023-04-26 11:57:08,050 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] No active profile set, falling back to 1 default profile: "default"
2023-04-26 11:57:08,756 INFO org.apache.catalina.core.StandardService [restartedMain] Starting service [Tomcat]
2023-04-26 11:57:08,756 INFO org.apache.catalina.core.StandardEngine [restartedMain] Starting Servlet engine: [Apache Tomcat/9.0.65]
2023-04-26 11:57:08,796 INFO org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] [restartedMain] Initializing Spring embedded WebApplicationContext
2023-04-26 11:57:08,863 INFO org.mongodb.driver.connection [cluster-ClusterId{value='6448c43cadee62176f290766', description='null'}-localhost:27017] Opened connection [connectionId{localValue:26, serverValue:67}] to localhost:27017
2023-04-26 11:57:08,863 INFO org.mongodb.driver.cluster [cluster-ClusterId{value='6448c43cadee62176f290766', description='null'}-localhost:27017] Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=6, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=847272}
2023-04-26 11:57:08,864 INFO org.mongodb.driver.client [restartedMain] MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync|spring-boot", "version": "4.6.1"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "5.15.0-69-generic"}, "platform": "Java/Eclipse Adoptium/17.0.6+10"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.Jep395RecordCodecProvider@269fb271]}, clusterSettings={hosts=[localhost:27017], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='30000 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, sendBufferSize=0}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, sendBufferSize=0}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=JAVA_LEGACY, serverApi=null, autoEncryptionSettings=null, contextProvider=null}
2023-04-26 11:57:08,870 INFO org.mongodb.driver.connection [cluster-rtt-ClusterId{value='6448c43cadee62176f290766', description='null'}-localhost:27017] Opened connection [connectionId{localValue:27, serverValue:68}] to localhost:27017
2023-04-26 11:57:09,166 INFO org.hibernate.jpa.internal.util.LogHelper [restartedMain] HHH000204: Processing PersistenceUnitInfo [name: default]
2023-04-26 11:57:09,186 INFO com.zaxxer.hikari.HikariDataSource [restartedMain] HikariPool-13 - Starting...
2023-04-26 11:57:09,193 INFO com.zaxxer.hikari.HikariDataSource [restartedMain] HikariPool-13 - Start completed.
2023-04-26 11:57:09,194 INFO org.hibernate.dialect.Dialect [restartedMain] HHH000400: Using dialect: org.hibernate.dialect.MySQL5Dialect
2023-04-26 11:57:09,293 INFO org.hibernate.engine.transaction.jta.platform.internal.JtaPlatformInitiator [restartedMain] HHH000490: Using JtaPlatform implementation: [org.hibernate.engine.transaction.jta.platform.internal.NoJtaPlatform]
2023-04-26 11:57:09,621 INFO org.apache.spark.SparkContext [restartedMain] Running Spark version 2.4.5
2023-04-26 11:57:09,622 INFO org.apache.spark.SparkContext [restartedMain] Submitted application: MyAppName
2023-04-26 11:57:09,624 INFO org.apache.spark.SecurityManager [restartedMain] Changing view acls to: inferyx
2023-04-26 11:57:09,625 INFO org.apache.spark.SecurityManager [restartedMain] Changing modify acls to: inferyx
2023-04-26 11:57:09,625 INFO org.apache.spark.SecurityManager [restartedMain] Changing view acls groups to: 
2023-04-26 11:57:09,625 INFO org.apache.spark.SecurityManager [restartedMain] Changing modify acls groups to: 
2023-04-26 11:57:09,625 INFO org.apache.spark.SecurityManager [restartedMain] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(inferyx); groups with view permissions: Set(); users  with modify permissions: Set(inferyx); groups with modify permissions: Set()
2023-04-26 11:57:09,686 INFO org.apache.spark.util.Utils [restartedMain] Successfully started service 'sparkDriver' on port 39307.
2023-04-26 11:57:09,688 INFO org.apache.spark.SparkEnv [restartedMain] Registering MapOutputTracker
2023-04-26 11:57:09,690 INFO org.apache.spark.SparkEnv [restartedMain] Registering BlockManagerMaster
2023-04-26 11:57:09,691 INFO org.apache.spark.storage.BlockManagerMasterEndpoint [restartedMain] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2023-04-26 11:57:09,692 INFO org.apache.spark.storage.BlockManagerMasterEndpoint [restartedMain] BlockManagerMasterEndpoint up
2023-04-26 11:57:09,693 INFO org.apache.spark.storage.DiskBlockManager [restartedMain] Created local directory at /tmp/blockmgr-9d533f98-8d1b-4108-b15a-8d9859326575
2023-04-26 11:57:09,693 INFO org.apache.spark.storage.memory.MemoryStore [restartedMain] MemoryStore started with capacity 998.4 MB
2023-04-26 11:57:09,695 INFO org.apache.spark.SparkEnv [restartedMain] Registering OutputCommitCoordinator
2023-04-26 11:57:09,702 INFO org.spark_project.jetty.server.Server [restartedMain] jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2023-04-26 11:57:09,704 INFO org.spark_project.jetty.server.Server [restartedMain] Started @901936ms
2023-04-26 11:57:09,705 INFO org.spark_project.jetty.server.AbstractConnector [restartedMain] Started ServerConnector@70bec08b{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2023-04-26 11:57:09,706 INFO org.apache.spark.util.Utils [restartedMain] Successfully started service 'SparkUI' on port 4040.
2023-04-26 11:57:09,706 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@76c0d702{/jobs,null,AVAILABLE,@Spark}
2023-04-26 11:57:09,707 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@27f59b1{/jobs/json,null,AVAILABLE,@Spark}
2023-04-26 11:57:09,708 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@7a6593a9{/jobs/job,null,AVAILABLE,@Spark}
2023-04-26 11:57:09,710 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@59755507{/jobs/job/json,null,AVAILABLE,@Spark}
2023-04-26 11:57:09,711 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@29fdf1ec{/stages,null,AVAILABLE,@Spark}
2023-04-26 11:57:09,712 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@51b797b3{/stages/json,null,AVAILABLE,@Spark}
2023-04-26 11:57:09,713 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@719aef7{/stages/stage,null,AVAILABLE,@Spark}
2023-04-26 11:57:09,714 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@74b70e7e{/stages/stage/json,null,AVAILABLE,@Spark}
2023-04-26 11:57:09,715 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@3203f395{/stages/pool,null,AVAILABLE,@Spark}
2023-04-26 11:57:09,716 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@5f64f069{/stages/pool/json,null,AVAILABLE,@Spark}
2023-04-26 11:57:09,717 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@15408364{/storage,null,AVAILABLE,@Spark}
2023-04-26 11:57:09,718 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@59c8bb4c{/storage/json,null,AVAILABLE,@Spark}
2023-04-26 11:57:09,718 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@747a74bb{/storage/rdd,null,AVAILABLE,@Spark}
2023-04-26 11:57:09,719 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@ce525e5{/storage/rdd/json,null,AVAILABLE,@Spark}
2023-04-26 11:57:09,719 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@1dafdcbe{/environment,null,AVAILABLE,@Spark}
2023-04-26 11:57:09,720 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@1a1169c{/environment/json,null,AVAILABLE,@Spark}
2023-04-26 11:57:09,720 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@26d965bf{/executors,null,AVAILABLE,@Spark}
2023-04-26 11:57:09,720 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@336b68c{/executors/json,null,AVAILABLE,@Spark}
2023-04-26 11:57:09,721 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@601fc078{/executors/threadDump,null,AVAILABLE,@Spark}
2023-04-26 11:57:09,721 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@69033f11{/executors/threadDump/json,null,AVAILABLE,@Spark}
2023-04-26 11:57:09,722 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@49a4b13{/static,null,AVAILABLE,@Spark}
2023-04-26 11:57:09,723 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@322e0fda{/,null,AVAILABLE,@Spark}
2023-04-26 11:57:09,723 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@2c2c05e7{/api,null,AVAILABLE,@Spark}
2023-04-26 11:57:09,724 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@ab34680{/jobs/job/kill,null,AVAILABLE,@Spark}
2023-04-26 11:57:09,725 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@4821c8d3{/stages/stage/kill,null,AVAILABLE,@Spark}
2023-04-26 11:57:09,725 INFO org.apache.spark.ui.SparkUI [restartedMain] Bound SparkUI to 0.0.0.0, and started at http://192.168.1.125:4040
2023-04-26 11:57:09,760 INFO org.apache.spark.executor.Executor [restartedMain] Starting executor ID driver on host localhost
2023-04-26 11:57:09,772 INFO org.apache.spark.util.Utils [restartedMain] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41569.
2023-04-26 11:57:09,772 INFO org.apache.spark.network.netty.NettyBlockTransferService [restartedMain] Server created on 192.168.1.125:41569
2023-04-26 11:57:09,773 INFO org.apache.spark.storage.BlockManager [restartedMain] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2023-04-26 11:57:09,773 INFO org.apache.spark.storage.BlockManagerMaster [restartedMain] Registering BlockManager BlockManagerId(driver, 192.168.1.125, 41569, None)
2023-04-26 11:57:09,774 INFO org.apache.spark.storage.BlockManagerMasterEndpoint [dispatcher-event-loop-2] Registering block manager 192.168.1.125:41569 with 998.4 MB RAM, BlockManagerId(driver, 192.168.1.125, 41569, None)
2023-04-26 11:57:09,776 INFO org.apache.spark.storage.BlockManagerMaster [restartedMain] Registered BlockManager BlockManagerId(driver, 192.168.1.125, 41569, None)
2023-04-26 11:57:09,776 INFO org.apache.spark.storage.BlockManager [restartedMain] Initialized BlockManager: BlockManagerId(driver, 192.168.1.125, 41569, None)
2023-04-26 11:57:09,778 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@57cd2f16{/metrics/json,null,AVAILABLE,@Spark}
2023-04-26 11:57:11,157 INFO org.apache.kafka.clients.consumer.ConsumerConfig [restartedMain] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-book-group-13
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = book-group
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2023-04-26 11:57:11,161 INFO org.apache.kafka.common.utils.AppInfoParser [restartedMain] Kafka version: 3.1.1
2023-04-26 11:57:11,161 INFO org.apache.kafka.common.utils.AppInfoParser [restartedMain] Kafka commitId: 97671528ba54a138
2023-04-26 11:57:11,162 INFO org.apache.kafka.common.utils.AppInfoParser [restartedMain] Kafka startTimeMs: 1682490431161
2023-04-26 11:57:11,162 INFO org.apache.kafka.clients.consumer.KafkaConsumer [restartedMain] [Consumer clientId=consumer-book-group-13, groupId=book-group] Subscribed to topic(s): my-topic
2023-04-26 11:57:11,196 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-13, groupId=book-group] Resetting the last seen epoch of partition my-topic-0 to 0 since the associated topicId changed from null to QqwJ2XdJQbi2BDXQbzT51g
2023-04-26 11:57:11,197 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-13, groupId=book-group] Cluster ID: 6CnV1ouTQjGsP-r9Rd2WRA
2023-04-26 11:57:11,198 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-13, groupId=book-group] Discovered group coordinator wks-012:9092 (id: 2147483647 rack: null)
2023-04-26 11:57:11,208 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-13, groupId=book-group] (Re-)joining group
2023-04-26 11:57:11,213 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-13, groupId=book-group] Request joining group due to: need to re-join with the given member-id
2023-04-26 11:57:11,213 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-13, groupId=book-group] (Re-)joining group
2023-04-26 11:57:11,217 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-13, groupId=book-group] Successfully joined group with generation Generation{generationId=155, memberId='consumer-book-group-13-f22ed525-94e1-40c7-8656-5ded2c8fefe5', protocol='range'}
2023-04-26 11:57:11,217 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-13, groupId=book-group] Finished assignment for group at generation 155: {consumer-book-group-13-f22ed525-94e1-40c7-8656-5ded2c8fefe5=Assignment(partitions=[my-topic-0])}
2023-04-26 11:57:11,221 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-13, groupId=book-group] Successfully synced group in generation Generation{generationId=155, memberId='consumer-book-group-13-f22ed525-94e1-40c7-8656-5ded2c8fefe5', protocol='range'}
2023-04-26 11:57:11,221 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-13, groupId=book-group] Notifying assignor about the new Assignment(partitions=[my-topic-0])
2023-04-26 11:57:11,222 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-13, groupId=book-group] Adding newly assigned partitions: my-topic-0
2023-04-26 11:57:11,230 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-13, groupId=book-group] Setting offset for partition my-topic-0 to the committed offset FetchPosition{offset=415, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[wks-012:9092 (id: 0 rack: null)], epoch=0}}
2023-04-26 11:57:11,280 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] Started SpringBootSecurityJwtMongodbApplication in 3.276 seconds (JVM running for 903.513)
2023-04-26 11:57:11,285 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] This is a info message
2023-04-26 11:57:11,286 WARN com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] This is a warn message
2023-04-26 11:57:20,585 INFO org.apache.catalina.core.StandardService [Thread-202] Stopping service [Tomcat]
2023-04-26 11:57:20,601 WARN org.apache.catalina.loader.WebappClassLoaderBase [Thread-202] The web application [ROOT] appears to have started a thread named [cluster-ClusterId{value='6448c43cadee62176f290766', description='null'}-localhost:27017] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/jdk.internal.misc.Unsafe.park(Native Method)
 java.base@17.0.6/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:252)
 java.base@17.0.6/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1672)
 app//com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.waitForSignalOrTimeout(DefaultServerMonitor.java:300)
 app//com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.waitForNext(DefaultServerMonitor.java:281)
 app//com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:179)
 java.base@17.0.6/java.lang.Thread.run(Thread.java:833)
2023-04-26 11:57:20,606 WARN org.apache.catalina.loader.WebappClassLoaderBase [Thread-202] The web application [ROOT] appears to have started a thread named [cluster-rtt-ClusterId{value='6448c43cadee62176f290766', description='null'}-localhost:27017] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/java.lang.Thread.sleep(Native Method)
 app//com.mongodb.internal.connection.DefaultServerMonitor.waitForNext(DefaultServerMonitor.java:443)
 app//com.mongodb.internal.connection.DefaultServerMonitor.access$1500(DefaultServerMonitor.java:64)
 app//com.mongodb.internal.connection.DefaultServerMonitor$RoundTripTimeRunnable.run(DefaultServerMonitor.java:415)
 java.base@17.0.6/java.lang.Thread.run(Thread.java:833)
2023-04-26 11:57:20,607 WARN org.apache.catalina.loader.WebappClassLoaderBase [Thread-202] The web application [ROOT] appears to have started a thread named [MaintenanceTimer-14-thread-1] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/jdk.internal.misc.Unsafe.park(Native Method)
 java.base@17.0.6/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:252)
 java.base@17.0.6/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1672)
 java.base@17.0.6/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182)
 java.base@17.0.6/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1062)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1122)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
 java.base@17.0.6/java.lang.Thread.run(Thread.java:833)
2023-04-26 11:57:20,611 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-13, groupId=book-group] Revoke previously assigned partitions my-topic-0
2023-04-26 11:57:20,611 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-13, groupId=book-group] Member consumer-book-group-13-f22ed525-94e1-40c7-8656-5ded2c8fefe5 sending LeaveGroup request to coordinator wks-012:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2023-04-26 11:57:20,616 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-13, groupId=book-group] Resetting generation due to: consumer pro-actively leaving the group
2023-04-26 11:57:20,616 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-13, groupId=book-group] Request joining group due to: consumer pro-actively leaving the group
2023-04-26 11:57:20,617 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-13, groupId=book-group] Unsubscribed all topics or patterns and assigned partitions
2023-04-26 11:57:20,617 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-13, groupId=book-group] Resetting generation due to: consumer pro-actively leaving the group
2023-04-26 11:57:20,617 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-13, groupId=book-group] Request joining group due to: consumer pro-actively leaving the group
2023-04-26 11:57:20,618 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Metrics scheduler closed
2023-04-26 11:57:20,618 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-04-26 11:57:20,618 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Metrics reporters closed
2023-04-26 11:57:20,622 INFO org.apache.kafka.common.utils.AppInfoParser [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] App info kafka.consumer for consumer-book-group-13 unregistered
2023-04-26 11:57:20,629 INFO org.spark_project.jetty.server.AbstractConnector [Thread-202] Stopped Spark@70bec08b{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2023-04-26 11:57:20,631 INFO org.apache.spark.ui.SparkUI [Thread-202] Stopped Spark web UI at http://192.168.1.125:4040
2023-04-26 11:57:20,636 INFO org.apache.spark.MapOutputTrackerMasterEndpoint [dispatcher-event-loop-2] MapOutputTrackerMasterEndpoint stopped!
2023-04-26 11:57:20,728 INFO org.apache.spark.storage.memory.MemoryStore [Thread-202] MemoryStore cleared
2023-04-26 11:57:20,728 INFO org.apache.spark.storage.BlockManager [Thread-202] BlockManager stopped
2023-04-26 11:57:20,729 INFO org.apache.spark.storage.BlockManagerMaster [Thread-202] BlockManagerMaster stopped
2023-04-26 11:57:20,730 INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [dispatcher-event-loop-3] OutputCommitCoordinator stopped!
2023-04-26 11:57:20,769 INFO org.apache.spark.SparkContext [Thread-202] Successfully stopped SparkContext
2023-04-26 11:57:20,769 INFO org.apache.spark.SparkContext [Thread-202] SparkContext already stopped.
2023-04-26 11:57:20,771 INFO com.zaxxer.hikari.HikariDataSource [Thread-202] HikariPool-13 - Shutdown initiated...
2023-04-26 11:57:20,819 INFO com.zaxxer.hikari.HikariDataSource [Thread-202] HikariPool-13 - Shutdown completed.
2023-04-26 11:57:21,329 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] Starting SpringBootSecurityJwtMongodbApplication using Java 17.0.6 on wks-012 with PID 102147 (/home/inferyx/git/SpringApplicationWithSecurity/target/classes started by inferyx in /home/inferyx/git/SpringApplicationWithSecurity)
2023-04-26 11:57:21,330 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] No active profile set, falling back to 1 default profile: "default"
2023-04-26 11:57:22,239 INFO org.apache.catalina.core.StandardService [restartedMain] Starting service [Tomcat]
2023-04-26 11:57:22,239 INFO org.apache.catalina.core.StandardEngine [restartedMain] Starting Servlet engine: [Apache Tomcat/9.0.65]
2023-04-26 11:57:22,268 INFO org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] [restartedMain] Initializing Spring embedded WebApplicationContext
2023-04-26 11:57:22,311 INFO org.mongodb.driver.client [restartedMain] MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync|spring-boot", "version": "4.6.1"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "5.15.0-69-generic"}, "platform": "Java/Eclipse Adoptium/17.0.6+10"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.Jep395RecordCodecProvider@269fb271]}, clusterSettings={hosts=[localhost:27017], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='30000 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, sendBufferSize=0}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, sendBufferSize=0}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=JAVA_LEGACY, serverApi=null, autoEncryptionSettings=null, contextProvider=null}
2023-04-26 11:57:22,313 INFO org.mongodb.driver.connection [cluster-ClusterId{value='6448c44aadee62176f290767', description='null'}-localhost:27017] Opened connection [connectionId{localValue:28, serverValue:69}] to localhost:27017
2023-04-26 11:57:22,313 INFO org.mongodb.driver.cluster [cluster-ClusterId{value='6448c44aadee62176f290767', description='null'}-localhost:27017] Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=6, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=900058}
2023-04-26 11:57:22,314 INFO org.mongodb.driver.connection [cluster-rtt-ClusterId{value='6448c44aadee62176f290767', description='null'}-localhost:27017] Opened connection [connectionId{localValue:29, serverValue:70}] to localhost:27017
2023-04-26 11:57:22,603 INFO org.hibernate.jpa.internal.util.LogHelper [restartedMain] HHH000204: Processing PersistenceUnitInfo [name: default]
2023-04-26 11:57:22,634 INFO com.zaxxer.hikari.HikariDataSource [restartedMain] HikariPool-14 - Starting...
2023-04-26 11:57:22,639 INFO com.zaxxer.hikari.HikariDataSource [restartedMain] HikariPool-14 - Start completed.
2023-04-26 11:57:22,639 INFO org.hibernate.dialect.Dialect [restartedMain] HHH000400: Using dialect: org.hibernate.dialect.MySQL5Dialect
2023-04-26 11:57:22,717 INFO org.hibernate.engine.transaction.jta.platform.internal.JtaPlatformInitiator [restartedMain] HHH000490: Using JtaPlatform implementation: [org.hibernate.engine.transaction.jta.platform.internal.NoJtaPlatform]
2023-04-26 11:57:23,013 INFO org.apache.spark.SparkContext [restartedMain] Running Spark version 2.4.5
2023-04-26 11:57:23,013 INFO org.apache.spark.SparkContext [restartedMain] Submitted application: MyAppName
2023-04-26 11:57:23,014 INFO org.apache.spark.SecurityManager [restartedMain] Changing view acls to: inferyx
2023-04-26 11:57:23,015 INFO org.apache.spark.SecurityManager [restartedMain] Changing modify acls to: inferyx
2023-04-26 11:57:23,015 INFO org.apache.spark.SecurityManager [restartedMain] Changing view acls groups to: 
2023-04-26 11:57:23,015 INFO org.apache.spark.SecurityManager [restartedMain] Changing modify acls groups to: 
2023-04-26 11:57:23,015 INFO org.apache.spark.SecurityManager [restartedMain] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(inferyx); groups with view permissions: Set(); users  with modify permissions: Set(inferyx); groups with modify permissions: Set()
2023-04-26 11:57:23,072 INFO org.apache.spark.util.Utils [restartedMain] Successfully started service 'sparkDriver' on port 44699.
2023-04-26 11:57:23,081 INFO org.apache.spark.SparkEnv [restartedMain] Registering MapOutputTracker
2023-04-26 11:57:23,083 INFO org.apache.spark.SparkEnv [restartedMain] Registering BlockManagerMaster
2023-04-26 11:57:23,083 INFO org.apache.spark.storage.BlockManagerMasterEndpoint [restartedMain] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2023-04-26 11:57:23,084 INFO org.apache.spark.storage.BlockManagerMasterEndpoint [restartedMain] BlockManagerMasterEndpoint up
2023-04-26 11:57:23,085 INFO org.apache.spark.storage.DiskBlockManager [restartedMain] Created local directory at /tmp/blockmgr-dd353b8b-c7ef-4495-b1aa-248b684cb271
2023-04-26 11:57:23,086 INFO org.apache.spark.storage.memory.MemoryStore [restartedMain] MemoryStore started with capacity 998.4 MB
2023-04-26 11:57:23,088 INFO org.apache.spark.SparkEnv [restartedMain] Registering OutputCommitCoordinator
2023-04-26 11:57:23,094 INFO org.spark_project.jetty.server.Server [restartedMain] jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2023-04-26 11:57:23,097 INFO org.spark_project.jetty.server.Server [restartedMain] Started @915330ms
2023-04-26 11:57:23,098 INFO org.spark_project.jetty.server.AbstractConnector [restartedMain] Started ServerConnector@7632e0b1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2023-04-26 11:57:23,098 INFO org.apache.spark.util.Utils [restartedMain] Successfully started service 'SparkUI' on port 4040.
2023-04-26 11:57:23,099 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@544047da{/jobs,null,AVAILABLE,@Spark}
2023-04-26 11:57:23,100 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@7628ff41{/jobs/json,null,AVAILABLE,@Spark}
2023-04-26 11:57:23,100 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@5d58cc40{/jobs/job,null,AVAILABLE,@Spark}
2023-04-26 11:57:23,101 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@3c8e8832{/jobs/job/json,null,AVAILABLE,@Spark}
2023-04-26 11:57:23,101 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@6aa666ee{/stages,null,AVAILABLE,@Spark}
2023-04-26 11:57:23,102 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@5bc54e0c{/stages/json,null,AVAILABLE,@Spark}
2023-04-26 11:57:23,102 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@40e4a3f8{/stages/stage,null,AVAILABLE,@Spark}
2023-04-26 11:57:23,103 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@8636cce{/stages/stage/json,null,AVAILABLE,@Spark}
2023-04-26 11:57:23,104 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@51abfa3a{/stages/pool,null,AVAILABLE,@Spark}
2023-04-26 11:57:23,105 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@20ef7b2d{/stages/pool/json,null,AVAILABLE,@Spark}
2023-04-26 11:57:23,106 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@45f1e7ab{/storage,null,AVAILABLE,@Spark}
2023-04-26 11:57:23,106 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@5667e6c7{/storage/json,null,AVAILABLE,@Spark}
2023-04-26 11:57:23,107 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@5893bc1b{/storage/rdd,null,AVAILABLE,@Spark}
2023-04-26 11:57:23,108 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@3fe8237{/storage/rdd/json,null,AVAILABLE,@Spark}
2023-04-26 11:57:23,108 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@4978e15c{/environment,null,AVAILABLE,@Spark}
2023-04-26 11:57:23,109 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@2040776a{/environment/json,null,AVAILABLE,@Spark}
2023-04-26 11:57:23,110 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@6576741d{/executors,null,AVAILABLE,@Spark}
2023-04-26 11:57:23,111 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@12eebdbe{/executors/json,null,AVAILABLE,@Spark}
2023-04-26 11:57:23,111 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@1c92dcbe{/executors/threadDump,null,AVAILABLE,@Spark}
2023-04-26 11:57:23,112 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@3fca1361{/executors/threadDump/json,null,AVAILABLE,@Spark}
2023-04-26 11:57:23,114 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@694a5e19{/static,null,AVAILABLE,@Spark}
2023-04-26 11:57:23,115 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@eae1dc3{/,null,AVAILABLE,@Spark}
2023-04-26 11:57:23,116 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@4d35784{/api,null,AVAILABLE,@Spark}
2023-04-26 11:57:23,116 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@506c3c96{/jobs/job/kill,null,AVAILABLE,@Spark}
2023-04-26 11:57:23,117 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@2ed2635d{/stages/stage/kill,null,AVAILABLE,@Spark}
2023-04-26 11:57:23,118 INFO org.apache.spark.ui.SparkUI [restartedMain] Bound SparkUI to 0.0.0.0, and started at http://192.168.1.125:4040
2023-04-26 11:57:23,147 INFO org.apache.spark.executor.Executor [restartedMain] Starting executor ID driver on host localhost
2023-04-26 11:57:23,150 INFO org.apache.spark.util.Utils [restartedMain] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45233.
2023-04-26 11:57:23,151 INFO org.apache.spark.network.netty.NettyBlockTransferService [restartedMain] Server created on 192.168.1.125:45233
2023-04-26 11:57:23,151 INFO org.apache.spark.storage.BlockManager [restartedMain] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2023-04-26 11:57:23,152 INFO org.apache.spark.storage.BlockManagerMaster [restartedMain] Registering BlockManager BlockManagerId(driver, 192.168.1.125, 45233, None)
2023-04-26 11:57:23,153 INFO org.apache.spark.storage.BlockManagerMasterEndpoint [dispatcher-event-loop-2] Registering block manager 192.168.1.125:45233 with 998.4 MB RAM, BlockManagerId(driver, 192.168.1.125, 45233, None)
2023-04-26 11:57:23,156 INFO org.apache.spark.storage.BlockManagerMaster [restartedMain] Registered BlockManager BlockManagerId(driver, 192.168.1.125, 45233, None)
2023-04-26 11:57:23,157 INFO org.apache.spark.storage.BlockManager [restartedMain] Initialized BlockManager: BlockManagerId(driver, 192.168.1.125, 45233, None)
2023-04-26 11:57:23,161 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@14ab7b49{/metrics/json,null,AVAILABLE,@Spark}
2023-04-26 11:57:24,634 INFO org.apache.kafka.clients.consumer.ConsumerConfig [restartedMain] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-book-group-14
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = book-group
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2023-04-26 11:57:24,639 INFO org.apache.kafka.common.utils.AppInfoParser [restartedMain] Kafka version: 3.1.1
2023-04-26 11:57:24,639 INFO org.apache.kafka.common.utils.AppInfoParser [restartedMain] Kafka commitId: 97671528ba54a138
2023-04-26 11:57:24,640 INFO org.apache.kafka.common.utils.AppInfoParser [restartedMain] Kafka startTimeMs: 1682490444639
2023-04-26 11:57:24,640 INFO org.apache.kafka.clients.consumer.KafkaConsumer [restartedMain] [Consumer clientId=consumer-book-group-14, groupId=book-group] Subscribed to topic(s): my-topic
2023-04-26 11:57:24,646 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-14, groupId=book-group] Resetting the last seen epoch of partition my-topic-0 to 0 since the associated topicId changed from null to QqwJ2XdJQbi2BDXQbzT51g
2023-04-26 11:57:24,647 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-14, groupId=book-group] Cluster ID: 6CnV1ouTQjGsP-r9Rd2WRA
2023-04-26 11:57:24,647 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-14, groupId=book-group] Discovered group coordinator wks-012:9092 (id: 2147483647 rack: null)
2023-04-26 11:57:24,649 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-14, groupId=book-group] (Re-)joining group
2023-04-26 11:57:24,653 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-14, groupId=book-group] Request joining group due to: need to re-join with the given member-id
2023-04-26 11:57:24,654 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-14, groupId=book-group] (Re-)joining group
2023-04-26 11:57:24,661 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-14, groupId=book-group] Successfully joined group with generation Generation{generationId=157, memberId='consumer-book-group-14-d347927b-581b-4c67-a008-425973d7e1ac', protocol='range'}
2023-04-26 11:57:24,662 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-14, groupId=book-group] Finished assignment for group at generation 157: {consumer-book-group-14-d347927b-581b-4c67-a008-425973d7e1ac=Assignment(partitions=[my-topic-0])}
2023-04-26 11:57:24,671 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-14, groupId=book-group] Successfully synced group in generation Generation{generationId=157, memberId='consumer-book-group-14-d347927b-581b-4c67-a008-425973d7e1ac', protocol='range'}
2023-04-26 11:57:24,672 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-14, groupId=book-group] Notifying assignor about the new Assignment(partitions=[my-topic-0])
2023-04-26 11:57:24,672 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-14, groupId=book-group] Adding newly assigned partitions: my-topic-0
2023-04-26 11:57:24,674 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] Started SpringBootSecurityJwtMongodbApplication in 3.472 seconds (JVM running for 916.907)
2023-04-26 11:57:24,679 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-14, groupId=book-group] Setting offset for partition my-topic-0 to the committed offset FetchPosition{offset=415, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[wks-012:9092 (id: 0 rack: null)], epoch=0}}
2023-04-26 11:57:24,679 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] This is a info message
2023-04-26 11:57:24,679 WARN com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] This is a warn message
2023-04-26 12:00:40,826 INFO org.apache.catalina.core.StandardService [Thread-219] Stopping service [Tomcat]
2023-04-26 12:00:40,829 WARN org.apache.catalina.loader.WebappClassLoaderBase [Thread-219] The web application [ROOT] appears to have started a thread named [cluster-ClusterId{value='6448c44aadee62176f290767', description='null'}-localhost:27017] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/jdk.internal.misc.Unsafe.park(Native Method)
 java.base@17.0.6/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:252)
 java.base@17.0.6/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1672)
 app//com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.waitForSignalOrTimeout(DefaultServerMonitor.java:300)
 app//com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.waitForNext(DefaultServerMonitor.java:281)
 app//com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:179)
 java.base@17.0.6/java.lang.Thread.run(Thread.java:833)
2023-04-26 12:00:40,830 WARN org.apache.catalina.loader.WebappClassLoaderBase [Thread-219] The web application [ROOT] appears to have started a thread named [cluster-rtt-ClusterId{value='6448c44aadee62176f290767', description='null'}-localhost:27017] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/java.lang.Thread.sleep(Native Method)
 app//com.mongodb.internal.connection.DefaultServerMonitor.waitForNext(DefaultServerMonitor.java:443)
 app//com.mongodb.internal.connection.DefaultServerMonitor.access$1500(DefaultServerMonitor.java:64)
 app//com.mongodb.internal.connection.DefaultServerMonitor$RoundTripTimeRunnable.run(DefaultServerMonitor.java:415)
 java.base@17.0.6/java.lang.Thread.run(Thread.java:833)
2023-04-26 12:00:40,831 WARN org.apache.catalina.loader.WebappClassLoaderBase [Thread-219] The web application [ROOT] appears to have started a thread named [MaintenanceTimer-15-thread-1] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/jdk.internal.misc.Unsafe.park(Native Method)
 java.base@17.0.6/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:252)
 java.base@17.0.6/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1672)
 java.base@17.0.6/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182)
 java.base@17.0.6/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1062)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1122)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
 java.base@17.0.6/java.lang.Thread.run(Thread.java:833)
2023-04-26 12:00:40,834 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-14, groupId=book-group] Revoke previously assigned partitions my-topic-0
2023-04-26 12:00:40,835 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-14, groupId=book-group] Member consumer-book-group-14-d347927b-581b-4c67-a008-425973d7e1ac sending LeaveGroup request to coordinator wks-012:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2023-04-26 12:00:40,835 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-14, groupId=book-group] Resetting generation due to: consumer pro-actively leaving the group
2023-04-26 12:00:40,835 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-14, groupId=book-group] Request joining group due to: consumer pro-actively leaving the group
2023-04-26 12:00:40,835 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-14, groupId=book-group] Unsubscribed all topics or patterns and assigned partitions
2023-04-26 12:00:40,836 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-14, groupId=book-group] Resetting generation due to: consumer pro-actively leaving the group
2023-04-26 12:00:40,836 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-14, groupId=book-group] Request joining group due to: consumer pro-actively leaving the group
2023-04-26 12:00:40,839 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Metrics scheduler closed
2023-04-26 12:00:40,839 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-04-26 12:00:40,839 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Metrics reporters closed
2023-04-26 12:00:40,841 INFO org.apache.kafka.common.utils.AppInfoParser [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] App info kafka.consumer for consumer-book-group-14 unregistered
2023-04-26 12:00:40,847 INFO org.spark_project.jetty.server.AbstractConnector [Thread-219] Stopped Spark@7632e0b1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2023-04-26 12:00:40,889 INFO org.apache.spark.ui.SparkUI [Thread-219] Stopped Spark web UI at http://192.168.1.125:4040
2023-04-26 12:00:40,891 INFO org.apache.spark.MapOutputTrackerMasterEndpoint [dispatcher-event-loop-3] MapOutputTrackerMasterEndpoint stopped!
2023-04-26 12:00:40,900 INFO org.apache.spark.storage.memory.MemoryStore [Thread-219] MemoryStore cleared
2023-04-26 12:00:40,900 INFO org.apache.spark.storage.BlockManager [Thread-219] BlockManager stopped
2023-04-26 12:00:40,901 INFO org.apache.spark.storage.BlockManagerMaster [Thread-219] BlockManagerMaster stopped
2023-04-26 12:00:40,901 INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [dispatcher-event-loop-0] OutputCommitCoordinator stopped!
2023-04-26 12:00:40,907 INFO org.apache.spark.SparkContext [Thread-219] Successfully stopped SparkContext
2023-04-26 12:00:40,908 INFO org.apache.spark.SparkContext [Thread-219] SparkContext already stopped.
2023-04-26 12:00:40,909 INFO com.zaxxer.hikari.HikariDataSource [Thread-219] HikariPool-14 - Shutdown initiated...
2023-04-26 12:00:40,914 INFO com.zaxxer.hikari.HikariDataSource [Thread-219] HikariPool-14 - Shutdown completed.
2023-04-26 12:00:41,196 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] Starting SpringBootSecurityJwtMongodbApplication using Java 17.0.6 on wks-012 with PID 102147 (/home/inferyx/git/SpringApplicationWithSecurity/target/classes started by inferyx in /home/inferyx/git/SpringApplicationWithSecurity)
2023-04-26 12:00:41,196 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] No active profile set, falling back to 1 default profile: "default"
2023-04-26 12:00:41,988 INFO org.apache.catalina.core.StandardService [restartedMain] Starting service [Tomcat]
2023-04-26 12:00:41,988 INFO org.apache.catalina.core.StandardEngine [restartedMain] Starting Servlet engine: [Apache Tomcat/9.0.65]
2023-04-26 12:00:42,025 INFO org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] [restartedMain] Initializing Spring embedded WebApplicationContext
2023-04-26 12:00:42,082 INFO org.mongodb.driver.client [restartedMain] MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync|spring-boot", "version": "4.6.1"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "5.15.0-69-generic"}, "platform": "Java/Eclipse Adoptium/17.0.6+10"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.Jep395RecordCodecProvider@269fb271]}, clusterSettings={hosts=[localhost:27017], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='30000 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, sendBufferSize=0}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, sendBufferSize=0}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=JAVA_LEGACY, serverApi=null, autoEncryptionSettings=null, contextProvider=null}
2023-04-26 12:00:42,100 INFO org.mongodb.driver.connection [cluster-ClusterId{value='6448c512adee62176f290768', description='null'}-localhost:27017] Opened connection [connectionId{localValue:30, serverValue:71}] to localhost:27017
2023-04-26 12:00:42,100 INFO org.mongodb.driver.cluster [cluster-ClusterId{value='6448c512adee62176f290768', description='null'}-localhost:27017] Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=6, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=17083662}
2023-04-26 12:00:42,100 INFO org.mongodb.driver.connection [cluster-rtt-ClusterId{value='6448c512adee62176f290768', description='null'}-localhost:27017] Opened connection [connectionId{localValue:31, serverValue:72}] to localhost:27017
2023-04-26 12:00:42,359 INFO org.hibernate.jpa.internal.util.LogHelper [restartedMain] HHH000204: Processing PersistenceUnitInfo [name: default]
2023-04-26 12:00:42,371 INFO com.zaxxer.hikari.HikariDataSource [restartedMain] HikariPool-15 - Starting...
2023-04-26 12:00:42,376 INFO com.zaxxer.hikari.HikariDataSource [restartedMain] HikariPool-15 - Start completed.
2023-04-26 12:00:42,376 INFO org.hibernate.dialect.Dialect [restartedMain] HHH000400: Using dialect: org.hibernate.dialect.MySQL5Dialect
2023-04-26 12:00:42,464 INFO org.hibernate.engine.transaction.jta.platform.internal.JtaPlatformInitiator [restartedMain] HHH000490: Using JtaPlatform implementation: [org.hibernate.engine.transaction.jta.platform.internal.NoJtaPlatform]
2023-04-26 12:00:42,784 INFO org.apache.spark.SparkContext [restartedMain] Running Spark version 2.4.5
2023-04-26 12:00:42,785 INFO org.apache.spark.SparkContext [restartedMain] Submitted application: MyAppName
2023-04-26 12:00:42,786 INFO org.apache.spark.SecurityManager [restartedMain] Changing view acls to: inferyx
2023-04-26 12:00:42,786 INFO org.apache.spark.SecurityManager [restartedMain] Changing modify acls to: inferyx
2023-04-26 12:00:42,786 INFO org.apache.spark.SecurityManager [restartedMain] Changing view acls groups to: 
2023-04-26 12:00:42,787 INFO org.apache.spark.SecurityManager [restartedMain] Changing modify acls groups to: 
2023-04-26 12:00:42,787 INFO org.apache.spark.SecurityManager [restartedMain] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(inferyx); groups with view permissions: Set(); users  with modify permissions: Set(inferyx); groups with modify permissions: Set()
2023-04-26 12:00:42,847 INFO org.apache.spark.util.Utils [restartedMain] Successfully started service 'sparkDriver' on port 45719.
2023-04-26 12:00:42,850 INFO org.apache.spark.SparkEnv [restartedMain] Registering MapOutputTracker
2023-04-26 12:00:42,851 INFO org.apache.spark.SparkEnv [restartedMain] Registering BlockManagerMaster
2023-04-26 12:00:42,851 INFO org.apache.spark.storage.BlockManagerMasterEndpoint [restartedMain] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2023-04-26 12:00:42,851 INFO org.apache.spark.storage.BlockManagerMasterEndpoint [restartedMain] BlockManagerMasterEndpoint up
2023-04-26 12:00:42,852 INFO org.apache.spark.storage.DiskBlockManager [restartedMain] Created local directory at /tmp/blockmgr-cc2e19ca-9259-40b3-8dca-56205f07dd9a
2023-04-26 12:00:42,853 INFO org.apache.spark.storage.memory.MemoryStore [restartedMain] MemoryStore started with capacity 998.4 MB
2023-04-26 12:00:42,856 INFO org.apache.spark.SparkEnv [restartedMain] Registering OutputCommitCoordinator
2023-04-26 12:00:42,862 INFO org.spark_project.jetty.server.Server [restartedMain] jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2023-04-26 12:00:42,873 INFO org.spark_project.jetty.server.Server [restartedMain] Started @1115106ms
2023-04-26 12:00:42,875 INFO org.spark_project.jetty.server.AbstractConnector [restartedMain] Started ServerConnector@1451541a{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2023-04-26 12:00:42,875 INFO org.apache.spark.util.Utils [restartedMain] Successfully started service 'SparkUI' on port 4040.
2023-04-26 12:00:42,876 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@6d715010{/jobs,null,AVAILABLE,@Spark}
2023-04-26 12:00:42,876 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@108af5b1{/jobs/json,null,AVAILABLE,@Spark}
2023-04-26 12:00:42,876 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@59979eec{/jobs/job,null,AVAILABLE,@Spark}
2023-04-26 12:00:42,877 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@47f8e040{/jobs/job/json,null,AVAILABLE,@Spark}
2023-04-26 12:00:42,877 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@20eed97f{/stages,null,AVAILABLE,@Spark}
2023-04-26 12:00:42,878 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@670e87ae{/stages/json,null,AVAILABLE,@Spark}
2023-04-26 12:00:42,880 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@28b055d{/stages/stage,null,AVAILABLE,@Spark}
2023-04-26 12:00:42,881 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@2711fac{/stages/stage/json,null,AVAILABLE,@Spark}
2023-04-26 12:00:42,883 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@54021bbc{/stages/pool,null,AVAILABLE,@Spark}
2023-04-26 12:00:42,884 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@47a2d565{/stages/pool/json,null,AVAILABLE,@Spark}
2023-04-26 12:00:42,885 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@1d598121{/storage,null,AVAILABLE,@Spark}
2023-04-26 12:00:42,886 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@175f6159{/storage/json,null,AVAILABLE,@Spark}
2023-04-26 12:00:42,888 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@53c447f5{/storage/rdd,null,AVAILABLE,@Spark}
2023-04-26 12:00:42,889 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@21cb915{/storage/rdd/json,null,AVAILABLE,@Spark}
2023-04-26 12:00:42,891 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@531d70fe{/environment,null,AVAILABLE,@Spark}
2023-04-26 12:00:42,892 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@3fa4966d{/environment/json,null,AVAILABLE,@Spark}
2023-04-26 12:00:42,893 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@4e2e8583{/executors,null,AVAILABLE,@Spark}
2023-04-26 12:00:42,894 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@299d4942{/executors/json,null,AVAILABLE,@Spark}
2023-04-26 12:00:42,895 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@113f560f{/executors/threadDump,null,AVAILABLE,@Spark}
2023-04-26 12:00:42,896 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@a33bdbb{/executors/threadDump/json,null,AVAILABLE,@Spark}
2023-04-26 12:00:42,898 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@300b23e3{/static,null,AVAILABLE,@Spark}
2023-04-26 12:00:42,899 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@731fabf9{/,null,AVAILABLE,@Spark}
2023-04-26 12:00:42,900 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@187b14bb{/api,null,AVAILABLE,@Spark}
2023-04-26 12:00:42,901 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@664f9254{/jobs/job/kill,null,AVAILABLE,@Spark}
2023-04-26 12:00:42,905 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@6801ad07{/stages/stage/kill,null,AVAILABLE,@Spark}
2023-04-26 12:00:42,905 INFO org.apache.spark.ui.SparkUI [restartedMain] Bound SparkUI to 0.0.0.0, and started at http://192.168.1.125:4040
2023-04-26 12:00:42,933 INFO org.apache.spark.executor.Executor [restartedMain] Starting executor ID driver on host localhost
2023-04-26 12:00:42,937 INFO org.apache.spark.util.Utils [restartedMain] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38551.
2023-04-26 12:00:42,938 INFO org.apache.spark.network.netty.NettyBlockTransferService [restartedMain] Server created on 192.168.1.125:38551
2023-04-26 12:00:42,939 INFO org.apache.spark.storage.BlockManager [restartedMain] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2023-04-26 12:00:42,939 INFO org.apache.spark.storage.BlockManagerMaster [restartedMain] Registering BlockManager BlockManagerId(driver, 192.168.1.125, 38551, None)
2023-04-26 12:00:42,939 INFO org.apache.spark.storage.BlockManagerMasterEndpoint [dispatcher-event-loop-2] Registering block manager 192.168.1.125:38551 with 998.4 MB RAM, BlockManagerId(driver, 192.168.1.125, 38551, None)
2023-04-26 12:00:42,940 INFO org.apache.spark.storage.BlockManagerMaster [restartedMain] Registered BlockManager BlockManagerId(driver, 192.168.1.125, 38551, None)
2023-04-26 12:00:42,940 INFO org.apache.spark.storage.BlockManager [restartedMain] Initialized BlockManager: BlockManagerId(driver, 192.168.1.125, 38551, None)
2023-04-26 12:00:42,941 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@1805cef3{/metrics/json,null,AVAILABLE,@Spark}
2023-04-26 12:00:44,314 INFO org.apache.kafka.clients.consumer.ConsumerConfig [restartedMain] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-book-group-15
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = book-group
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2023-04-26 12:00:44,319 INFO org.apache.kafka.common.utils.AppInfoParser [restartedMain] Kafka version: 3.1.1
2023-04-26 12:00:44,319 INFO org.apache.kafka.common.utils.AppInfoParser [restartedMain] Kafka commitId: 97671528ba54a138
2023-04-26 12:00:44,319 INFO org.apache.kafka.common.utils.AppInfoParser [restartedMain] Kafka startTimeMs: 1682490644319
2023-04-26 12:00:44,320 INFO org.apache.kafka.clients.consumer.KafkaConsumer [restartedMain] [Consumer clientId=consumer-book-group-15, groupId=book-group] Subscribed to topic(s): my-topic
2023-04-26 12:00:44,326 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-15, groupId=book-group] Resetting the last seen epoch of partition my-topic-0 to 0 since the associated topicId changed from null to QqwJ2XdJQbi2BDXQbzT51g
2023-04-26 12:00:44,327 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-15, groupId=book-group] Cluster ID: 6CnV1ouTQjGsP-r9Rd2WRA
2023-04-26 12:00:44,328 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-15, groupId=book-group] Discovered group coordinator wks-012:9092 (id: 2147483647 rack: null)
2023-04-26 12:00:44,333 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-15, groupId=book-group] (Re-)joining group
2023-04-26 12:00:44,338 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-15, groupId=book-group] Request joining group due to: need to re-join with the given member-id
2023-04-26 12:00:44,338 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-15, groupId=book-group] (Re-)joining group
2023-04-26 12:00:44,341 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-15, groupId=book-group] Successfully joined group with generation Generation{generationId=159, memberId='consumer-book-group-15-4b29ca69-e657-4bcb-9d69-7679eb69f86f', protocol='range'}
2023-04-26 12:00:44,341 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-15, groupId=book-group] Finished assignment for group at generation 159: {consumer-book-group-15-4b29ca69-e657-4bcb-9d69-7679eb69f86f=Assignment(partitions=[my-topic-0])}
2023-04-26 12:00:44,346 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-15, groupId=book-group] Successfully synced group in generation Generation{generationId=159, memberId='consumer-book-group-15-4b29ca69-e657-4bcb-9d69-7679eb69f86f', protocol='range'}
2023-04-26 12:00:44,347 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-15, groupId=book-group] Notifying assignor about the new Assignment(partitions=[my-topic-0])
2023-04-26 12:00:44,347 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-15, groupId=book-group] Adding newly assigned partitions: my-topic-0
2023-04-26 12:00:44,353 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-15, groupId=book-group] Setting offset for partition my-topic-0 to the committed offset FetchPosition{offset=415, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[wks-012:9092 (id: 0 rack: null)], epoch=0}}
2023-04-26 12:00:44,361 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] Started SpringBootSecurityJwtMongodbApplication in 3.219 seconds (JVM running for 1116.594)
2023-04-26 12:00:44,365 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] This is a info message
2023-04-26 12:00:44,366 WARN com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] This is a warn message
2023-04-26 12:00:48,708 INFO org.apache.catalina.core.StandardService [Thread-235] Stopping service [Tomcat]
2023-04-26 12:00:48,711 WARN org.apache.catalina.loader.WebappClassLoaderBase [Thread-235] The web application [ROOT] appears to have started a thread named [cluster-ClusterId{value='6448c512adee62176f290768', description='null'}-localhost:27017] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/jdk.internal.misc.Unsafe.park(Native Method)
 java.base@17.0.6/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:252)
 java.base@17.0.6/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1672)
 app//com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.waitForSignalOrTimeout(DefaultServerMonitor.java:300)
 app//com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.waitForNext(DefaultServerMonitor.java:281)
 app//com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:179)
 java.base@17.0.6/java.lang.Thread.run(Thread.java:833)
2023-04-26 12:00:48,712 WARN org.apache.catalina.loader.WebappClassLoaderBase [Thread-235] The web application [ROOT] appears to have started a thread named [cluster-rtt-ClusterId{value='6448c512adee62176f290768', description='null'}-localhost:27017] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/java.lang.Thread.sleep(Native Method)
 app//com.mongodb.internal.connection.DefaultServerMonitor.waitForNext(DefaultServerMonitor.java:443)
 app//com.mongodb.internal.connection.DefaultServerMonitor.access$1500(DefaultServerMonitor.java:64)
 app//com.mongodb.internal.connection.DefaultServerMonitor$RoundTripTimeRunnable.run(DefaultServerMonitor.java:415)
 java.base@17.0.6/java.lang.Thread.run(Thread.java:833)
2023-04-26 12:00:48,713 WARN org.apache.catalina.loader.WebappClassLoaderBase [Thread-235] The web application [ROOT] appears to have started a thread named [MaintenanceTimer-16-thread-1] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/jdk.internal.misc.Unsafe.park(Native Method)
 java.base@17.0.6/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:252)
 java.base@17.0.6/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1672)
 java.base@17.0.6/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182)
 java.base@17.0.6/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1062)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1122)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
 java.base@17.0.6/java.lang.Thread.run(Thread.java:833)
2023-04-26 12:00:48,717 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-15, groupId=book-group] Revoke previously assigned partitions my-topic-0
2023-04-26 12:00:48,717 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-15, groupId=book-group] Member consumer-book-group-15-4b29ca69-e657-4bcb-9d69-7679eb69f86f sending LeaveGroup request to coordinator wks-012:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2023-04-26 12:00:48,717 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-15, groupId=book-group] Resetting generation due to: consumer pro-actively leaving the group
2023-04-26 12:00:48,718 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-15, groupId=book-group] Request joining group due to: consumer pro-actively leaving the group
2023-04-26 12:00:48,718 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-15, groupId=book-group] Unsubscribed all topics or patterns and assigned partitions
2023-04-26 12:00:48,719 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-15, groupId=book-group] Resetting generation due to: consumer pro-actively leaving the group
2023-04-26 12:00:48,719 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-15, groupId=book-group] Request joining group due to: consumer pro-actively leaving the group
2023-04-26 12:00:48,729 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Metrics scheduler closed
2023-04-26 12:00:48,730 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-04-26 12:00:48,731 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Metrics reporters closed
2023-04-26 12:00:48,735 INFO org.apache.kafka.common.utils.AppInfoParser [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] App info kafka.consumer for consumer-book-group-15 unregistered
2023-04-26 12:00:48,741 INFO org.spark_project.jetty.server.AbstractConnector [Thread-235] Stopped Spark@1451541a{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2023-04-26 12:00:48,748 INFO org.apache.spark.ui.SparkUI [Thread-235] Stopped Spark web UI at http://192.168.1.125:4040
2023-04-26 12:00:48,751 INFO org.apache.spark.MapOutputTrackerMasterEndpoint [dispatcher-event-loop-2] MapOutputTrackerMasterEndpoint stopped!
2023-04-26 12:00:48,762 INFO org.apache.spark.storage.memory.MemoryStore [Thread-235] MemoryStore cleared
2023-04-26 12:00:48,762 INFO org.apache.spark.storage.BlockManager [Thread-235] BlockManager stopped
2023-04-26 12:00:48,763 INFO org.apache.spark.storage.BlockManagerMaster [Thread-235] BlockManagerMaster stopped
2023-04-26 12:00:48,763 INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [dispatcher-event-loop-3] OutputCommitCoordinator stopped!
2023-04-26 12:00:48,770 INFO org.apache.spark.SparkContext [Thread-235] Successfully stopped SparkContext
2023-04-26 12:00:48,770 INFO org.apache.spark.SparkContext [Thread-235] SparkContext already stopped.
2023-04-26 12:00:48,771 INFO com.zaxxer.hikari.HikariDataSource [Thread-235] HikariPool-15 - Shutdown initiated...
2023-04-26 12:00:48,773 INFO com.zaxxer.hikari.HikariDataSource [Thread-235] HikariPool-15 - Shutdown completed.
2023-04-26 12:00:49,128 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] Starting SpringBootSecurityJwtMongodbApplication using Java 17.0.6 on wks-012 with PID 102147 (/home/inferyx/git/SpringApplicationWithSecurity/target/classes started by inferyx in /home/inferyx/git/SpringApplicationWithSecurity)
2023-04-26 12:00:49,129 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] No active profile set, falling back to 1 default profile: "default"
2023-04-26 12:00:49,913 INFO org.apache.catalina.core.StandardService [restartedMain] Starting service [Tomcat]
2023-04-26 12:00:49,914 INFO org.apache.catalina.core.StandardEngine [restartedMain] Starting Servlet engine: [Apache Tomcat/9.0.65]
2023-04-26 12:00:49,945 INFO org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] [restartedMain] Initializing Spring embedded WebApplicationContext
2023-04-26 12:00:49,996 INFO org.mongodb.driver.client [restartedMain] MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync|spring-boot", "version": "4.6.1"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "5.15.0-69-generic"}, "platform": "Java/Eclipse Adoptium/17.0.6+10"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.Jep395RecordCodecProvider@269fb271]}, clusterSettings={hosts=[localhost:27017], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='30000 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, sendBufferSize=0}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, sendBufferSize=0}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=JAVA_LEGACY, serverApi=null, autoEncryptionSettings=null, contextProvider=null}
2023-04-26 12:00:49,998 INFO org.mongodb.driver.connection [cluster-rtt-ClusterId{value='6448c519adee62176f290769', description='null'}-localhost:27017] Opened connection [connectionId{localValue:33, serverValue:74}] to localhost:27017
2023-04-26 12:00:49,999 INFO org.mongodb.driver.connection [cluster-ClusterId{value='6448c519adee62176f290769', description='null'}-localhost:27017] Opened connection [connectionId{localValue:32, serverValue:73}] to localhost:27017
2023-04-26 12:00:49,999 INFO org.mongodb.driver.cluster [cluster-ClusterId{value='6448c519adee62176f290769', description='null'}-localhost:27017] Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=6, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=835152}
2023-04-26 12:00:50,221 INFO org.hibernate.jpa.internal.util.LogHelper [restartedMain] HHH000204: Processing PersistenceUnitInfo [name: default]
2023-04-26 12:00:50,235 INFO com.zaxxer.hikari.HikariDataSource [restartedMain] HikariPool-16 - Starting...
2023-04-26 12:00:50,239 INFO com.zaxxer.hikari.HikariDataSource [restartedMain] HikariPool-16 - Start completed.
2023-04-26 12:00:50,240 INFO org.hibernate.dialect.Dialect [restartedMain] HHH000400: Using dialect: org.hibernate.dialect.MySQL5Dialect
2023-04-26 12:00:50,325 INFO org.hibernate.engine.transaction.jta.platform.internal.JtaPlatformInitiator [restartedMain] HHH000490: Using JtaPlatform implementation: [org.hibernate.engine.transaction.jta.platform.internal.NoJtaPlatform]
2023-04-26 12:00:50,590 INFO org.apache.spark.SparkContext [restartedMain] Running Spark version 2.4.5
2023-04-26 12:00:50,590 INFO org.apache.spark.SparkContext [restartedMain] Submitted application: MyAppName
2023-04-26 12:00:50,592 INFO org.apache.spark.SecurityManager [restartedMain] Changing view acls to: inferyx
2023-04-26 12:00:50,592 INFO org.apache.spark.SecurityManager [restartedMain] Changing modify acls to: inferyx
2023-04-26 12:00:50,592 INFO org.apache.spark.SecurityManager [restartedMain] Changing view acls groups to: 
2023-04-26 12:00:50,593 INFO org.apache.spark.SecurityManager [restartedMain] Changing modify acls groups to: 
2023-04-26 12:00:50,593 INFO org.apache.spark.SecurityManager [restartedMain] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(inferyx); groups with view permissions: Set(); users  with modify permissions: Set(inferyx); groups with modify permissions: Set()
2023-04-26 12:00:50,639 INFO org.apache.spark.util.Utils [restartedMain] Successfully started service 'sparkDriver' on port 43295.
2023-04-26 12:00:50,642 INFO org.apache.spark.SparkEnv [restartedMain] Registering MapOutputTracker
2023-04-26 12:00:50,643 INFO org.apache.spark.SparkEnv [restartedMain] Registering BlockManagerMaster
2023-04-26 12:00:50,644 INFO org.apache.spark.storage.BlockManagerMasterEndpoint [restartedMain] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2023-04-26 12:00:50,644 INFO org.apache.spark.storage.BlockManagerMasterEndpoint [restartedMain] BlockManagerMasterEndpoint up
2023-04-26 12:00:50,645 INFO org.apache.spark.storage.DiskBlockManager [restartedMain] Created local directory at /tmp/blockmgr-e879e262-8026-4097-9de3-7e585ce4ee96
2023-04-26 12:00:50,645 INFO org.apache.spark.storage.memory.MemoryStore [restartedMain] MemoryStore started with capacity 998.4 MB
2023-04-26 12:00:50,647 INFO org.apache.spark.SparkEnv [restartedMain] Registering OutputCommitCoordinator
2023-04-26 12:00:50,651 INFO org.spark_project.jetty.server.Server [restartedMain] jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2023-04-26 12:00:50,652 INFO org.spark_project.jetty.server.Server [restartedMain] Started @1122885ms
2023-04-26 12:00:50,653 INFO org.spark_project.jetty.server.AbstractConnector [restartedMain] Started ServerConnector@bcd9e2{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2023-04-26 12:00:50,654 INFO org.apache.spark.util.Utils [restartedMain] Successfully started service 'SparkUI' on port 4040.
2023-04-26 12:00:50,654 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@49c8fea{/jobs,null,AVAILABLE,@Spark}
2023-04-26 12:00:50,654 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@58ea8b28{/jobs/json,null,AVAILABLE,@Spark}
2023-04-26 12:00:50,655 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@3d5573d{/jobs/job,null,AVAILABLE,@Spark}
2023-04-26 12:00:50,655 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@65c0ed6c{/jobs/job/json,null,AVAILABLE,@Spark}
2023-04-26 12:00:50,655 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@5c3dde1{/stages,null,AVAILABLE,@Spark}
2023-04-26 12:00:50,656 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@97cb5ba{/stages/json,null,AVAILABLE,@Spark}
2023-04-26 12:00:50,656 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@20b97717{/stages/stage,null,AVAILABLE,@Spark}
2023-04-26 12:00:50,656 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@427055e1{/stages/stage/json,null,AVAILABLE,@Spark}
2023-04-26 12:00:50,657 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@59c7a49b{/stages/pool,null,AVAILABLE,@Spark}
2023-04-26 12:00:50,658 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@357a0b7c{/stages/pool/json,null,AVAILABLE,@Spark}
2023-04-26 12:00:50,659 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@5e90d30b{/storage,null,AVAILABLE,@Spark}
2023-04-26 12:00:50,659 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@1e8f5a65{/storage/json,null,AVAILABLE,@Spark}
2023-04-26 12:00:50,660 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@7a88d653{/storage/rdd,null,AVAILABLE,@Spark}
2023-04-26 12:00:50,661 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@1fcc1395{/storage/rdd/json,null,AVAILABLE,@Spark}
2023-04-26 12:00:50,661 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@471c93ec{/environment,null,AVAILABLE,@Spark}
2023-04-26 12:00:50,664 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@23a4a518{/environment/json,null,AVAILABLE,@Spark}
2023-04-26 12:00:50,665 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@1fa2d571{/executors,null,AVAILABLE,@Spark}
2023-04-26 12:00:50,665 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@608d3dab{/executors/json,null,AVAILABLE,@Spark}
2023-04-26 12:00:50,666 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@432d41a5{/executors/threadDump,null,AVAILABLE,@Spark}
2023-04-26 12:00:50,666 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@45a7a0bd{/executors/threadDump/json,null,AVAILABLE,@Spark}
2023-04-26 12:00:50,667 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@4ca7fc25{/static,null,AVAILABLE,@Spark}
2023-04-26 12:00:50,668 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@45cef07f{/,null,AVAILABLE,@Spark}
2023-04-26 12:00:50,668 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@6cc7b10a{/api,null,AVAILABLE,@Spark}
2023-04-26 12:00:50,669 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@6728c615{/jobs/job/kill,null,AVAILABLE,@Spark}
2023-04-26 12:00:50,670 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@55578f87{/stages/stage/kill,null,AVAILABLE,@Spark}
2023-04-26 12:00:50,670 INFO org.apache.spark.ui.SparkUI [restartedMain] Bound SparkUI to 0.0.0.0, and started at http://192.168.1.125:4040
2023-04-26 12:00:50,700 INFO org.apache.spark.executor.Executor [restartedMain] Starting executor ID driver on host localhost
2023-04-26 12:00:50,706 INFO org.apache.spark.util.Utils [restartedMain] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34551.
2023-04-26 12:00:50,707 INFO org.apache.spark.network.netty.NettyBlockTransferService [restartedMain] Server created on 192.168.1.125:34551
2023-04-26 12:00:50,707 INFO org.apache.spark.storage.BlockManager [restartedMain] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2023-04-26 12:00:50,708 INFO org.apache.spark.storage.BlockManagerMaster [restartedMain] Registering BlockManager BlockManagerId(driver, 192.168.1.125, 34551, None)
2023-04-26 12:00:50,708 INFO org.apache.spark.storage.BlockManagerMasterEndpoint [dispatcher-event-loop-2] Registering block manager 192.168.1.125:34551 with 998.4 MB RAM, BlockManagerId(driver, 192.168.1.125, 34551, None)
2023-04-26 12:00:50,709 INFO org.apache.spark.storage.BlockManagerMaster [restartedMain] Registered BlockManager BlockManagerId(driver, 192.168.1.125, 34551, None)
2023-04-26 12:00:50,709 INFO org.apache.spark.storage.BlockManager [restartedMain] Initialized BlockManager: BlockManagerId(driver, 192.168.1.125, 34551, None)
2023-04-26 12:00:50,711 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@2a97e098{/metrics/json,null,AVAILABLE,@Spark}
2023-04-26 12:00:52,039 INFO org.apache.kafka.clients.consumer.ConsumerConfig [restartedMain] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-book-group-16
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = book-group
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2023-04-26 12:00:52,046 INFO org.apache.kafka.common.utils.AppInfoParser [restartedMain] Kafka version: 3.1.1
2023-04-26 12:00:52,046 INFO org.apache.kafka.common.utils.AppInfoParser [restartedMain] Kafka commitId: 97671528ba54a138
2023-04-26 12:00:52,046 INFO org.apache.kafka.common.utils.AppInfoParser [restartedMain] Kafka startTimeMs: 1682490652046
2023-04-26 12:00:52,047 INFO org.apache.kafka.clients.consumer.KafkaConsumer [restartedMain] [Consumer clientId=consumer-book-group-16, groupId=book-group] Subscribed to topic(s): my-topic
2023-04-26 12:00:52,053 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-16, groupId=book-group] Resetting the last seen epoch of partition my-topic-0 to 0 since the associated topicId changed from null to QqwJ2XdJQbi2BDXQbzT51g
2023-04-26 12:00:52,054 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-16, groupId=book-group] Cluster ID: 6CnV1ouTQjGsP-r9Rd2WRA
2023-04-26 12:00:52,054 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-16, groupId=book-group] Discovered group coordinator wks-012:9092 (id: 2147483647 rack: null)
2023-04-26 12:00:52,061 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-16, groupId=book-group] (Re-)joining group
2023-04-26 12:00:52,065 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-16, groupId=book-group] Request joining group due to: need to re-join with the given member-id
2023-04-26 12:00:52,065 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-16, groupId=book-group] (Re-)joining group
2023-04-26 12:00:52,069 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] Started SpringBootSecurityJwtMongodbApplication in 3.011 seconds (JVM running for 1124.302)
2023-04-26 12:00:52,072 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] This is a info message
2023-04-26 12:00:52,072 WARN com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] This is a warn message
2023-04-26 12:00:52,080 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-16, groupId=book-group] Successfully joined group with generation Generation{generationId=161, memberId='consumer-book-group-16-f8966966-71ba-436c-bc63-6f85cc0ded45', protocol='range'}
2023-04-26 12:00:52,083 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-16, groupId=book-group] Finished assignment for group at generation 161: {consumer-book-group-16-f8966966-71ba-436c-bc63-6f85cc0ded45=Assignment(partitions=[my-topic-0])}
2023-04-26 12:00:52,088 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-16, groupId=book-group] Successfully synced group in generation Generation{generationId=161, memberId='consumer-book-group-16-f8966966-71ba-436c-bc63-6f85cc0ded45', protocol='range'}
2023-04-26 12:00:52,088 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-16, groupId=book-group] Notifying assignor about the new Assignment(partitions=[my-topic-0])
2023-04-26 12:00:52,088 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-16, groupId=book-group] Adding newly assigned partitions: my-topic-0
2023-04-26 12:00:52,091 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-16, groupId=book-group] Setting offset for partition my-topic-0 to the committed offset FetchPosition{offset=415, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[wks-012:9092 (id: 0 rack: null)], epoch=0}}
2023-04-26 12:04:38,350 INFO org.apache.catalina.core.StandardService [Thread-252] Stopping service [Tomcat]
2023-04-26 12:04:38,354 WARN org.apache.catalina.loader.WebappClassLoaderBase [Thread-252] The web application [ROOT] appears to have started a thread named [cluster-ClusterId{value='6448c519adee62176f290769', description='null'}-localhost:27017] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/jdk.internal.misc.Unsafe.park(Native Method)
 java.base@17.0.6/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:252)
 java.base@17.0.6/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1672)
 app//com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.waitForSignalOrTimeout(DefaultServerMonitor.java:300)
 app//com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.waitForNext(DefaultServerMonitor.java:281)
 app//com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:179)
 java.base@17.0.6/java.lang.Thread.run(Thread.java:833)
2023-04-26 12:04:38,354 WARN org.apache.catalina.loader.WebappClassLoaderBase [Thread-252] The web application [ROOT] appears to have started a thread named [cluster-rtt-ClusterId{value='6448c519adee62176f290769', description='null'}-localhost:27017] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/java.lang.Thread.sleep(Native Method)
 app//com.mongodb.internal.connection.DefaultServerMonitor.waitForNext(DefaultServerMonitor.java:443)
 app//com.mongodb.internal.connection.DefaultServerMonitor.access$1500(DefaultServerMonitor.java:64)
 app//com.mongodb.internal.connection.DefaultServerMonitor$RoundTripTimeRunnable.run(DefaultServerMonitor.java:415)
 java.base@17.0.6/java.lang.Thread.run(Thread.java:833)
2023-04-26 12:04:38,356 WARN org.apache.catalina.loader.WebappClassLoaderBase [Thread-252] The web application [ROOT] appears to have started a thread named [MaintenanceTimer-17-thread-1] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/jdk.internal.misc.Unsafe.park(Native Method)
 java.base@17.0.6/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:252)
 java.base@17.0.6/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1672)
 java.base@17.0.6/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182)
 java.base@17.0.6/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1062)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1122)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
 java.base@17.0.6/java.lang.Thread.run(Thread.java:833)
2023-04-26 12:04:38,362 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-16, groupId=book-group] Revoke previously assigned partitions my-topic-0
2023-04-26 12:04:38,363 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-16, groupId=book-group] Member consumer-book-group-16-f8966966-71ba-436c-bc63-6f85cc0ded45 sending LeaveGroup request to coordinator wks-012:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2023-04-26 12:04:38,363 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-16, groupId=book-group] Resetting generation due to: consumer pro-actively leaving the group
2023-04-26 12:04:38,363 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-16, groupId=book-group] Request joining group due to: consumer pro-actively leaving the group
2023-04-26 12:04:38,363 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-16, groupId=book-group] Unsubscribed all topics or patterns and assigned partitions
2023-04-26 12:04:38,364 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-16, groupId=book-group] Resetting generation due to: consumer pro-actively leaving the group
2023-04-26 12:04:38,364 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-16, groupId=book-group] Request joining group due to: consumer pro-actively leaving the group
2023-04-26 12:04:38,367 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Metrics scheduler closed
2023-04-26 12:04:38,368 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-04-26 12:04:38,368 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Metrics reporters closed
2023-04-26 12:04:38,371 INFO org.apache.kafka.common.utils.AppInfoParser [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] App info kafka.consumer for consumer-book-group-16 unregistered
2023-04-26 12:04:38,379 INFO org.spark_project.jetty.server.AbstractConnector [Thread-252] Stopped Spark@bcd9e2{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2023-04-26 12:04:38,380 INFO org.apache.spark.ui.SparkUI [Thread-252] Stopped Spark web UI at http://192.168.1.125:4040
2023-04-26 12:04:38,387 INFO org.apache.spark.MapOutputTrackerMasterEndpoint [dispatcher-event-loop-1] MapOutputTrackerMasterEndpoint stopped!
2023-04-26 12:04:38,439 INFO org.apache.spark.storage.memory.MemoryStore [Thread-252] MemoryStore cleared
2023-04-26 12:04:38,440 INFO org.apache.spark.storage.BlockManager [Thread-252] BlockManager stopped
2023-04-26 12:04:38,440 INFO org.apache.spark.storage.BlockManagerMaster [Thread-252] BlockManagerMaster stopped
2023-04-26 12:04:38,440 INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [dispatcher-event-loop-1] OutputCommitCoordinator stopped!
2023-04-26 12:04:38,447 INFO org.apache.spark.SparkContext [Thread-252] Successfully stopped SparkContext
2023-04-26 12:04:38,447 INFO org.apache.spark.SparkContext [Thread-252] SparkContext already stopped.
2023-04-26 12:04:38,449 INFO com.zaxxer.hikari.HikariDataSource [Thread-252] HikariPool-16 - Shutdown initiated...
2023-04-26 12:04:38,451 INFO com.zaxxer.hikari.HikariDataSource [Thread-252] HikariPool-16 - Shutdown completed.
2023-04-26 12:04:38,684 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] Starting SpringBootSecurityJwtMongodbApplication using Java 17.0.6 on wks-012 with PID 102147 (/home/inferyx/git/SpringApplicationWithSecurity/target/classes started by inferyx in /home/inferyx/git/SpringApplicationWithSecurity)
2023-04-26 12:04:38,684 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] No active profile set, falling back to 1 default profile: "default"
2023-04-26 12:04:39,369 INFO org.apache.catalina.core.StandardService [restartedMain] Starting service [Tomcat]
2023-04-26 12:04:39,369 INFO org.apache.catalina.core.StandardEngine [restartedMain] Starting Servlet engine: [Apache Tomcat/9.0.65]
2023-04-26 12:04:39,404 INFO org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] [restartedMain] Initializing Spring embedded WebApplicationContext
2023-04-26 12:04:39,449 INFO org.mongodb.driver.client [restartedMain] MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync|spring-boot", "version": "4.6.1"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "5.15.0-69-generic"}, "platform": "Java/Eclipse Adoptium/17.0.6+10"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.Jep395RecordCodecProvider@269fb271]}, clusterSettings={hosts=[localhost:27017], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='30000 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, sendBufferSize=0}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, sendBufferSize=0}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=JAVA_LEGACY, serverApi=null, autoEncryptionSettings=null, contextProvider=null}
2023-04-26 12:04:39,451 INFO org.mongodb.driver.connection [cluster-rtt-ClusterId{value='6448c5ffadee62176f29076a', description='null'}-localhost:27017] Opened connection [connectionId{localValue:35, serverValue:76}] to localhost:27017
2023-04-26 12:04:39,451 INFO org.mongodb.driver.connection [cluster-ClusterId{value='6448c5ffadee62176f29076a', description='null'}-localhost:27017] Opened connection [connectionId{localValue:34, serverValue:75}] to localhost:27017
2023-04-26 12:04:39,451 INFO org.mongodb.driver.cluster [cluster-ClusterId{value='6448c5ffadee62176f29076a', description='null'}-localhost:27017] Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=6, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=991048}
2023-04-26 12:04:39,724 INFO org.hibernate.jpa.internal.util.LogHelper [restartedMain] HHH000204: Processing PersistenceUnitInfo [name: default]
2023-04-26 12:04:39,736 INFO com.zaxxer.hikari.HikariDataSource [restartedMain] HikariPool-17 - Starting...
2023-04-26 12:04:39,741 INFO com.zaxxer.hikari.HikariDataSource [restartedMain] HikariPool-17 - Start completed.
2023-04-26 12:04:39,741 INFO org.hibernate.dialect.Dialect [restartedMain] HHH000400: Using dialect: org.hibernate.dialect.MySQL5Dialect
2023-04-26 12:04:39,812 INFO org.hibernate.engine.transaction.jta.platform.internal.JtaPlatformInitiator [restartedMain] HHH000490: Using JtaPlatform implementation: [org.hibernate.engine.transaction.jta.platform.internal.NoJtaPlatform]
2023-04-26 12:04:40,125 INFO org.apache.spark.SparkContext [restartedMain] Running Spark version 2.4.5
2023-04-26 12:04:40,126 INFO org.apache.spark.SparkContext [restartedMain] Submitted application: MyAppName
2023-04-26 12:04:40,127 INFO org.apache.spark.SecurityManager [restartedMain] Changing view acls to: inferyx
2023-04-26 12:04:40,127 INFO org.apache.spark.SecurityManager [restartedMain] Changing modify acls to: inferyx
2023-04-26 12:04:40,127 INFO org.apache.spark.SecurityManager [restartedMain] Changing view acls groups to: 
2023-04-26 12:04:40,128 INFO org.apache.spark.SecurityManager [restartedMain] Changing modify acls groups to: 
2023-04-26 12:04:40,128 INFO org.apache.spark.SecurityManager [restartedMain] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(inferyx); groups with view permissions: Set(); users  with modify permissions: Set(inferyx); groups with modify permissions: Set()
2023-04-26 12:04:40,186 INFO org.apache.spark.util.Utils [restartedMain] Successfully started service 'sparkDriver' on port 36809.
2023-04-26 12:04:40,190 INFO org.apache.spark.SparkEnv [restartedMain] Registering MapOutputTracker
2023-04-26 12:04:40,192 INFO org.apache.spark.SparkEnv [restartedMain] Registering BlockManagerMaster
2023-04-26 12:04:40,192 INFO org.apache.spark.storage.BlockManagerMasterEndpoint [restartedMain] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2023-04-26 12:04:40,192 INFO org.apache.spark.storage.BlockManagerMasterEndpoint [restartedMain] BlockManagerMasterEndpoint up
2023-04-26 12:04:40,193 INFO org.apache.spark.storage.DiskBlockManager [restartedMain] Created local directory at /tmp/blockmgr-7ac22925-4678-4b6a-af2f-54ab64b088ca
2023-04-26 12:04:40,194 INFO org.apache.spark.storage.memory.MemoryStore [restartedMain] MemoryStore started with capacity 998.4 MB
2023-04-26 12:04:40,196 INFO org.apache.spark.SparkEnv [restartedMain] Registering OutputCommitCoordinator
2023-04-26 12:04:40,201 INFO org.spark_project.jetty.server.Server [restartedMain] jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2023-04-26 12:04:40,203 INFO org.spark_project.jetty.server.Server [restartedMain] Started @1352435ms
2023-04-26 12:04:40,204 INFO org.spark_project.jetty.server.AbstractConnector [restartedMain] Started ServerConnector@577ab19b{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2023-04-26 12:04:40,204 INFO org.apache.spark.util.Utils [restartedMain] Successfully started service 'SparkUI' on port 4040.
2023-04-26 12:04:40,204 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@71e393ae{/jobs,null,AVAILABLE,@Spark}
2023-04-26 12:04:40,205 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@4d83e1df{/jobs/json,null,AVAILABLE,@Spark}
2023-04-26 12:04:40,206 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@223bcfe3{/jobs/job,null,AVAILABLE,@Spark}
2023-04-26 12:04:40,206 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@20cf5456{/jobs/job/json,null,AVAILABLE,@Spark}
2023-04-26 12:04:40,207 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@4d5ccd02{/stages,null,AVAILABLE,@Spark}
2023-04-26 12:04:40,208 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@1f83406f{/stages/json,null,AVAILABLE,@Spark}
2023-04-26 12:04:40,208 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@633bc0ab{/stages/stage,null,AVAILABLE,@Spark}
2023-04-26 12:04:40,209 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@390c457e{/stages/stage/json,null,AVAILABLE,@Spark}
2023-04-26 12:04:40,209 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@1213ab80{/stages/pool,null,AVAILABLE,@Spark}
2023-04-26 12:04:40,209 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@17161195{/stages/pool/json,null,AVAILABLE,@Spark}
2023-04-26 12:04:40,210 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@2a95a544{/storage,null,AVAILABLE,@Spark}
2023-04-26 12:04:40,210 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@71e932dd{/storage/json,null,AVAILABLE,@Spark}
2023-04-26 12:04:40,210 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@7e29d8cb{/storage/rdd,null,AVAILABLE,@Spark}
2023-04-26 12:04:40,211 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@5994e422{/storage/rdd/json,null,AVAILABLE,@Spark}
2023-04-26 12:04:40,211 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@5eacc5d1{/environment,null,AVAILABLE,@Spark}
2023-04-26 12:04:40,212 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@66eb6b03{/environment/json,null,AVAILABLE,@Spark}
2023-04-26 12:04:40,212 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@56b40895{/executors,null,AVAILABLE,@Spark}
2023-04-26 12:04:40,213 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@287f5395{/executors/json,null,AVAILABLE,@Spark}
2023-04-26 12:04:40,213 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@10c58906{/executors/threadDump,null,AVAILABLE,@Spark}
2023-04-26 12:04:40,214 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@2d4b5b6e{/executors/threadDump/json,null,AVAILABLE,@Spark}
2023-04-26 12:04:40,214 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@5bcd1ef2{/static,null,AVAILABLE,@Spark}
2023-04-26 12:04:40,215 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@97c1de5{/,null,AVAILABLE,@Spark}
2023-04-26 12:04:40,215 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@354d3875{/api,null,AVAILABLE,@Spark}
2023-04-26 12:04:40,216 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@71a329a0{/jobs/job/kill,null,AVAILABLE,@Spark}
2023-04-26 12:04:40,217 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@18598144{/stages/stage/kill,null,AVAILABLE,@Spark}
2023-04-26 12:04:40,217 INFO org.apache.spark.ui.SparkUI [restartedMain] Bound SparkUI to 0.0.0.0, and started at http://192.168.1.125:4040
2023-04-26 12:04:40,268 INFO org.apache.spark.executor.Executor [restartedMain] Starting executor ID driver on host localhost
2023-04-26 12:04:40,277 INFO org.apache.spark.util.Utils [restartedMain] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37843.
2023-04-26 12:04:40,278 INFO org.apache.spark.network.netty.NettyBlockTransferService [restartedMain] Server created on 192.168.1.125:37843
2023-04-26 12:04:40,278 INFO org.apache.spark.storage.BlockManager [restartedMain] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2023-04-26 12:04:40,278 INFO org.apache.spark.storage.BlockManagerMaster [restartedMain] Registering BlockManager BlockManagerId(driver, 192.168.1.125, 37843, None)
2023-04-26 12:04:40,279 INFO org.apache.spark.storage.BlockManagerMasterEndpoint [dispatcher-event-loop-2] Registering block manager 192.168.1.125:37843 with 998.4 MB RAM, BlockManagerId(driver, 192.168.1.125, 37843, None)
2023-04-26 12:04:40,280 INFO org.apache.spark.storage.BlockManagerMaster [restartedMain] Registered BlockManager BlockManagerId(driver, 192.168.1.125, 37843, None)
2023-04-26 12:04:40,280 INFO org.apache.spark.storage.BlockManager [restartedMain] Initialized BlockManager: BlockManagerId(driver, 192.168.1.125, 37843, None)
2023-04-26 12:04:40,282 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@774aa47f{/metrics/json,null,AVAILABLE,@Spark}
2023-04-26 12:04:41,921 INFO org.apache.kafka.clients.consumer.ConsumerConfig [restartedMain] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-book-group-17
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = book-group
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2023-04-26 12:04:41,925 INFO org.apache.kafka.common.utils.AppInfoParser [restartedMain] Kafka version: 3.1.1
2023-04-26 12:04:41,926 INFO org.apache.kafka.common.utils.AppInfoParser [restartedMain] Kafka commitId: 97671528ba54a138
2023-04-26 12:04:41,926 INFO org.apache.kafka.common.utils.AppInfoParser [restartedMain] Kafka startTimeMs: 1682490881925
2023-04-26 12:04:41,926 INFO org.apache.kafka.clients.consumer.KafkaConsumer [restartedMain] [Consumer clientId=consumer-book-group-17, groupId=book-group] Subscribed to topic(s): my-topic
2023-04-26 12:04:41,932 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-17, groupId=book-group] Resetting the last seen epoch of partition my-topic-0 to 0 since the associated topicId changed from null to QqwJ2XdJQbi2BDXQbzT51g
2023-04-26 12:04:41,932 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-17, groupId=book-group] Cluster ID: 6CnV1ouTQjGsP-r9Rd2WRA
2023-04-26 12:04:41,933 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-17, groupId=book-group] Discovered group coordinator wks-012:9092 (id: 2147483647 rack: null)
2023-04-26 12:04:41,936 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-17, groupId=book-group] (Re-)joining group
2023-04-26 12:04:41,942 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-17, groupId=book-group] Request joining group due to: need to re-join with the given member-id
2023-04-26 12:04:41,943 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-17, groupId=book-group] (Re-)joining group
2023-04-26 12:04:41,946 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-17, groupId=book-group] Successfully joined group with generation Generation{generationId=163, memberId='consumer-book-group-17-cfa6489e-3e8f-43b1-98fc-b622d1842d6c', protocol='range'}
2023-04-26 12:04:41,946 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-17, groupId=book-group] Finished assignment for group at generation 163: {consumer-book-group-17-cfa6489e-3e8f-43b1-98fc-b622d1842d6c=Assignment(partitions=[my-topic-0])}
2023-04-26 12:04:41,949 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-17, groupId=book-group] Successfully synced group in generation Generation{generationId=163, memberId='consumer-book-group-17-cfa6489e-3e8f-43b1-98fc-b622d1842d6c', protocol='range'}
2023-04-26 12:04:41,950 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-17, groupId=book-group] Notifying assignor about the new Assignment(partitions=[my-topic-0])
2023-04-26 12:04:41,950 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-17, groupId=book-group] Adding newly assigned partitions: my-topic-0
2023-04-26 12:04:41,956 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-17, groupId=book-group] Setting offset for partition my-topic-0 to the committed offset FetchPosition{offset=415, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[wks-012:9092 (id: 0 rack: null)], epoch=0}}
2023-04-26 12:04:41,964 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] Started SpringBootSecurityJwtMongodbApplication in 3.333 seconds (JVM running for 1354.197)
2023-04-26 12:04:41,968 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] This is a info message
2023-04-26 12:04:41,968 WARN com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] This is a warn message
2023-04-26 12:04:46,319 INFO org.apache.catalina.core.StandardService [Thread-269] Stopping service [Tomcat]
2023-04-26 12:04:46,327 WARN org.apache.catalina.loader.WebappClassLoaderBase [Thread-269] The web application [ROOT] appears to have started a thread named [cluster-ClusterId{value='6448c5ffadee62176f29076a', description='null'}-localhost:27017] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/jdk.internal.misc.Unsafe.park(Native Method)
 java.base@17.0.6/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:252)
 java.base@17.0.6/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1672)
 app//com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.waitForSignalOrTimeout(DefaultServerMonitor.java:300)
 app//com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.waitForNext(DefaultServerMonitor.java:281)
 app//com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:179)
 java.base@17.0.6/java.lang.Thread.run(Thread.java:833)
2023-04-26 12:04:46,328 WARN org.apache.catalina.loader.WebappClassLoaderBase [Thread-269] The web application [ROOT] appears to have started a thread named [cluster-rtt-ClusterId{value='6448c5ffadee62176f29076a', description='null'}-localhost:27017] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/java.lang.Thread.sleep(Native Method)
 app//com.mongodb.internal.connection.DefaultServerMonitor.waitForNext(DefaultServerMonitor.java:443)
 app//com.mongodb.internal.connection.DefaultServerMonitor.access$1500(DefaultServerMonitor.java:64)
 app//com.mongodb.internal.connection.DefaultServerMonitor$RoundTripTimeRunnable.run(DefaultServerMonitor.java:415)
 java.base@17.0.6/java.lang.Thread.run(Thread.java:833)
2023-04-26 12:04:46,329 WARN org.apache.catalina.loader.WebappClassLoaderBase [Thread-269] The web application [ROOT] appears to have started a thread named [MaintenanceTimer-18-thread-1] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/jdk.internal.misc.Unsafe.park(Native Method)
 java.base@17.0.6/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:252)
 java.base@17.0.6/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1672)
 java.base@17.0.6/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182)
 java.base@17.0.6/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1062)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1122)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
 java.base@17.0.6/java.lang.Thread.run(Thread.java:833)
2023-04-26 12:04:46,332 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-17, groupId=book-group] Revoke previously assigned partitions my-topic-0
2023-04-26 12:04:46,332 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-17, groupId=book-group] Member consumer-book-group-17-cfa6489e-3e8f-43b1-98fc-b622d1842d6c sending LeaveGroup request to coordinator wks-012:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2023-04-26 12:04:46,333 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-17, groupId=book-group] Resetting generation due to: consumer pro-actively leaving the group
2023-04-26 12:04:46,334 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-17, groupId=book-group] Request joining group due to: consumer pro-actively leaving the group
2023-04-26 12:04:46,334 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-17, groupId=book-group] Unsubscribed all topics or patterns and assigned partitions
2023-04-26 12:04:46,336 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-17, groupId=book-group] Resetting generation due to: consumer pro-actively leaving the group
2023-04-26 12:04:46,336 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-17, groupId=book-group] Request joining group due to: consumer pro-actively leaving the group
2023-04-26 12:04:46,337 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Metrics scheduler closed
2023-04-26 12:04:46,337 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-04-26 12:04:46,337 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Metrics reporters closed
2023-04-26 12:04:46,341 INFO org.apache.kafka.common.utils.AppInfoParser [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] App info kafka.consumer for consumer-book-group-17 unregistered
2023-04-26 12:04:46,350 INFO org.spark_project.jetty.server.AbstractConnector [Thread-269] Stopped Spark@577ab19b{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2023-04-26 12:04:46,354 INFO org.apache.spark.ui.SparkUI [Thread-269] Stopped Spark web UI at http://192.168.1.125:4040
2023-04-26 12:04:46,356 INFO org.apache.spark.MapOutputTrackerMasterEndpoint [dispatcher-event-loop-2] MapOutputTrackerMasterEndpoint stopped!
2023-04-26 12:04:46,372 INFO org.apache.spark.storage.memory.MemoryStore [Thread-269] MemoryStore cleared
2023-04-26 12:04:46,372 INFO org.apache.spark.storage.BlockManager [Thread-269] BlockManager stopped
2023-04-26 12:04:46,373 INFO org.apache.spark.storage.BlockManagerMaster [Thread-269] BlockManagerMaster stopped
2023-04-26 12:04:46,373 INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [dispatcher-event-loop-3] OutputCommitCoordinator stopped!
2023-04-26 12:04:46,388 INFO org.apache.spark.SparkContext [Thread-269] Successfully stopped SparkContext
2023-04-26 12:04:46,389 INFO org.apache.spark.SparkContext [Thread-269] SparkContext already stopped.
2023-04-26 12:04:46,390 INFO com.zaxxer.hikari.HikariDataSource [Thread-269] HikariPool-17 - Shutdown initiated...
2023-04-26 12:04:46,394 INFO com.zaxxer.hikari.HikariDataSource [Thread-269] HikariPool-17 - Shutdown completed.
2023-04-26 12:04:46,679 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] Starting SpringBootSecurityJwtMongodbApplication using Java 17.0.6 on wks-012 with PID 102147 (/home/inferyx/git/SpringApplicationWithSecurity/target/classes started by inferyx in /home/inferyx/git/SpringApplicationWithSecurity)
2023-04-26 12:04:46,680 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] No active profile set, falling back to 1 default profile: "default"
2023-04-26 12:04:47,313 INFO org.apache.catalina.core.StandardService [restartedMain] Starting service [Tomcat]
2023-04-26 12:04:47,313 INFO org.apache.catalina.core.StandardEngine [restartedMain] Starting Servlet engine: [Apache Tomcat/9.0.65]
2023-04-26 12:04:47,341 INFO org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] [restartedMain] Initializing Spring embedded WebApplicationContext
2023-04-26 12:04:47,387 INFO org.mongodb.driver.client [restartedMain] MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync|spring-boot", "version": "4.6.1"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "5.15.0-69-generic"}, "platform": "Java/Eclipse Adoptium/17.0.6+10"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.Jep395RecordCodecProvider@269fb271]}, clusterSettings={hosts=[localhost:27017], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='30000 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, sendBufferSize=0}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, sendBufferSize=0}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=JAVA_LEGACY, serverApi=null, autoEncryptionSettings=null, contextProvider=null}
2023-04-26 12:04:47,388 INFO org.mongodb.driver.connection [cluster-ClusterId{value='6448c607adee62176f29076b', description='null'}-localhost:27017] Opened connection [connectionId{localValue:36, serverValue:78}] to localhost:27017
2023-04-26 12:04:47,389 INFO org.mongodb.driver.connection [cluster-rtt-ClusterId{value='6448c607adee62176f29076b', description='null'}-localhost:27017] Opened connection [connectionId{localValue:37, serverValue:77}] to localhost:27017
2023-04-26 12:04:47,389 INFO org.mongodb.driver.cluster [cluster-ClusterId{value='6448c607adee62176f29076b', description='null'}-localhost:27017] Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=6, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=976497}
2023-04-26 12:04:47,676 INFO org.hibernate.jpa.internal.util.LogHelper [restartedMain] HHH000204: Processing PersistenceUnitInfo [name: default]
2023-04-26 12:04:47,696 INFO com.zaxxer.hikari.HikariDataSource [restartedMain] HikariPool-18 - Starting...
2023-04-26 12:04:47,702 INFO com.zaxxer.hikari.HikariDataSource [restartedMain] HikariPool-18 - Start completed.
2023-04-26 12:04:47,703 INFO org.hibernate.dialect.Dialect [restartedMain] HHH000400: Using dialect: org.hibernate.dialect.MySQL5Dialect
2023-04-26 12:04:47,860 INFO org.hibernate.engine.transaction.jta.platform.internal.JtaPlatformInitiator [restartedMain] HHH000490: Using JtaPlatform implementation: [org.hibernate.engine.transaction.jta.platform.internal.NoJtaPlatform]
2023-04-26 12:04:48,279 INFO org.apache.spark.SparkContext [restartedMain] Running Spark version 2.4.5
2023-04-26 12:04:48,280 INFO org.apache.spark.SparkContext [restartedMain] Submitted application: MyAppName
2023-04-26 12:04:48,282 INFO org.apache.spark.SecurityManager [restartedMain] Changing view acls to: inferyx
2023-04-26 12:04:48,282 INFO org.apache.spark.SecurityManager [restartedMain] Changing modify acls to: inferyx
2023-04-26 12:04:48,282 INFO org.apache.spark.SecurityManager [restartedMain] Changing view acls groups to: 
2023-04-26 12:04:48,283 INFO org.apache.spark.SecurityManager [restartedMain] Changing modify acls groups to: 
2023-04-26 12:04:48,283 INFO org.apache.spark.SecurityManager [restartedMain] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(inferyx); groups with view permissions: Set(); users  with modify permissions: Set(inferyx); groups with modify permissions: Set()
2023-04-26 12:04:48,357 INFO org.apache.spark.util.Utils [restartedMain] Successfully started service 'sparkDriver' on port 34561.
2023-04-26 12:04:48,371 INFO org.apache.spark.SparkEnv [restartedMain] Registering MapOutputTracker
2023-04-26 12:04:48,372 INFO org.apache.spark.SparkEnv [restartedMain] Registering BlockManagerMaster
2023-04-26 12:04:48,373 INFO org.apache.spark.storage.BlockManagerMasterEndpoint [restartedMain] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2023-04-26 12:04:48,373 INFO org.apache.spark.storage.BlockManagerMasterEndpoint [restartedMain] BlockManagerMasterEndpoint up
2023-04-26 12:04:48,374 INFO org.apache.spark.storage.DiskBlockManager [restartedMain] Created local directory at /tmp/blockmgr-7e3ec5d3-04a6-49d2-9d90-e0e239042cad
2023-04-26 12:04:48,374 INFO org.apache.spark.storage.memory.MemoryStore [restartedMain] MemoryStore started with capacity 998.4 MB
2023-04-26 12:04:48,377 INFO org.apache.spark.SparkEnv [restartedMain] Registering OutputCommitCoordinator
2023-04-26 12:04:48,381 INFO org.spark_project.jetty.server.Server [restartedMain] jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2023-04-26 12:04:48,389 INFO org.spark_project.jetty.server.Server [restartedMain] Started @1360622ms
2023-04-26 12:04:48,390 INFO org.spark_project.jetty.server.AbstractConnector [restartedMain] Started ServerConnector@2a457a79{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2023-04-26 12:04:48,391 INFO org.apache.spark.util.Utils [restartedMain] Successfully started service 'SparkUI' on port 4040.
2023-04-26 12:04:48,391 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@17b65a75{/jobs,null,AVAILABLE,@Spark}
2023-04-26 12:04:48,392 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@13259eb9{/jobs/json,null,AVAILABLE,@Spark}
2023-04-26 12:04:48,392 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@7ee4a289{/jobs/job,null,AVAILABLE,@Spark}
2023-04-26 12:04:48,392 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@7f73999e{/jobs/job/json,null,AVAILABLE,@Spark}
2023-04-26 12:04:48,393 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@5ab311f7{/stages,null,AVAILABLE,@Spark}
2023-04-26 12:04:48,393 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@655c9c4e{/stages/json,null,AVAILABLE,@Spark}
2023-04-26 12:04:48,394 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@3e778a86{/stages/stage,null,AVAILABLE,@Spark}
2023-04-26 12:04:48,394 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@5dab4bbf{/stages/stage/json,null,AVAILABLE,@Spark}
2023-04-26 12:04:48,394 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@1f55c554{/stages/pool,null,AVAILABLE,@Spark}
2023-04-26 12:04:48,396 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@1e6a941c{/stages/pool/json,null,AVAILABLE,@Spark}
2023-04-26 12:04:48,396 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@1c4b1eed{/storage,null,AVAILABLE,@Spark}
2023-04-26 12:04:48,397 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@1b3a4868{/storage/json,null,AVAILABLE,@Spark}
2023-04-26 12:04:48,397 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@2af4513e{/storage/rdd,null,AVAILABLE,@Spark}
2023-04-26 12:04:48,397 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@60f4fa08{/storage/rdd/json,null,AVAILABLE,@Spark}
2023-04-26 12:04:48,398 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@248c3c8d{/environment,null,AVAILABLE,@Spark}
2023-04-26 12:04:48,399 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@6d3c597c{/environment/json,null,AVAILABLE,@Spark}
2023-04-26 12:04:48,399 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@65498844{/executors,null,AVAILABLE,@Spark}
2023-04-26 12:04:48,400 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@22aa1a5f{/executors/json,null,AVAILABLE,@Spark}
2023-04-26 12:04:48,400 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@670732d3{/executors/threadDump,null,AVAILABLE,@Spark}
2023-04-26 12:04:48,401 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@687f70f6{/executors/threadDump/json,null,AVAILABLE,@Spark}
2023-04-26 12:04:48,401 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@415dce15{/static,null,AVAILABLE,@Spark}
2023-04-26 12:04:48,402 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@33d74c83{/,null,AVAILABLE,@Spark}
2023-04-26 12:04:48,402 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@6d187180{/api,null,AVAILABLE,@Spark}
2023-04-26 12:04:48,403 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@51f100b8{/jobs/job/kill,null,AVAILABLE,@Spark}
2023-04-26 12:04:48,403 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@58cbed51{/stages/stage/kill,null,AVAILABLE,@Spark}
2023-04-26 12:04:48,403 INFO org.apache.spark.ui.SparkUI [restartedMain] Bound SparkUI to 0.0.0.0, and started at http://192.168.1.125:4040
2023-04-26 12:04:48,441 INFO org.apache.spark.executor.Executor [restartedMain] Starting executor ID driver on host localhost
2023-04-26 12:04:48,445 INFO org.apache.spark.util.Utils [restartedMain] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37625.
2023-04-26 12:04:48,445 INFO org.apache.spark.network.netty.NettyBlockTransferService [restartedMain] Server created on 192.168.1.125:37625
2023-04-26 12:04:48,445 INFO org.apache.spark.storage.BlockManager [restartedMain] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2023-04-26 12:04:48,446 INFO org.apache.spark.storage.BlockManagerMaster [restartedMain] Registering BlockManager BlockManagerId(driver, 192.168.1.125, 37625, None)
2023-04-26 12:04:48,446 INFO org.apache.spark.storage.BlockManagerMasterEndpoint [dispatcher-event-loop-2] Registering block manager 192.168.1.125:37625 with 998.4 MB RAM, BlockManagerId(driver, 192.168.1.125, 37625, None)
2023-04-26 12:04:48,446 INFO org.apache.spark.storage.BlockManagerMaster [restartedMain] Registered BlockManager BlockManagerId(driver, 192.168.1.125, 37625, None)
2023-04-26 12:04:48,447 INFO org.apache.spark.storage.BlockManager [restartedMain] Initialized BlockManager: BlockManagerId(driver, 192.168.1.125, 37625, None)
2023-04-26 12:04:48,448 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@75666d2a{/metrics/json,null,AVAILABLE,@Spark}
2023-04-26 12:04:49,935 INFO org.apache.kafka.clients.consumer.ConsumerConfig [restartedMain] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-book-group-18
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = book-group
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2023-04-26 12:04:49,939 INFO org.apache.kafka.common.utils.AppInfoParser [restartedMain] Kafka version: 3.1.1
2023-04-26 12:04:49,939 INFO org.apache.kafka.common.utils.AppInfoParser [restartedMain] Kafka commitId: 97671528ba54a138
2023-04-26 12:04:49,940 INFO org.apache.kafka.common.utils.AppInfoParser [restartedMain] Kafka startTimeMs: 1682490889939
2023-04-26 12:04:49,940 INFO org.apache.kafka.clients.consumer.KafkaConsumer [restartedMain] [Consumer clientId=consumer-book-group-18, groupId=book-group] Subscribed to topic(s): my-topic
2023-04-26 12:04:49,945 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-18, groupId=book-group] Resetting the last seen epoch of partition my-topic-0 to 0 since the associated topicId changed from null to QqwJ2XdJQbi2BDXQbzT51g
2023-04-26 12:04:49,946 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-18, groupId=book-group] Cluster ID: 6CnV1ouTQjGsP-r9Rd2WRA
2023-04-26 12:04:49,948 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-18, groupId=book-group] Discovered group coordinator wks-012:9092 (id: 2147483647 rack: null)
2023-04-26 12:04:49,948 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-18, groupId=book-group] (Re-)joining group
2023-04-26 12:04:49,953 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-18, groupId=book-group] Request joining group due to: need to re-join with the given member-id
2023-04-26 12:04:49,954 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-18, groupId=book-group] (Re-)joining group
2023-04-26 12:04:49,957 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-18, groupId=book-group] Successfully joined group with generation Generation{generationId=165, memberId='consumer-book-group-18-77343b22-cf7a-4d9b-9add-85049a650d98', protocol='range'}
2023-04-26 12:04:49,957 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-18, groupId=book-group] Finished assignment for group at generation 165: {consumer-book-group-18-77343b22-cf7a-4d9b-9add-85049a650d98=Assignment(partitions=[my-topic-0])}
2023-04-26 12:04:49,960 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] Started SpringBootSecurityJwtMongodbApplication in 3.343 seconds (JVM running for 1362.193)
2023-04-26 12:04:49,965 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-18, groupId=book-group] Successfully synced group in generation Generation{generationId=165, memberId='consumer-book-group-18-77343b22-cf7a-4d9b-9add-85049a650d98', protocol='range'}
2023-04-26 12:04:49,965 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-18, groupId=book-group] Notifying assignor about the new Assignment(partitions=[my-topic-0])
2023-04-26 12:04:49,966 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-18, groupId=book-group] Adding newly assigned partitions: my-topic-0
2023-04-26 12:04:49,968 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] This is a info message
2023-04-26 12:04:49,969 WARN com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] This is a warn message
2023-04-26 12:04:49,973 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-18, groupId=book-group] Setting offset for partition my-topic-0 to the committed offset FetchPosition{offset=415, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[wks-012:9092 (id: 0 rack: null)], epoch=0}}
2023-04-26 12:04:56,719 INFO org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] [http-nio-8080-exec-1] Initializing Spring DispatcherServlet 'dispatcherServlet'
2023-04-26 12:04:57,750 INFO org.springdoc.api.AbstractOpenApiResource [http-nio-8080-exec-9] Init duration for springdoc-openapi is: 327 ms
2023-04-26 12:05:50,358 INFO org.mongodb.driver.connection [http-nio-8080-exec-1] Opened connection [connectionId{localValue:38, serverValue:79}] to localhost:27017
2023-04-26 12:09:01,474 INFO org.apache.catalina.core.StandardService [Thread-286] Stopping service [Tomcat]
2023-04-26 12:09:01,475 INFO org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] [Thread-286] Destroying Spring FrameworkServlet 'dispatcherServlet'
2023-04-26 12:09:01,491 WARN org.apache.catalina.loader.WebappClassLoaderBase [Thread-286] The web application [ROOT] appears to have started a thread named [cluster-ClusterId{value='6448c607adee62176f29076b', description='null'}-localhost:27017] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/jdk.internal.misc.Unsafe.park(Native Method)
 java.base@17.0.6/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:252)
 java.base@17.0.6/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1672)
 app//com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.waitForSignalOrTimeout(DefaultServerMonitor.java:300)
 app//com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.waitForNext(DefaultServerMonitor.java:281)
 app//com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:179)
 java.base@17.0.6/java.lang.Thread.run(Thread.java:833)
2023-04-26 12:09:01,496 WARN org.apache.catalina.loader.WebappClassLoaderBase [Thread-286] The web application [ROOT] appears to have started a thread named [cluster-rtt-ClusterId{value='6448c607adee62176f29076b', description='null'}-localhost:27017] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/java.lang.Thread.sleep(Native Method)
 app//com.mongodb.internal.connection.DefaultServerMonitor.waitForNext(DefaultServerMonitor.java:443)
 app//com.mongodb.internal.connection.DefaultServerMonitor.access$1500(DefaultServerMonitor.java:64)
 app//com.mongodb.internal.connection.DefaultServerMonitor$RoundTripTimeRunnable.run(DefaultServerMonitor.java:415)
 java.base@17.0.6/java.lang.Thread.run(Thread.java:833)
2023-04-26 12:09:01,497 WARN org.apache.catalina.loader.WebappClassLoaderBase [Thread-286] The web application [ROOT] appears to have started a thread named [MaintenanceTimer-19-thread-1] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/jdk.internal.misc.Unsafe.park(Native Method)
 java.base@17.0.6/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:252)
 java.base@17.0.6/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1672)
 java.base@17.0.6/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182)
 java.base@17.0.6/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1062)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1122)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
 java.base@17.0.6/java.lang.Thread.run(Thread.java:833)
2023-04-26 12:09:01,503 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-18, groupId=book-group] Revoke previously assigned partitions my-topic-0
2023-04-26 12:09:01,504 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-18, groupId=book-group] Member consumer-book-group-18-77343b22-cf7a-4d9b-9add-85049a650d98 sending LeaveGroup request to coordinator wks-012:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2023-04-26 12:09:01,507 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-18, groupId=book-group] Resetting generation due to: consumer pro-actively leaving the group
2023-04-26 12:09:01,507 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-18, groupId=book-group] Request joining group due to: consumer pro-actively leaving the group
2023-04-26 12:09:01,507 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-18, groupId=book-group] Unsubscribed all topics or patterns and assigned partitions
2023-04-26 12:09:01,508 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-18, groupId=book-group] Resetting generation due to: consumer pro-actively leaving the group
2023-04-26 12:09:01,509 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-18, groupId=book-group] Request joining group due to: consumer pro-actively leaving the group
2023-04-26 12:09:01,509 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Metrics scheduler closed
2023-04-26 12:09:01,509 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-04-26 12:09:01,510 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Metrics reporters closed
2023-04-26 12:09:01,526 INFO org.apache.kafka.common.utils.AppInfoParser [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] App info kafka.consumer for consumer-book-group-18 unregistered
2023-04-26 12:09:01,531 INFO org.spark_project.jetty.server.AbstractConnector [Thread-286] Stopped Spark@2a457a79{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2023-04-26 12:09:01,533 INFO org.apache.spark.ui.SparkUI [Thread-286] Stopped Spark web UI at http://192.168.1.125:4040
2023-04-26 12:09:01,543 INFO org.apache.spark.MapOutputTrackerMasterEndpoint [dispatcher-event-loop-0] MapOutputTrackerMasterEndpoint stopped!
2023-04-26 12:09:01,580 INFO org.apache.spark.storage.memory.MemoryStore [Thread-286] MemoryStore cleared
2023-04-26 12:09:01,580 INFO org.apache.spark.storage.BlockManager [Thread-286] BlockManager stopped
2023-04-26 12:09:01,581 INFO org.apache.spark.storage.BlockManagerMaster [Thread-286] BlockManagerMaster stopped
2023-04-26 12:09:01,581 INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [dispatcher-event-loop-1] OutputCommitCoordinator stopped!
2023-04-26 12:09:01,605 INFO org.apache.spark.SparkContext [Thread-286] Successfully stopped SparkContext
2023-04-26 12:09:01,605 INFO org.apache.spark.SparkContext [Thread-286] SparkContext already stopped.
2023-04-26 12:09:01,606 INFO com.zaxxer.hikari.HikariDataSource [Thread-286] HikariPool-18 - Shutdown initiated...
2023-04-26 12:09:01,611 INFO com.zaxxer.hikari.HikariDataSource [Thread-286] HikariPool-18 - Shutdown completed.
2023-04-26 12:09:02,098 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] Starting SpringBootSecurityJwtMongodbApplication using Java 17.0.6 on wks-012 with PID 102147 (/home/inferyx/git/SpringApplicationWithSecurity/target/classes started by inferyx in /home/inferyx/git/SpringApplicationWithSecurity)
2023-04-26 12:09:02,099 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] No active profile set, falling back to 1 default profile: "default"
2023-04-26 12:09:02,796 INFO org.apache.catalina.core.StandardService [restartedMain] Starting service [Tomcat]
2023-04-26 12:09:02,797 INFO org.apache.catalina.core.StandardEngine [restartedMain] Starting Servlet engine: [Apache Tomcat/9.0.65]
2023-04-26 12:09:02,833 INFO org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] [restartedMain] Initializing Spring embedded WebApplicationContext
2023-04-26 12:09:02,878 INFO org.mongodb.driver.client [restartedMain] MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync|spring-boot", "version": "4.6.1"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "5.15.0-69-generic"}, "platform": "Java/Eclipse Adoptium/17.0.6+10"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.Jep395RecordCodecProvider@269fb271]}, clusterSettings={hosts=[localhost:27017], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='30000 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, sendBufferSize=0}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, sendBufferSize=0}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=JAVA_LEGACY, serverApi=null, autoEncryptionSettings=null, contextProvider=null}
2023-04-26 12:09:02,881 INFO org.mongodb.driver.connection [cluster-ClusterId{value='6448c706adee62176f29076c', description='null'}-localhost:27017] Opened connection [connectionId{localValue:39, serverValue:80}] to localhost:27017
2023-04-26 12:09:02,881 INFO org.mongodb.driver.cluster [cluster-ClusterId{value='6448c706adee62176f29076c', description='null'}-localhost:27017] Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=6, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1213934}
2023-04-26 12:09:02,899 INFO org.mongodb.driver.connection [cluster-rtt-ClusterId{value='6448c706adee62176f29076c', description='null'}-localhost:27017] Opened connection [connectionId{localValue:40, serverValue:81}] to localhost:27017
2023-04-26 12:09:03,231 INFO org.hibernate.jpa.internal.util.LogHelper [restartedMain] HHH000204: Processing PersistenceUnitInfo [name: default]
2023-04-26 12:09:03,255 INFO com.zaxxer.hikari.HikariDataSource [restartedMain] HikariPool-19 - Starting...
2023-04-26 12:09:03,260 INFO com.zaxxer.hikari.HikariDataSource [restartedMain] HikariPool-19 - Start completed.
2023-04-26 12:09:03,261 INFO org.hibernate.dialect.Dialect [restartedMain] HHH000400: Using dialect: org.hibernate.dialect.MySQL5Dialect
2023-04-26 12:09:03,359 INFO org.hibernate.engine.transaction.jta.platform.internal.JtaPlatformInitiator [restartedMain] HHH000490: Using JtaPlatform implementation: [org.hibernate.engine.transaction.jta.platform.internal.NoJtaPlatform]
2023-04-26 12:09:03,646 INFO org.apache.spark.SparkContext [restartedMain] Running Spark version 2.4.5
2023-04-26 12:09:03,647 INFO org.apache.spark.SparkContext [restartedMain] Submitted application: MyAppName
2023-04-26 12:09:03,656 INFO org.apache.spark.SecurityManager [restartedMain] Changing view acls to: inferyx
2023-04-26 12:09:03,657 INFO org.apache.spark.SecurityManager [restartedMain] Changing modify acls to: inferyx
2023-04-26 12:09:03,657 INFO org.apache.spark.SecurityManager [restartedMain] Changing view acls groups to: 
2023-04-26 12:09:03,657 INFO org.apache.spark.SecurityManager [restartedMain] Changing modify acls groups to: 
2023-04-26 12:09:03,658 INFO org.apache.spark.SecurityManager [restartedMain] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(inferyx); groups with view permissions: Set(); users  with modify permissions: Set(inferyx); groups with modify permissions: Set()
2023-04-26 12:09:03,727 INFO org.apache.spark.util.Utils [restartedMain] Successfully started service 'sparkDriver' on port 37175.
2023-04-26 12:09:03,733 INFO org.apache.spark.SparkEnv [restartedMain] Registering MapOutputTracker
2023-04-26 12:09:03,735 INFO org.apache.spark.SparkEnv [restartedMain] Registering BlockManagerMaster
2023-04-26 12:09:03,735 INFO org.apache.spark.storage.BlockManagerMasterEndpoint [restartedMain] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2023-04-26 12:09:03,736 INFO org.apache.spark.storage.BlockManagerMasterEndpoint [restartedMain] BlockManagerMasterEndpoint up
2023-04-26 12:09:03,737 INFO org.apache.spark.storage.DiskBlockManager [restartedMain] Created local directory at /tmp/blockmgr-dc214804-5b2a-4768-811d-6a5fe36ed0ab
2023-04-26 12:09:03,737 INFO org.apache.spark.storage.memory.MemoryStore [restartedMain] MemoryStore started with capacity 998.4 MB
2023-04-26 12:09:03,741 INFO org.apache.spark.SparkEnv [restartedMain] Registering OutputCommitCoordinator
2023-04-26 12:09:03,747 INFO org.spark_project.jetty.server.Server [restartedMain] jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2023-04-26 12:09:03,749 INFO org.spark_project.jetty.server.Server [restartedMain] Started @1615982ms
2023-04-26 12:09:03,750 INFO org.spark_project.jetty.server.AbstractConnector [restartedMain] Started ServerConnector@6a89a85d{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2023-04-26 12:09:03,750 INFO org.apache.spark.util.Utils [restartedMain] Successfully started service 'SparkUI' on port 4040.
2023-04-26 12:09:03,751 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@5cfac573{/jobs,null,AVAILABLE,@Spark}
2023-04-26 12:09:03,752 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@3dfc713{/jobs/json,null,AVAILABLE,@Spark}
2023-04-26 12:09:03,752 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@4b4a6537{/jobs/job,null,AVAILABLE,@Spark}
2023-04-26 12:09:03,753 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@6c26f743{/jobs/job/json,null,AVAILABLE,@Spark}
2023-04-26 12:09:03,753 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@667d76e9{/stages,null,AVAILABLE,@Spark}
2023-04-26 12:09:03,753 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@1be7d26a{/stages/json,null,AVAILABLE,@Spark}
2023-04-26 12:09:03,754 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@31c7ffc{/stages/stage,null,AVAILABLE,@Spark}
2023-04-26 12:09:03,754 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@58db0c33{/stages/stage/json,null,AVAILABLE,@Spark}
2023-04-26 12:09:03,754 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@551cf3ff{/stages/pool,null,AVAILABLE,@Spark}
2023-04-26 12:09:03,755 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@70cd0fb4{/stages/pool/json,null,AVAILABLE,@Spark}
2023-04-26 12:09:03,755 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@10ce6dce{/storage,null,AVAILABLE,@Spark}
2023-04-26 12:09:03,756 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@10f44170{/storage/json,null,AVAILABLE,@Spark}
2023-04-26 12:09:03,756 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@22c5db9a{/storage/rdd,null,AVAILABLE,@Spark}
2023-04-26 12:09:03,757 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@3aedc0d9{/storage/rdd/json,null,AVAILABLE,@Spark}
2023-04-26 12:09:03,757 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@592e6a57{/environment,null,AVAILABLE,@Spark}
2023-04-26 12:09:03,757 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@6be34fc3{/environment/json,null,AVAILABLE,@Spark}
2023-04-26 12:09:03,758 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@67f6402e{/executors,null,AVAILABLE,@Spark}
2023-04-26 12:09:03,758 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@33c9bf01{/executors/json,null,AVAILABLE,@Spark}
2023-04-26 12:09:03,758 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@191f17fd{/executors/threadDump,null,AVAILABLE,@Spark}
2023-04-26 12:09:03,759 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@6873ce4a{/executors/threadDump/json,null,AVAILABLE,@Spark}
2023-04-26 12:09:03,759 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@3dd1045a{/static,null,AVAILABLE,@Spark}
2023-04-26 12:09:03,760 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@439e8556{/,null,AVAILABLE,@Spark}
2023-04-26 12:09:03,760 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@225fd227{/api,null,AVAILABLE,@Spark}
2023-04-26 12:09:03,760 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@61ee74a3{/jobs/job/kill,null,AVAILABLE,@Spark}
2023-04-26 12:09:03,761 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@56edf9fd{/stages/stage/kill,null,AVAILABLE,@Spark}
2023-04-26 12:09:03,761 INFO org.apache.spark.ui.SparkUI [restartedMain] Bound SparkUI to 0.0.0.0, and started at http://192.168.1.125:4040
2023-04-26 12:09:03,793 INFO org.apache.spark.executor.Executor [restartedMain] Starting executor ID driver on host localhost
2023-04-26 12:09:03,801 INFO org.apache.spark.util.Utils [restartedMain] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35431.
2023-04-26 12:09:03,802 INFO org.apache.spark.network.netty.NettyBlockTransferService [restartedMain] Server created on 192.168.1.125:35431
2023-04-26 12:09:03,802 INFO org.apache.spark.storage.BlockManager [restartedMain] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2023-04-26 12:09:03,802 INFO org.apache.spark.storage.BlockManagerMaster [restartedMain] Registering BlockManager BlockManagerId(driver, 192.168.1.125, 35431, None)
2023-04-26 12:09:03,803 INFO org.apache.spark.storage.BlockManagerMasterEndpoint [dispatcher-event-loop-1] Registering block manager 192.168.1.125:35431 with 998.4 MB RAM, BlockManagerId(driver, 192.168.1.125, 35431, None)
2023-04-26 12:09:03,803 INFO org.apache.spark.storage.BlockManagerMaster [restartedMain] Registered BlockManager BlockManagerId(driver, 192.168.1.125, 35431, None)
2023-04-26 12:09:03,804 INFO org.apache.spark.storage.BlockManager [restartedMain] Initialized BlockManager: BlockManagerId(driver, 192.168.1.125, 35431, None)
2023-04-26 12:09:03,805 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@52dd6d1b{/metrics/json,null,AVAILABLE,@Spark}
2023-04-26 12:09:05,474 INFO org.apache.kafka.clients.consumer.ConsumerConfig [restartedMain] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-book-group-19
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = book-group
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2023-04-26 12:09:05,478 INFO org.apache.kafka.common.utils.AppInfoParser [restartedMain] Kafka version: 3.1.1
2023-04-26 12:09:05,479 INFO org.apache.kafka.common.utils.AppInfoParser [restartedMain] Kafka commitId: 97671528ba54a138
2023-04-26 12:09:05,479 INFO org.apache.kafka.common.utils.AppInfoParser [restartedMain] Kafka startTimeMs: 1682491145478
2023-04-26 12:09:05,479 INFO org.apache.kafka.clients.consumer.KafkaConsumer [restartedMain] [Consumer clientId=consumer-book-group-19, groupId=book-group] Subscribed to topic(s): my-topic
2023-04-26 12:09:05,487 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-19, groupId=book-group] Resetting the last seen epoch of partition my-topic-0 to 0 since the associated topicId changed from null to QqwJ2XdJQbi2BDXQbzT51g
2023-04-26 12:09:05,488 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-19, groupId=book-group] Cluster ID: 6CnV1ouTQjGsP-r9Rd2WRA
2023-04-26 12:09:05,493 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-19, groupId=book-group] Discovered group coordinator wks-012:9092 (id: 2147483647 rack: null)
2023-04-26 12:09:05,495 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-19, groupId=book-group] (Re-)joining group
2023-04-26 12:09:05,500 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-19, groupId=book-group] Request joining group due to: need to re-join with the given member-id
2023-04-26 12:09:05,500 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-19, groupId=book-group] (Re-)joining group
2023-04-26 12:09:05,502 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] Started SpringBootSecurityJwtMongodbApplication in 3.531 seconds (JVM running for 1617.735)
2023-04-26 12:09:05,503 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-19, groupId=book-group] Successfully joined group with generation Generation{generationId=167, memberId='consumer-book-group-19-642fe059-35d8-4f66-8971-699a7c13200a', protocol='range'}
2023-04-26 12:09:05,503 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-19, groupId=book-group] Finished assignment for group at generation 167: {consumer-book-group-19-642fe059-35d8-4f66-8971-699a7c13200a=Assignment(partitions=[my-topic-0])}
2023-04-26 12:09:05,506 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-19, groupId=book-group] Successfully synced group in generation Generation{generationId=167, memberId='consumer-book-group-19-642fe059-35d8-4f66-8971-699a7c13200a', protocol='range'}
2023-04-26 12:09:05,507 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] This is a info message
2023-04-26 12:09:05,507 WARN com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] This is a warn message
2023-04-26 12:09:05,507 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-19, groupId=book-group] Notifying assignor about the new Assignment(partitions=[my-topic-0])
2023-04-26 12:09:05,509 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-19, groupId=book-group] Adding newly assigned partitions: my-topic-0
2023-04-26 12:09:05,517 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-19, groupId=book-group] Setting offset for partition my-topic-0 to the committed offset FetchPosition{offset=415, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[wks-012:9092 (id: 0 rack: null)], epoch=0}}
2023-04-26 12:09:24,319 INFO org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] [http-nio-8080-exec-1] Initializing Spring DispatcherServlet 'dispatcherServlet'
2023-04-26 12:09:25,203 INFO org.springdoc.api.AbstractOpenApiResource [http-nio-8080-exec-9] Init duration for springdoc-openapi is: 327 ms
2023-04-26 12:09:53,175 INFO org.mongodb.driver.connection [http-nio-8080-exec-1] Opened connection [connectionId{localValue:41, serverValue:82}] to localhost:27017
2023-04-26 12:10:59,325 INFO org.apache.catalina.core.StandardService [Thread-302] Stopping service [Tomcat]
2023-04-26 12:10:59,326 INFO org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] [Thread-302] Destroying Spring FrameworkServlet 'dispatcherServlet'
2023-04-26 12:10:59,330 WARN org.apache.catalina.loader.WebappClassLoaderBase [Thread-302] The web application [ROOT] appears to have started a thread named [cluster-ClusterId{value='6448c706adee62176f29076c', description='null'}-localhost:27017] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/jdk.internal.misc.Unsafe.park(Native Method)
 java.base@17.0.6/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:252)
 java.base@17.0.6/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1672)
 app//com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.waitForSignalOrTimeout(DefaultServerMonitor.java:300)
 app//com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.waitForNext(DefaultServerMonitor.java:281)
 app//com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:179)
 java.base@17.0.6/java.lang.Thread.run(Thread.java:833)
2023-04-26 12:10:59,337 WARN org.apache.catalina.loader.WebappClassLoaderBase [Thread-302] The web application [ROOT] appears to have started a thread named [cluster-rtt-ClusterId{value='6448c706adee62176f29076c', description='null'}-localhost:27017] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/java.lang.Thread.sleep(Native Method)
 app//com.mongodb.internal.connection.DefaultServerMonitor.waitForNext(DefaultServerMonitor.java:443)
 app//com.mongodb.internal.connection.DefaultServerMonitor.access$1500(DefaultServerMonitor.java:64)
 app//com.mongodb.internal.connection.DefaultServerMonitor$RoundTripTimeRunnable.run(DefaultServerMonitor.java:415)
 java.base@17.0.6/java.lang.Thread.run(Thread.java:833)
2023-04-26 12:10:59,339 WARN org.apache.catalina.loader.WebappClassLoaderBase [Thread-302] The web application [ROOT] appears to have started a thread named [MaintenanceTimer-20-thread-1] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/jdk.internal.misc.Unsafe.park(Native Method)
 java.base@17.0.6/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:252)
 java.base@17.0.6/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1672)
 java.base@17.0.6/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182)
 java.base@17.0.6/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1062)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1122)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
 java.base@17.0.6/java.lang.Thread.run(Thread.java:833)
2023-04-26 12:10:59,342 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-19, groupId=book-group] Revoke previously assigned partitions my-topic-0
2023-04-26 12:10:59,343 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-19, groupId=book-group] Member consumer-book-group-19-642fe059-35d8-4f66-8971-699a7c13200a sending LeaveGroup request to coordinator wks-012:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2023-04-26 12:10:59,343 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-19, groupId=book-group] Resetting generation due to: consumer pro-actively leaving the group
2023-04-26 12:10:59,343 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-19, groupId=book-group] Request joining group due to: consumer pro-actively leaving the group
2023-04-26 12:10:59,343 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-19, groupId=book-group] Unsubscribed all topics or patterns and assigned partitions
2023-04-26 12:10:59,346 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-19, groupId=book-group] Resetting generation due to: consumer pro-actively leaving the group
2023-04-26 12:10:59,346 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-19, groupId=book-group] Request joining group due to: consumer pro-actively leaving the group
2023-04-26 12:10:59,347 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Metrics scheduler closed
2023-04-26 12:10:59,347 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-04-26 12:10:59,347 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Metrics reporters closed
2023-04-26 12:10:59,352 INFO org.apache.kafka.common.utils.AppInfoParser [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] App info kafka.consumer for consumer-book-group-19 unregistered
2023-04-26 12:10:59,360 INFO org.spark_project.jetty.server.AbstractConnector [Thread-302] Stopped Spark@6a89a85d{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2023-04-26 12:10:59,362 INFO org.apache.spark.ui.SparkUI [Thread-302] Stopped Spark web UI at http://192.168.1.125:4040
2023-04-26 12:10:59,365 INFO org.apache.spark.MapOutputTrackerMasterEndpoint [dispatcher-event-loop-3] MapOutputTrackerMasterEndpoint stopped!
2023-04-26 12:10:59,388 INFO org.apache.spark.storage.memory.MemoryStore [Thread-302] MemoryStore cleared
2023-04-26 12:10:59,389 INFO org.apache.spark.storage.BlockManager [Thread-302] BlockManager stopped
2023-04-26 12:10:59,389 INFO org.apache.spark.storage.BlockManagerMaster [Thread-302] BlockManagerMaster stopped
2023-04-26 12:10:59,390 INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [dispatcher-event-loop-3] OutputCommitCoordinator stopped!
2023-04-26 12:10:59,399 INFO org.apache.spark.SparkContext [Thread-302] Successfully stopped SparkContext
2023-04-26 12:10:59,399 INFO org.apache.spark.SparkContext [Thread-302] SparkContext already stopped.
2023-04-26 12:10:59,401 INFO com.zaxxer.hikari.HikariDataSource [Thread-302] HikariPool-19 - Shutdown initiated...
2023-04-26 12:10:59,406 INFO com.zaxxer.hikari.HikariDataSource [Thread-302] HikariPool-19 - Shutdown completed.
2023-04-26 12:10:59,838 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] Starting SpringBootSecurityJwtMongodbApplication using Java 17.0.6 on wks-012 with PID 102147 (/home/inferyx/git/SpringApplicationWithSecurity/target/classes started by inferyx in /home/inferyx/git/SpringApplicationWithSecurity)
2023-04-26 12:10:59,838 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] No active profile set, falling back to 1 default profile: "default"
2023-04-26 12:11:00,570 INFO org.apache.catalina.core.StandardService [restartedMain] Starting service [Tomcat]
2023-04-26 12:11:00,571 INFO org.apache.catalina.core.StandardEngine [restartedMain] Starting Servlet engine: [Apache Tomcat/9.0.65]
2023-04-26 12:11:00,598 INFO org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] [restartedMain] Initializing Spring embedded WebApplicationContext
2023-04-26 12:11:00,640 INFO org.mongodb.driver.client [restartedMain] MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync|spring-boot", "version": "4.6.1"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "5.15.0-69-generic"}, "platform": "Java/Eclipse Adoptium/17.0.6+10"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.Jep395RecordCodecProvider@269fb271]}, clusterSettings={hosts=[localhost:27017], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='30000 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, sendBufferSize=0}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, sendBufferSize=0}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=JAVA_LEGACY, serverApi=null, autoEncryptionSettings=null, contextProvider=null}
2023-04-26 12:11:00,641 INFO org.mongodb.driver.connection [cluster-ClusterId{value='6448c77cadee62176f29076d', description='null'}-localhost:27017] Opened connection [connectionId{localValue:42, serverValue:83}] to localhost:27017
2023-04-26 12:11:00,641 INFO org.mongodb.driver.connection [cluster-rtt-ClusterId{value='6448c77cadee62176f29076d', description='null'}-localhost:27017] Opened connection [connectionId{localValue:43, serverValue:84}] to localhost:27017
2023-04-26 12:11:00,641 INFO org.mongodb.driver.cluster [cluster-ClusterId{value='6448c77cadee62176f29076d', description='null'}-localhost:27017] Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=6, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=668151}
2023-04-26 12:11:00,847 INFO org.hibernate.jpa.internal.util.LogHelper [restartedMain] HHH000204: Processing PersistenceUnitInfo [name: default]
2023-04-26 12:11:00,859 INFO com.zaxxer.hikari.HikariDataSource [restartedMain] HikariPool-20 - Starting...
2023-04-26 12:11:00,864 INFO com.zaxxer.hikari.HikariDataSource [restartedMain] HikariPool-20 - Start completed.
2023-04-26 12:11:00,865 INFO org.hibernate.dialect.Dialect [restartedMain] HHH000400: Using dialect: org.hibernate.dialect.MySQL5Dialect
2023-04-26 12:11:00,936 INFO org.hibernate.engine.transaction.jta.platform.internal.JtaPlatformInitiator [restartedMain] HHH000490: Using JtaPlatform implementation: [org.hibernate.engine.transaction.jta.platform.internal.NoJtaPlatform]
2023-04-26 12:11:01,186 INFO org.apache.spark.SparkContext [restartedMain] Running Spark version 2.4.5
2023-04-26 12:11:01,187 INFO org.apache.spark.SparkContext [restartedMain] Submitted application: MyAppName
2023-04-26 12:11:01,188 INFO org.apache.spark.SecurityManager [restartedMain] Changing view acls to: inferyx
2023-04-26 12:11:01,189 INFO org.apache.spark.SecurityManager [restartedMain] Changing modify acls to: inferyx
2023-04-26 12:11:01,189 INFO org.apache.spark.SecurityManager [restartedMain] Changing view acls groups to: 
2023-04-26 12:11:01,189 INFO org.apache.spark.SecurityManager [restartedMain] Changing modify acls groups to: 
2023-04-26 12:11:01,190 INFO org.apache.spark.SecurityManager [restartedMain] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(inferyx); groups with view permissions: Set(); users  with modify permissions: Set(inferyx); groups with modify permissions: Set()
2023-04-26 12:11:01,236 INFO org.apache.spark.util.Utils [restartedMain] Successfully started service 'sparkDriver' on port 36545.
2023-04-26 12:11:01,238 INFO org.apache.spark.SparkEnv [restartedMain] Registering MapOutputTracker
2023-04-26 12:11:01,239 INFO org.apache.spark.SparkEnv [restartedMain] Registering BlockManagerMaster
2023-04-26 12:11:01,240 INFO org.apache.spark.storage.BlockManagerMasterEndpoint [restartedMain] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2023-04-26 12:11:01,240 INFO org.apache.spark.storage.BlockManagerMasterEndpoint [restartedMain] BlockManagerMasterEndpoint up
2023-04-26 12:11:01,240 INFO org.apache.spark.storage.DiskBlockManager [restartedMain] Created local directory at /tmp/blockmgr-f6faa9a3-4295-460d-8a6e-7757bbbfe144
2023-04-26 12:11:01,241 INFO org.apache.spark.storage.memory.MemoryStore [restartedMain] MemoryStore started with capacity 998.4 MB
2023-04-26 12:11:01,242 INFO org.apache.spark.SparkEnv [restartedMain] Registering OutputCommitCoordinator
2023-04-26 12:11:01,246 INFO org.spark_project.jetty.server.Server [restartedMain] jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2023-04-26 12:11:01,252 INFO org.spark_project.jetty.server.Server [restartedMain] Started @1733485ms
2023-04-26 12:11:01,254 INFO org.spark_project.jetty.server.AbstractConnector [restartedMain] Started ServerConnector@59b69054{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2023-04-26 12:11:01,254 INFO org.apache.spark.util.Utils [restartedMain] Successfully started service 'SparkUI' on port 4040.
2023-04-26 12:11:01,255 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@615c9289{/jobs,null,AVAILABLE,@Spark}
2023-04-26 12:11:01,255 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@39625941{/jobs/json,null,AVAILABLE,@Spark}
2023-04-26 12:11:01,256 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@6cadd5b7{/jobs/job,null,AVAILABLE,@Spark}
2023-04-26 12:11:01,256 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@174dc46{/jobs/job/json,null,AVAILABLE,@Spark}
2023-04-26 12:11:01,256 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@4b9de3b{/stages,null,AVAILABLE,@Spark}
2023-04-26 12:11:01,256 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@2f3a37bc{/stages/json,null,AVAILABLE,@Spark}
2023-04-26 12:11:01,257 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@2d384f81{/stages/stage,null,AVAILABLE,@Spark}
2023-04-26 12:11:01,257 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@a89208c{/stages/stage/json,null,AVAILABLE,@Spark}
2023-04-26 12:11:01,257 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@408b7fe0{/stages/pool,null,AVAILABLE,@Spark}
2023-04-26 12:11:01,258 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@3e72379a{/stages/pool/json,null,AVAILABLE,@Spark}
2023-04-26 12:11:01,258 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@51592392{/storage,null,AVAILABLE,@Spark}
2023-04-26 12:11:01,259 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@1217f475{/storage/json,null,AVAILABLE,@Spark}
2023-04-26 12:11:01,259 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@9f8fda8{/storage/rdd,null,AVAILABLE,@Spark}
2023-04-26 12:11:01,259 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@2619c5ea{/storage/rdd/json,null,AVAILABLE,@Spark}
2023-04-26 12:11:01,260 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@3e442708{/environment,null,AVAILABLE,@Spark}
2023-04-26 12:11:01,260 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@135dd8e9{/environment/json,null,AVAILABLE,@Spark}
2023-04-26 12:11:01,261 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@5d067297{/executors,null,AVAILABLE,@Spark}
2023-04-26 12:11:01,261 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@3558da48{/executors/json,null,AVAILABLE,@Spark}
2023-04-26 12:11:01,262 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@2abbc78c{/executors/threadDump,null,AVAILABLE,@Spark}
2023-04-26 12:11:01,262 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@57dcd8a2{/executors/threadDump/json,null,AVAILABLE,@Spark}
2023-04-26 12:11:01,263 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@39208a04{/static,null,AVAILABLE,@Spark}
2023-04-26 12:11:01,263 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@4a599df2{/,null,AVAILABLE,@Spark}
2023-04-26 12:11:01,264 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@3eaaa492{/api,null,AVAILABLE,@Spark}
2023-04-26 12:11:01,264 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@324fb963{/jobs/job/kill,null,AVAILABLE,@Spark}
2023-04-26 12:11:01,265 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@34ff04d5{/stages/stage/kill,null,AVAILABLE,@Spark}
2023-04-26 12:11:01,265 INFO org.apache.spark.ui.SparkUI [restartedMain] Bound SparkUI to 0.0.0.0, and started at http://192.168.1.125:4040
2023-04-26 12:11:01,289 INFO org.apache.spark.executor.Executor [restartedMain] Starting executor ID driver on host localhost
2023-04-26 12:11:01,292 INFO org.apache.spark.util.Utils [restartedMain] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37199.
2023-04-26 12:11:01,293 INFO org.apache.spark.network.netty.NettyBlockTransferService [restartedMain] Server created on 192.168.1.125:37199
2023-04-26 12:11:01,293 INFO org.apache.spark.storage.BlockManager [restartedMain] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2023-04-26 12:11:01,293 INFO org.apache.spark.storage.BlockManagerMaster [restartedMain] Registering BlockManager BlockManagerId(driver, 192.168.1.125, 37199, None)
2023-04-26 12:11:01,294 INFO org.apache.spark.storage.BlockManagerMasterEndpoint [dispatcher-event-loop-2] Registering block manager 192.168.1.125:37199 with 998.4 MB RAM, BlockManagerId(driver, 192.168.1.125, 37199, None)
2023-04-26 12:11:01,294 INFO org.apache.spark.storage.BlockManagerMaster [restartedMain] Registered BlockManager BlockManagerId(driver, 192.168.1.125, 37199, None)
2023-04-26 12:11:01,294 INFO org.apache.spark.storage.BlockManager [restartedMain] Initialized BlockManager: BlockManagerId(driver, 192.168.1.125, 37199, None)
2023-04-26 12:11:01,295 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@42b473e0{/metrics/json,null,AVAILABLE,@Spark}
2023-04-26 12:11:02,589 INFO org.apache.kafka.clients.consumer.ConsumerConfig [restartedMain] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-book-group-20
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = book-group
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2023-04-26 12:11:02,593 INFO org.apache.kafka.common.utils.AppInfoParser [restartedMain] Kafka version: 3.1.1
2023-04-26 12:11:02,593 INFO org.apache.kafka.common.utils.AppInfoParser [restartedMain] Kafka commitId: 97671528ba54a138
2023-04-26 12:11:02,594 INFO org.apache.kafka.common.utils.AppInfoParser [restartedMain] Kafka startTimeMs: 1682491262593
2023-04-26 12:11:02,594 INFO org.apache.kafka.clients.consumer.KafkaConsumer [restartedMain] [Consumer clientId=consumer-book-group-20, groupId=book-group] Subscribed to topic(s): my-topic
2023-04-26 12:11:02,600 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-20, groupId=book-group] Resetting the last seen epoch of partition my-topic-0 to 0 since the associated topicId changed from null to QqwJ2XdJQbi2BDXQbzT51g
2023-04-26 12:11:02,601 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-20, groupId=book-group] Cluster ID: 6CnV1ouTQjGsP-r9Rd2WRA
2023-04-26 12:11:02,601 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-20, groupId=book-group] Discovered group coordinator wks-012:9092 (id: 2147483647 rack: null)
2023-04-26 12:11:02,603 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-20, groupId=book-group] (Re-)joining group
2023-04-26 12:11:02,606 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-20, groupId=book-group] Request joining group due to: need to re-join with the given member-id
2023-04-26 12:11:02,606 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-20, groupId=book-group] (Re-)joining group
2023-04-26 12:11:02,609 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-20, groupId=book-group] Successfully joined group with generation Generation{generationId=169, memberId='consumer-book-group-20-c713a369-b434-4aa0-ac32-c7c41dc311c5', protocol='range'}
2023-04-26 12:11:02,610 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-20, groupId=book-group] Finished assignment for group at generation 169: {consumer-book-group-20-c713a369-b434-4aa0-ac32-c7c41dc311c5=Assignment(partitions=[my-topic-0])}
2023-04-26 12:11:02,614 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-20, groupId=book-group] Successfully synced group in generation Generation{generationId=169, memberId='consumer-book-group-20-c713a369-b434-4aa0-ac32-c7c41dc311c5', protocol='range'}
2023-04-26 12:11:02,614 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-20, groupId=book-group] Notifying assignor about the new Assignment(partitions=[my-topic-0])
2023-04-26 12:11:02,614 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-20, groupId=book-group] Adding newly assigned partitions: my-topic-0
2023-04-26 12:11:02,617 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-20, groupId=book-group] Setting offset for partition my-topic-0 to the committed offset FetchPosition{offset=415, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[wks-012:9092 (id: 0 rack: null)], epoch=0}}
2023-04-26 12:11:02,617 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] Started SpringBootSecurityJwtMongodbApplication in 2.878 seconds (JVM running for 1734.85)
2023-04-26 12:11:02,623 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] This is a info message
2023-04-26 12:11:02,624 WARN com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] This is a warn message
2023-04-26 12:11:20,504 INFO org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] [http-nio-8080-exec-1] Initializing Spring DispatcherServlet 'dispatcherServlet'
2023-04-26 12:11:20,546 INFO org.mongodb.driver.connection [http-nio-8080-exec-1] Opened connection [connectionId{localValue:44, serverValue:85}] to localhost:27017
2023-04-26 12:11:28,950 INFO org.apache.kafka.clients.producer.ProducerConfig [http-nio-8080-exec-2] ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-2
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.springframework.kafka.support.serializer.JsonSerializer

2023-04-26 12:11:28,952 INFO org.apache.kafka.clients.producer.KafkaProducer [http-nio-8080-exec-2] [Producer clientId=producer-2] Instantiated an idempotent producer.
2023-04-26 12:11:28,956 INFO org.apache.kafka.common.utils.AppInfoParser [http-nio-8080-exec-2] Kafka version: 3.1.1
2023-04-26 12:11:28,956 INFO org.apache.kafka.common.utils.AppInfoParser [http-nio-8080-exec-2] Kafka commitId: 97671528ba54a138
2023-04-26 12:11:28,957 INFO org.apache.kafka.common.utils.AppInfoParser [http-nio-8080-exec-2] Kafka startTimeMs: 1682491288956
2023-04-26 12:11:28,964 INFO org.apache.kafka.clients.Metadata [kafka-producer-network-thread | producer-2] [Producer clientId=producer-2] Resetting the last seen epoch of partition my-topic-0 to 0 since the associated topicId changed from null to QqwJ2XdJQbi2BDXQbzT51g
2023-04-26 12:11:28,966 INFO org.apache.kafka.clients.Metadata [kafka-producer-network-thread | producer-2] [Producer clientId=producer-2] Cluster ID: 6CnV1ouTQjGsP-r9Rd2WRA
2023-04-26 12:11:28,967 INFO org.apache.kafka.clients.producer.internals.TransactionManager [kafka-producer-network-thread | producer-2] [Producer clientId=producer-2] ProducerId set to 3001 with epoch 0
2023-04-26 12:11:49,062 INFO com.example.spring.jwt.mongodb.controllers.EmailController [http-nio-8080-exec-4] Get All Employee
2023-04-26 12:20:02,713 INFO org.apache.kafka.clients.NetworkClient [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-20, groupId=book-group] Node -1 disconnected.
2023-04-26 12:20:29,218 INFO org.apache.kafka.clients.NetworkClient [kafka-producer-network-thread | producer-2] [Producer clientId=producer-2] Node -1 disconnected.
2023-04-26 12:22:27,599 INFO org.apache.catalina.core.StandardService [RMI TCP Connection(42)-127.0.0.1] Stopping service [Tomcat]
2023-04-26 12:22:27,600 INFO org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] [RMI TCP Connection(42)-127.0.0.1] Destroying Spring FrameworkServlet 'dispatcherServlet'
2023-04-26 12:22:27,602 WARN org.apache.catalina.loader.WebappClassLoaderBase [RMI TCP Connection(42)-127.0.0.1] The web application [ROOT] appears to have started a thread named [cluster-ClusterId{value='6448c77cadee62176f29076d', description='null'}-localhost:27017] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/jdk.internal.misc.Unsafe.park(Native Method)
 java.base@17.0.6/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:252)
 java.base@17.0.6/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1672)
 app//com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.waitForSignalOrTimeout(DefaultServerMonitor.java:300)
 app//com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.waitForNext(DefaultServerMonitor.java:281)
 app//com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:179)
 java.base@17.0.6/java.lang.Thread.run(Thread.java:833)
2023-04-26 12:22:27,603 WARN org.apache.catalina.loader.WebappClassLoaderBase [RMI TCP Connection(42)-127.0.0.1] The web application [ROOT] appears to have started a thread named [cluster-rtt-ClusterId{value='6448c77cadee62176f29076d', description='null'}-localhost:27017] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/java.lang.Thread.sleep(Native Method)
 app//com.mongodb.internal.connection.DefaultServerMonitor.waitForNext(DefaultServerMonitor.java:443)
 app//com.mongodb.internal.connection.DefaultServerMonitor.access$1500(DefaultServerMonitor.java:64)
 app//com.mongodb.internal.connection.DefaultServerMonitor$RoundTripTimeRunnable.run(DefaultServerMonitor.java:415)
 java.base@17.0.6/java.lang.Thread.run(Thread.java:833)
2023-04-26 12:22:27,604 WARN org.apache.catalina.loader.WebappClassLoaderBase [RMI TCP Connection(42)-127.0.0.1] The web application [ROOT] appears to have started a thread named [MaintenanceTimer-21-thread-1] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/jdk.internal.misc.Unsafe.park(Native Method)
 java.base@17.0.6/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:252)
 java.base@17.0.6/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1672)
 java.base@17.0.6/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182)
 java.base@17.0.6/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1062)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1122)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
 java.base@17.0.6/java.lang.Thread.run(Thread.java:833)
2023-04-26 12:22:27,607 WARN org.apache.catalina.loader.WebappClassLoaderBase [RMI TCP Connection(42)-127.0.0.1] The web application [ROOT] appears to have started a thread named [kafka-producer-network-thread | producer-2] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/sun.nio.ch.EPoll.wait(Native Method)
 java.base@17.0.6/sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:118)
 java.base@17.0.6/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:129)
 java.base@17.0.6/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:141)
 app//org.apache.kafka.common.network.Selector.select(Selector.java:873)
 app//org.apache.kafka.common.network.Selector.poll(Selector.java:465)
 app//org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:560)
 app//org.apache.kafka.clients.producer.internals.Sender.runOnce(Sender.java:328)
 app//org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:243)
 java.base@17.0.6/java.lang.Thread.run(Thread.java:833)
2023-04-26 12:22:27,611 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-20, groupId=book-group] Revoke previously assigned partitions my-topic-0
2023-04-26 12:22:27,611 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-20, groupId=book-group] Member consumer-book-group-20-c713a369-b434-4aa0-ac32-c7c41dc311c5 sending LeaveGroup request to coordinator wks-012:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2023-04-26 12:22:27,612 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-20, groupId=book-group] Resetting generation due to: consumer pro-actively leaving the group
2023-04-26 12:22:27,612 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-20, groupId=book-group] Request joining group due to: consumer pro-actively leaving the group
2023-04-26 12:22:27,612 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-20, groupId=book-group] Unsubscribed all topics or patterns and assigned partitions
2023-04-26 12:22:27,613 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-20, groupId=book-group] Resetting generation due to: consumer pro-actively leaving the group
2023-04-26 12:22:27,613 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-20, groupId=book-group] Request joining group due to: consumer pro-actively leaving the group
2023-04-26 12:22:27,616 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Metrics scheduler closed
2023-04-26 12:22:27,616 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-04-26 12:22:27,617 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Metrics reporters closed
2023-04-26 12:22:27,619 INFO org.apache.kafka.common.utils.AppInfoParser [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] App info kafka.consumer for consumer-book-group-20 unregistered
2023-04-26 12:22:27,623 INFO org.apache.spark.SparkContext [Thread-7] Invoking stop() from shutdown hook
2023-04-26 12:22:27,625 INFO org.spark_project.jetty.server.AbstractConnector [Thread-7] Stopped Spark@59b69054{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2023-04-26 12:22:27,623 INFO org.apache.kafka.clients.producer.KafkaProducer [RMI TCP Connection(42)-127.0.0.1] [Producer clientId=producer-2] Closing the Kafka producer with timeoutMillis = 30000 ms.
2023-04-26 12:22:27,629 INFO org.apache.spark.ui.SparkUI [Thread-7] Stopped Spark web UI at http://192.168.1.125:4040
2023-04-26 12:22:27,630 INFO org.apache.kafka.common.metrics.Metrics [RMI TCP Connection(42)-127.0.0.1] Metrics scheduler closed
2023-04-26 12:22:27,631 INFO org.apache.kafka.common.metrics.Metrics [RMI TCP Connection(42)-127.0.0.1] Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-04-26 12:22:27,631 INFO org.apache.kafka.common.metrics.Metrics [RMI TCP Connection(42)-127.0.0.1] Metrics reporters closed
2023-04-26 12:22:27,632 INFO org.apache.kafka.common.utils.AppInfoParser [RMI TCP Connection(42)-127.0.0.1] App info kafka.producer for producer-2 unregistered
2023-04-26 12:22:27,632 INFO org.apache.spark.MapOutputTrackerMasterEndpoint [dispatcher-event-loop-3] MapOutputTrackerMasterEndpoint stopped!
2023-04-26 12:22:27,633 INFO org.apache.spark.SparkContext [RMI TCP Connection(42)-127.0.0.1] SparkContext already stopped.
2023-04-26 12:22:27,634 INFO org.apache.spark.SparkContext [RMI TCP Connection(42)-127.0.0.1] SparkContext already stopped.
2023-04-26 12:22:27,636 INFO com.zaxxer.hikari.HikariDataSource [RMI TCP Connection(42)-127.0.0.1] HikariPool-20 - Shutdown initiated...
2023-04-26 12:22:27,640 INFO com.zaxxer.hikari.HikariDataSource [RMI TCP Connection(42)-127.0.0.1] HikariPool-20 - Shutdown completed.
2023-04-26 12:51:30,841 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] Starting SpringBootSecurityJwtMongodbApplication using Java 17.0.6 on wks-012 with PID 109060 (/home/inferyx/git/SpringApplicationWithSecurity/target/classes started by inferyx in /home/inferyx/git/SpringApplicationWithSecurity)
2023-04-26 12:51:30,915 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] No active profile set, falling back to 1 default profile: "default"
2023-04-26 12:51:34,152 INFO org.apache.catalina.core.StandardService [restartedMain] Starting service [Tomcat]
2023-04-26 12:51:34,153 INFO org.apache.catalina.core.StandardEngine [restartedMain] Starting Servlet engine: [Apache Tomcat/9.0.65]
2023-04-26 12:51:34,344 INFO org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] [restartedMain] Initializing Spring embedded WebApplicationContext
2023-04-26 12:51:34,596 INFO org.mongodb.driver.client [restartedMain] MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync|spring-boot", "version": "4.6.1"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "5.15.0-69-generic"}, "platform": "Java/Eclipse Adoptium/17.0.6+10"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.Jep395RecordCodecProvider@22d7de23]}, clusterSettings={hosts=[localhost:27017], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='30000 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, sendBufferSize=0}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, sendBufferSize=0}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=JAVA_LEGACY, serverApi=null, autoEncryptionSettings=null, contextProvider=null}
2023-04-26 12:51:35,444 INFO org.hibernate.jpa.internal.util.LogHelper [restartedMain] HHH000204: Processing PersistenceUnitInfo [name: default]
2023-04-26 12:51:35,533 INFO org.hibernate.Version [restartedMain] HHH000412: Hibernate ORM core version 5.6.10.Final
2023-04-26 12:51:35,787 INFO org.hibernate.annotations.common.Version [restartedMain] HCANN000001: Hibernate Commons Annotations {5.1.2.Final}
2023-04-26 12:51:35,944 INFO com.zaxxer.hikari.HikariDataSource [restartedMain] HikariPool-1 - Starting...
2023-04-26 12:51:36,084 INFO org.mongodb.driver.connection [cluster-rtt-ClusterId{value='6448d0feb2e73c199341b595', description='null'}-localhost:27017] Opened connection [connectionId{localValue:1, serverValue:86}] to localhost:27017
2023-04-26 12:51:36,084 INFO org.mongodb.driver.connection [cluster-ClusterId{value='6448d0feb2e73c199341b595', description='null'}-localhost:27017] Opened connection [connectionId{localValue:2, serverValue:87}] to localhost:27017
2023-04-26 12:51:36,085 INFO org.mongodb.driver.cluster [cluster-ClusterId{value='6448d0feb2e73c199341b595', description='null'}-localhost:27017] Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=6, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1490756377}
2023-04-26 12:51:38,579 INFO com.zaxxer.hikari.HikariDataSource [restartedMain] HikariPool-1 - Start completed.
2023-04-26 12:51:38,614 INFO org.hibernate.dialect.Dialect [restartedMain] HHH000400: Using dialect: org.hibernate.dialect.MySQL5Dialect
2023-04-26 12:51:39,850 INFO org.hibernate.engine.transaction.jta.platform.internal.JtaPlatformInitiator [restartedMain] HHH000490: Using JtaPlatform implementation: [org.hibernate.engine.transaction.jta.platform.internal.NoJtaPlatform]
2023-04-26 12:51:41,010 WARN org.apache.spark.util.Utils [restartedMain] Your hostname, wks-012 resolves to a loopback address: 127.0.1.1; using 192.168.1.125 instead (on interface wlp6s0)
2023-04-26 12:51:41,012 WARN org.apache.spark.util.Utils [restartedMain] Set SPARK_LOCAL_IP if you need to bind to another address
2023-04-26 12:51:41,125 INFO org.apache.spark.SparkContext [restartedMain] Running Spark version 2.4.5
2023-04-26 12:51:41,602 WARN org.apache.hadoop.util.NativeCodeLoader [restartedMain] Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2023-04-26 12:51:41,880 INFO org.apache.spark.SparkContext [restartedMain] Submitted application: MyAppName
2023-04-26 12:51:41,974 INFO org.apache.spark.SecurityManager [restartedMain] Changing view acls to: inferyx
2023-04-26 12:51:41,975 INFO org.apache.spark.SecurityManager [restartedMain] Changing modify acls to: inferyx
2023-04-26 12:51:41,976 INFO org.apache.spark.SecurityManager [restartedMain] Changing view acls groups to: 
2023-04-26 12:51:41,978 INFO org.apache.spark.SecurityManager [restartedMain] Changing modify acls groups to: 
2023-04-26 12:51:41,979 INFO org.apache.spark.SecurityManager [restartedMain] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(inferyx); groups with view permissions: Set(); users  with modify permissions: Set(inferyx); groups with modify permissions: Set()
2023-04-26 12:51:42,550 INFO org.apache.spark.util.Utils [restartedMain] Successfully started service 'sparkDriver' on port 46687.
2023-04-26 12:51:42,598 INFO org.apache.spark.SparkEnv [restartedMain] Registering MapOutputTracker
2023-04-26 12:51:42,634 INFO org.apache.spark.SparkEnv [restartedMain] Registering BlockManagerMaster
2023-04-26 12:51:42,639 INFO org.apache.spark.storage.BlockManagerMasterEndpoint [restartedMain] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2023-04-26 12:51:42,641 INFO org.apache.spark.storage.BlockManagerMasterEndpoint [restartedMain] BlockManagerMasterEndpoint up
2023-04-26 12:51:42,661 INFO org.apache.spark.storage.DiskBlockManager [restartedMain] Created local directory at /tmp/blockmgr-6cc1e4a4-f2c7-49e4-9ca0-3057cac9602b
2023-04-26 12:51:42,705 INFO org.apache.spark.storage.memory.MemoryStore [restartedMain] MemoryStore started with capacity 998.4 MB
2023-04-26 12:51:42,738 INFO org.apache.spark.SparkEnv [restartedMain] Registering OutputCommitCoordinator
2023-04-26 12:51:42,893 INFO org.spark_project.jetty.util.log [restartedMain] Logging initialized @14555ms
2023-04-26 12:51:42,997 INFO org.spark_project.jetty.server.Server [restartedMain] jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2023-04-26 12:51:43,034 INFO org.spark_project.jetty.server.Server [restartedMain] Started @14695ms
2023-04-26 12:51:43,070 INFO org.spark_project.jetty.server.AbstractConnector [restartedMain] Started ServerConnector@7cd22b29{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2023-04-26 12:51:43,070 INFO org.apache.spark.util.Utils [restartedMain] Successfully started service 'SparkUI' on port 4040.
2023-04-26 12:51:43,102 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@124bed63{/jobs,null,AVAILABLE,@Spark}
2023-04-26 12:51:43,104 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@6c75f090{/jobs/json,null,AVAILABLE,@Spark}
2023-04-26 12:51:43,105 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@7783f976{/jobs/job,null,AVAILABLE,@Spark}
2023-04-26 12:51:43,107 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@251d7edf{/jobs/job/json,null,AVAILABLE,@Spark}
2023-04-26 12:51:43,109 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@4dbd6b43{/stages,null,AVAILABLE,@Spark}
2023-04-26 12:51:43,111 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@4b7ee60d{/stages/json,null,AVAILABLE,@Spark}
2023-04-26 12:51:43,113 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@5257fc3b{/stages/stage,null,AVAILABLE,@Spark}
2023-04-26 12:51:43,117 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@6339a414{/stages/stage/json,null,AVAILABLE,@Spark}
2023-04-26 12:51:43,120 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@41cc53c9{/stages/pool,null,AVAILABLE,@Spark}
2023-04-26 12:51:43,121 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@495f13e0{/stages/pool/json,null,AVAILABLE,@Spark}
2023-04-26 12:51:43,123 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@70b0b5a6{/storage,null,AVAILABLE,@Spark}
2023-04-26 12:51:43,124 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@5e87b6a0{/storage/json,null,AVAILABLE,@Spark}
2023-04-26 12:51:43,125 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@7d76e4a2{/storage/rdd,null,AVAILABLE,@Spark}
2023-04-26 12:51:43,127 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@4c0738ff{/storage/rdd/json,null,AVAILABLE,@Spark}
2023-04-26 12:51:43,129 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@396fb95c{/environment,null,AVAILABLE,@Spark}
2023-04-26 12:51:43,132 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@5abe2a67{/environment/json,null,AVAILABLE,@Spark}
2023-04-26 12:51:43,134 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@10278666{/executors,null,AVAILABLE,@Spark}
2023-04-26 12:51:43,136 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@6512945d{/executors/json,null,AVAILABLE,@Spark}
2023-04-26 12:51:43,139 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@217364fa{/executors/threadDump,null,AVAILABLE,@Spark}
2023-04-26 12:51:43,141 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@a358fa1{/executors/threadDump/json,null,AVAILABLE,@Spark}
2023-04-26 12:51:43,153 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@268d2c37{/static,null,AVAILABLE,@Spark}
2023-04-26 12:51:43,155 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@7a745e17{/,null,AVAILABLE,@Spark}
2023-04-26 12:51:43,180 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@29061e5f{/api,null,AVAILABLE,@Spark}
2023-04-26 12:51:43,182 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@294c8e62{/jobs/job/kill,null,AVAILABLE,@Spark}
2023-04-26 12:51:43,183 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@4ae14132{/stages/stage/kill,null,AVAILABLE,@Spark}
2023-04-26 12:51:43,187 INFO org.apache.spark.ui.SparkUI [restartedMain] Bound SparkUI to 0.0.0.0, and started at http://192.168.1.125:4040
2023-04-26 12:51:43,341 INFO org.apache.spark.executor.Executor [restartedMain] Starting executor ID driver on host localhost
2023-04-26 12:51:43,376 INFO org.apache.spark.util.Utils [restartedMain] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40861.
2023-04-26 12:51:43,378 INFO org.apache.spark.network.netty.NettyBlockTransferService [restartedMain] Server created on 192.168.1.125:40861
2023-04-26 12:51:43,382 INFO org.apache.spark.storage.BlockManager [restartedMain] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2023-04-26 12:51:43,429 INFO org.apache.spark.storage.BlockManagerMaster [restartedMain] Registering BlockManager BlockManagerId(driver, 192.168.1.125, 40861, None)
2023-04-26 12:51:43,442 INFO org.apache.spark.storage.BlockManagerMasterEndpoint [dispatcher-event-loop-2] Registering block manager 192.168.1.125:40861 with 998.4 MB RAM, BlockManagerId(driver, 192.168.1.125, 40861, None)
2023-04-26 12:51:43,448 INFO org.apache.spark.storage.BlockManagerMaster [restartedMain] Registered BlockManager BlockManagerId(driver, 192.168.1.125, 40861, None)
2023-04-26 12:51:43,450 INFO org.apache.spark.storage.BlockManager [restartedMain] Initialized BlockManager: BlockManagerId(driver, 192.168.1.125, 40861, None)
2023-04-26 12:51:43,472 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@6124183{/metrics/json,null,AVAILABLE,@Spark}
2023-04-26 12:51:48,116 INFO org.apache.kafka.clients.consumer.ConsumerConfig [restartedMain] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-book-group-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = book-group
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2023-04-26 12:51:48,303 INFO org.apache.kafka.common.utils.AppInfoParser [restartedMain] Kafka version: 3.1.1
2023-04-26 12:51:48,305 INFO org.apache.kafka.common.utils.AppInfoParser [restartedMain] Kafka commitId: 97671528ba54a138
2023-04-26 12:51:48,305 INFO org.apache.kafka.common.utils.AppInfoParser [restartedMain] Kafka startTimeMs: 1682493708300
2023-04-26 12:51:48,313 INFO org.apache.kafka.clients.consumer.KafkaConsumer [restartedMain] [Consumer clientId=consumer-book-group-1, groupId=book-group] Subscribed to topic(s): my-topic
2023-04-26 12:51:48,408 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] Started SpringBootSecurityJwtMongodbApplication in 18.321 seconds (JVM running for 20.07)
2023-04-26 12:51:48,418 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] This is a info message
2023-04-26 12:51:48,418 WARN com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] This is a warn message
2023-04-26 12:51:51,108 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-1, groupId=book-group] Resetting the last seen epoch of partition my-topic-0 to 0 since the associated topicId changed from null to QqwJ2XdJQbi2BDXQbzT51g
2023-04-26 12:51:51,113 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-1, groupId=book-group] Cluster ID: 6CnV1ouTQjGsP-r9Rd2WRA
2023-04-26 12:51:52,199 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-1, groupId=book-group] Discovered group coordinator wks-012:9092 (id: 2147483647 rack: null)
2023-04-26 12:51:52,203 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-1, groupId=book-group] (Re-)joining group
2023-04-26 12:51:53,377 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-1, groupId=book-group] Request joining group due to: need to re-join with the given member-id
2023-04-26 12:51:53,377 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-1, groupId=book-group] (Re-)joining group
2023-04-26 12:51:54,614 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-1, groupId=book-group] Successfully joined group with generation Generation{generationId=171, memberId='consumer-book-group-1-b9b71725-a26f-48ab-a962-9dcb629f29ec', protocol='range'}
2023-04-26 12:51:54,618 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-1, groupId=book-group] Finished assignment for group at generation 171: {consumer-book-group-1-b9b71725-a26f-48ab-a962-9dcb629f29ec=Assignment(partitions=[my-topic-0])}
2023-04-26 12:51:55,816 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-1, groupId=book-group] Successfully synced group in generation Generation{generationId=171, memberId='consumer-book-group-1-b9b71725-a26f-48ab-a962-9dcb629f29ec', protocol='range'}
2023-04-26 12:51:55,817 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-1, groupId=book-group] Notifying assignor about the new Assignment(partitions=[my-topic-0])
2023-04-26 12:51:55,823 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-1, groupId=book-group] Adding newly assigned partitions: my-topic-0
2023-04-26 12:51:55,982 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-1, groupId=book-group] Setting offset for partition my-topic-0 to the committed offset FetchPosition{offset=417, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[wks-012:9092 (id: 0 rack: null)], epoch=0}}
2023-04-26 12:52:17,651 INFO org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] [http-nio-8080-exec-1] Initializing Spring DispatcherServlet 'dispatcherServlet'
2023-04-26 12:52:20,518 INFO org.springdoc.api.AbstractOpenApiResource [http-nio-8080-exec-9] Init duration for springdoc-openapi is: 755 ms
2023-04-26 12:53:51,802 ERROR com.example.spring.jwt.mongodb.security.jwt.JwtUtils [http-nio-8080-exec-4] JWT token is expired: JWT expired at 2023-04-26T12:39:53Z. Current time: 2023-04-26T12:53:51Z, a difference of 838801 milliseconds.  Allowed clock skew: 0 milliseconds.
2023-04-26 12:53:51,803 ERROR com.example.spring.jwt.mongodb.security.jwt.AuthEntryPointJwt [http-nio-8080-exec-4] Unauthorized error: Full authentication is required to access this resource
2023-04-26 12:54:58,856 ERROR com.example.spring.jwt.mongodb.security.jwt.JwtUtils [http-nio-8080-exec-8] JWT token is expired: JWT expired at 2023-04-26T12:39:53Z. Current time: 2023-04-26T12:54:58Z, a difference of 905856 milliseconds.  Allowed clock skew: 0 milliseconds.
2023-04-26 12:54:58,858 ERROR com.example.spring.jwt.mongodb.security.jwt.AuthEntryPointJwt [http-nio-8080-exec-8] Unauthorized error: Full authentication is required to access this resource
2023-04-26 12:55:30,424 ERROR com.example.spring.jwt.mongodb.security.jwt.JwtUtils [http-nio-8080-exec-7] JWT token is expired: JWT expired at 2023-04-26T12:39:53Z. Current time: 2023-04-26T12:55:30Z, a difference of 937424 milliseconds.  Allowed clock skew: 0 milliseconds.
2023-04-26 12:55:30,858 INFO org.mongodb.driver.connection [http-nio-8080-exec-7] Opened connection [connectionId{localValue:3, serverValue:88}] to localhost:27017
2023-04-26 12:56:35,549 INFO org.apache.catalina.core.StandardService [Thread-4] Stopping service [Tomcat]
2023-04-26 12:56:35,551 INFO org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] [Thread-4] Destroying Spring FrameworkServlet 'dispatcherServlet'
2023-04-26 12:56:35,566 WARN org.apache.catalina.loader.WebappClassLoaderBase [Thread-4] The web application [ROOT] appears to have started a thread named [BufferPoolPruner-1-thread-1] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/jdk.internal.misc.Unsafe.park(Native Method)
 java.base@17.0.6/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:252)
 java.base@17.0.6/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1672)
 java.base@17.0.6/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182)
 java.base@17.0.6/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1062)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1122)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
 java.base@17.0.6/java.lang.Thread.run(Thread.java:833)
2023-04-26 12:56:35,570 WARN org.apache.catalina.loader.WebappClassLoaderBase [Thread-4] The web application [ROOT] appears to have started a thread named [cluster-ClusterId{value='6448d0feb2e73c199341b595', description='null'}-localhost:27017] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/jdk.internal.misc.Unsafe.park(Native Method)
 java.base@17.0.6/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:252)
 java.base@17.0.6/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1672)
 app//com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.waitForSignalOrTimeout(DefaultServerMonitor.java:300)
 app//com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.waitForNext(DefaultServerMonitor.java:281)
 app//com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:179)
 java.base@17.0.6/java.lang.Thread.run(Thread.java:833)
2023-04-26 12:56:35,573 WARN org.apache.catalina.loader.WebappClassLoaderBase [Thread-4] The web application [ROOT] appears to have started a thread named [cluster-rtt-ClusterId{value='6448d0feb2e73c199341b595', description='null'}-localhost:27017] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/java.lang.Thread.sleep(Native Method)
 app//com.mongodb.internal.connection.DefaultServerMonitor.waitForNext(DefaultServerMonitor.java:443)
 app//com.mongodb.internal.connection.DefaultServerMonitor.access$1500(DefaultServerMonitor.java:64)
 app//com.mongodb.internal.connection.DefaultServerMonitor$RoundTripTimeRunnable.run(DefaultServerMonitor.java:415)
 java.base@17.0.6/java.lang.Thread.run(Thread.java:833)
2023-04-26 12:56:35,575 WARN org.apache.catalina.loader.WebappClassLoaderBase [Thread-4] The web application [ROOT] appears to have started a thread named [MaintenanceTimer-2-thread-1] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/jdk.internal.misc.Unsafe.park(Native Method)
 java.base@17.0.6/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:252)
 java.base@17.0.6/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1672)
 java.base@17.0.6/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182)
 java.base@17.0.6/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1062)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1122)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
 java.base@17.0.6/java.lang.Thread.run(Thread.java:833)
2023-04-26 12:56:35,585 WARN org.apache.catalina.loader.WebappClassLoaderBase [Thread-4] The web application [ROOT] appears to have started a thread named [java-sdk-http-connection-reaper] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/java.lang.Thread.sleep(Native Method)
 app//com.amazonaws.http.IdleConnectionReaper.run(IdleConnectionReaper.java:188)
2023-04-26 12:56:35,606 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-1, groupId=book-group] Revoke previously assigned partitions my-topic-0
2023-04-26 12:56:35,608 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-1, groupId=book-group] Member consumer-book-group-1-b9b71725-a26f-48ab-a962-9dcb629f29ec sending LeaveGroup request to coordinator wks-012:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2023-04-26 12:56:35,609 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-1, groupId=book-group] Resetting generation due to: consumer pro-actively leaving the group
2023-04-26 12:56:35,609 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-1, groupId=book-group] Request joining group due to: consumer pro-actively leaving the group
2023-04-26 12:56:35,609 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-1, groupId=book-group] Unsubscribed all topics or patterns and assigned partitions
2023-04-26 12:56:35,610 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-1, groupId=book-group] Resetting generation due to: consumer pro-actively leaving the group
2023-04-26 12:56:35,611 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-1, groupId=book-group] Request joining group due to: consumer pro-actively leaving the group
2023-04-26 12:56:36,186 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Metrics scheduler closed
2023-04-26 12:56:36,188 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-04-26 12:56:36,189 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Metrics reporters closed
2023-04-26 12:56:36,215 INFO org.apache.kafka.common.utils.AppInfoParser [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] App info kafka.consumer for consumer-book-group-1 unregistered
2023-04-26 12:56:36,328 INFO org.spark_project.jetty.server.AbstractConnector [Thread-4] Stopped Spark@7cd22b29{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2023-04-26 12:56:36,345 INFO org.apache.spark.ui.SparkUI [Thread-4] Stopped Spark web UI at http://192.168.1.125:4040
2023-04-26 12:56:36,461 INFO org.apache.spark.MapOutputTrackerMasterEndpoint [dispatcher-event-loop-2] MapOutputTrackerMasterEndpoint stopped!
2023-04-26 12:56:36,629 INFO org.apache.spark.storage.memory.MemoryStore [Thread-4] MemoryStore cleared
2023-04-26 12:56:36,631 INFO org.apache.spark.storage.BlockManager [Thread-4] BlockManager stopped
2023-04-26 12:56:36,633 INFO org.apache.spark.storage.BlockManagerMaster [Thread-4] BlockManagerMaster stopped
2023-04-26 12:56:36,639 INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [dispatcher-event-loop-3] OutputCommitCoordinator stopped!
2023-04-26 12:56:36,660 INFO org.apache.spark.SparkContext [Thread-4] Successfully stopped SparkContext
2023-04-26 12:56:36,662 INFO org.apache.spark.SparkContext [Thread-4] SparkContext already stopped.
2023-04-26 12:56:36,674 INFO com.zaxxer.hikari.HikariDataSource [Thread-4] HikariPool-1 - Shutdown initiated...
2023-04-26 12:56:36,681 INFO com.zaxxer.hikari.HikariDataSource [Thread-4] HikariPool-1 - Shutdown completed.
2023-04-26 12:56:37,582 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] Starting SpringBootSecurityJwtMongodbApplication using Java 17.0.6 on wks-012 with PID 109060 (/home/inferyx/git/SpringApplicationWithSecurity/target/classes started by inferyx in /home/inferyx/git/SpringApplicationWithSecurity)
2023-04-26 12:56:37,583 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] No active profile set, falling back to 1 default profile: "default"
2023-04-26 12:56:38,549 INFO org.apache.catalina.core.StandardService [restartedMain] Starting service [Tomcat]
2023-04-26 12:56:38,550 INFO org.apache.catalina.core.StandardEngine [restartedMain] Starting Servlet engine: [Apache Tomcat/9.0.65]
2023-04-26 12:56:38,605 INFO org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] [restartedMain] Initializing Spring embedded WebApplicationContext
2023-04-26 12:56:38,668 INFO org.mongodb.driver.client [restartedMain] MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync|spring-boot", "version": "4.6.1"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "5.15.0-69-generic"}, "platform": "Java/Eclipse Adoptium/17.0.6+10"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.Jep395RecordCodecProvider@22d7de23]}, clusterSettings={hosts=[localhost:27017], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='30000 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, sendBufferSize=0}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, sendBufferSize=0}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=JAVA_LEGACY, serverApi=null, autoEncryptionSettings=null, contextProvider=null}
2023-04-26 12:56:38,676 INFO org.mongodb.driver.connection [cluster-ClusterId{value='6448d22eb2e73c199341b596', description='null'}-localhost:27017] Opened connection [connectionId{localValue:4, serverValue:89}] to localhost:27017
2023-04-26 12:56:38,677 INFO org.mongodb.driver.cluster [cluster-ClusterId{value='6448d22eb2e73c199341b596', description='null'}-localhost:27017] Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=6, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=8262463}
2023-04-26 12:56:38,677 INFO org.mongodb.driver.connection [cluster-rtt-ClusterId{value='6448d22eb2e73c199341b596', description='null'}-localhost:27017] Opened connection [connectionId{localValue:5, serverValue:90}] to localhost:27017
2023-04-26 12:56:38,935 INFO org.hibernate.jpa.internal.util.LogHelper [restartedMain] HHH000204: Processing PersistenceUnitInfo [name: default]
2023-04-26 12:56:38,961 INFO com.zaxxer.hikari.HikariDataSource [restartedMain] HikariPool-2 - Starting...
2023-04-26 12:56:38,968 INFO com.zaxxer.hikari.HikariDataSource [restartedMain] HikariPool-2 - Start completed.
2023-04-26 12:56:38,969 INFO org.hibernate.dialect.Dialect [restartedMain] HHH000400: Using dialect: org.hibernate.dialect.MySQL5Dialect
2023-04-26 12:56:39,636 INFO org.hibernate.engine.transaction.jta.platform.internal.JtaPlatformInitiator [restartedMain] HHH000490: Using JtaPlatform implementation: [org.hibernate.engine.transaction.jta.platform.internal.NoJtaPlatform]
2023-04-26 12:56:39,934 INFO org.apache.spark.SparkContext [restartedMain] Running Spark version 2.4.5
2023-04-26 12:56:39,935 INFO org.apache.spark.SparkContext [restartedMain] Submitted application: MyAppName
2023-04-26 12:56:39,938 INFO org.apache.spark.SecurityManager [restartedMain] Changing view acls to: inferyx
2023-04-26 12:56:39,938 INFO org.apache.spark.SecurityManager [restartedMain] Changing modify acls to: inferyx
2023-04-26 12:56:39,939 INFO org.apache.spark.SecurityManager [restartedMain] Changing view acls groups to: 
2023-04-26 12:56:39,939 INFO org.apache.spark.SecurityManager [restartedMain] Changing modify acls groups to: 
2023-04-26 12:56:39,939 INFO org.apache.spark.SecurityManager [restartedMain] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(inferyx); groups with view permissions: Set(); users  with modify permissions: Set(inferyx); groups with modify permissions: Set()
2023-04-26 12:56:40,011 INFO org.apache.spark.util.Utils [restartedMain] Successfully started service 'sparkDriver' on port 45577.
2023-04-26 12:56:40,017 INFO org.apache.spark.SparkEnv [restartedMain] Registering MapOutputTracker
2023-04-26 12:56:40,019 INFO org.apache.spark.SparkEnv [restartedMain] Registering BlockManagerMaster
2023-04-26 12:56:40,019 INFO org.apache.spark.storage.BlockManagerMasterEndpoint [restartedMain] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2023-04-26 12:56:40,020 INFO org.apache.spark.storage.BlockManagerMasterEndpoint [restartedMain] BlockManagerMasterEndpoint up
2023-04-26 12:56:40,021 INFO org.apache.spark.storage.DiskBlockManager [restartedMain] Created local directory at /tmp/blockmgr-23c7ce1e-c9d2-4039-a41a-d4c755127a35
2023-04-26 12:56:40,022 INFO org.apache.spark.storage.memory.MemoryStore [restartedMain] MemoryStore started with capacity 998.4 MB
2023-04-26 12:56:40,025 INFO org.apache.spark.SparkEnv [restartedMain] Registering OutputCommitCoordinator
2023-04-26 12:56:40,040 INFO org.spark_project.jetty.server.Server [restartedMain] jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2023-04-26 12:56:40,044 INFO org.spark_project.jetty.server.Server [restartedMain] Started @311706ms
2023-04-26 12:56:40,046 INFO org.spark_project.jetty.server.AbstractConnector [restartedMain] Started ServerConnector@6e33aeda{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2023-04-26 12:56:40,046 INFO org.apache.spark.util.Utils [restartedMain] Successfully started service 'SparkUI' on port 4040.
2023-04-26 12:56:40,047 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@53e2ef2a{/jobs,null,AVAILABLE,@Spark}
2023-04-26 12:56:40,048 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@227462ef{/jobs/json,null,AVAILABLE,@Spark}
2023-04-26 12:56:40,049 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@66e06681{/jobs/job,null,AVAILABLE,@Spark}
2023-04-26 12:56:40,050 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@15d641f9{/jobs/job/json,null,AVAILABLE,@Spark}
2023-04-26 12:56:40,051 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@510977d6{/stages,null,AVAILABLE,@Spark}
2023-04-26 12:56:40,052 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@506b1402{/stages/json,null,AVAILABLE,@Spark}
2023-04-26 12:56:40,053 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@355ab4e0{/stages/stage,null,AVAILABLE,@Spark}
2023-04-26 12:56:40,054 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@12a4e23b{/stages/stage/json,null,AVAILABLE,@Spark}
2023-04-26 12:56:40,055 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@889907e{/stages/pool,null,AVAILABLE,@Spark}
2023-04-26 12:56:40,056 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@ca4e69{/stages/pool/json,null,AVAILABLE,@Spark}
2023-04-26 12:56:40,057 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@60d70724{/storage,null,AVAILABLE,@Spark}
2023-04-26 12:56:40,058 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@5557943f{/storage/json,null,AVAILABLE,@Spark}
2023-04-26 12:56:40,058 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@1199e48b{/storage/rdd,null,AVAILABLE,@Spark}
2023-04-26 12:56:40,059 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@437259d7{/storage/rdd/json,null,AVAILABLE,@Spark}
2023-04-26 12:56:40,061 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@1bc498ba{/environment,null,AVAILABLE,@Spark}
2023-04-26 12:56:40,062 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@725b0191{/environment/json,null,AVAILABLE,@Spark}
2023-04-26 12:56:40,063 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@2c380eed{/executors,null,AVAILABLE,@Spark}
2023-04-26 12:56:40,064 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@7d55465c{/executors/json,null,AVAILABLE,@Spark}
2023-04-26 12:56:40,065 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@426b0004{/executors/threadDump,null,AVAILABLE,@Spark}
2023-04-26 12:56:40,066 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@6896d651{/executors/threadDump/json,null,AVAILABLE,@Spark}
2023-04-26 12:56:40,067 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@4b5e2c8{/static,null,AVAILABLE,@Spark}
2023-04-26 12:56:40,068 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@530533a4{/,null,AVAILABLE,@Spark}
2023-04-26 12:56:40,069 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@497472e0{/api,null,AVAILABLE,@Spark}
2023-04-26 12:56:40,070 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@3c86024a{/jobs/job/kill,null,AVAILABLE,@Spark}
2023-04-26 12:56:40,071 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@46648e6d{/stages/stage/kill,null,AVAILABLE,@Spark}
2023-04-26 12:56:40,071 INFO org.apache.spark.ui.SparkUI [restartedMain] Bound SparkUI to 0.0.0.0, and started at http://192.168.1.125:4040
2023-04-26 12:56:40,106 INFO org.apache.spark.executor.Executor [restartedMain] Starting executor ID driver on host localhost
2023-04-26 12:56:40,111 INFO org.apache.spark.util.Utils [restartedMain] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46233.
2023-04-26 12:56:40,112 INFO org.apache.spark.network.netty.NettyBlockTransferService [restartedMain] Server created on 192.168.1.125:46233
2023-04-26 12:56:40,113 INFO org.apache.spark.storage.BlockManager [restartedMain] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2023-04-26 12:56:40,113 INFO org.apache.spark.storage.BlockManagerMaster [restartedMain] Registering BlockManager BlockManagerId(driver, 192.168.1.125, 46233, None)
2023-04-26 12:56:40,115 INFO org.apache.spark.storage.BlockManagerMasterEndpoint [dispatcher-event-loop-2] Registering block manager 192.168.1.125:46233 with 998.4 MB RAM, BlockManagerId(driver, 192.168.1.125, 46233, None)
2023-04-26 12:56:40,115 INFO org.apache.spark.storage.BlockManagerMaster [restartedMain] Registered BlockManager BlockManagerId(driver, 192.168.1.125, 46233, None)
2023-04-26 12:56:40,116 INFO org.apache.spark.storage.BlockManager [restartedMain] Initialized BlockManager: BlockManagerId(driver, 192.168.1.125, 46233, None)
2023-04-26 12:56:40,118 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@66f57998{/metrics/json,null,AVAILABLE,@Spark}
2023-04-26 12:56:45,662 INFO org.apache.kafka.clients.consumer.ConsumerConfig [restartedMain] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-book-group-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = book-group
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2023-04-26 12:56:45,672 INFO org.apache.kafka.common.utils.AppInfoParser [restartedMain] Kafka version: 3.1.1
2023-04-26 12:56:45,672 INFO org.apache.kafka.common.utils.AppInfoParser [restartedMain] Kafka commitId: 97671528ba54a138
2023-04-26 12:56:45,672 INFO org.apache.kafka.common.utils.AppInfoParser [restartedMain] Kafka startTimeMs: 1682494005672
2023-04-26 12:56:45,673 INFO org.apache.kafka.clients.consumer.KafkaConsumer [restartedMain] [Consumer clientId=consumer-book-group-2, groupId=book-group] Subscribed to topic(s): my-topic
2023-04-26 12:56:45,691 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] Started SpringBootSecurityJwtMongodbApplication in 8.196 seconds (JVM running for 317.352)
2023-04-26 12:56:45,695 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] This is a info message
2023-04-26 12:56:45,696 WARN com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] This is a warn message
2023-04-26 12:56:45,736 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-2, groupId=book-group] Resetting the last seen epoch of partition my-topic-0 to 0 since the associated topicId changed from null to QqwJ2XdJQbi2BDXQbzT51g
2023-04-26 12:56:45,737 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-2, groupId=book-group] Cluster ID: 6CnV1ouTQjGsP-r9Rd2WRA
2023-04-26 12:56:45,737 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-2, groupId=book-group] Discovered group coordinator wks-012:9092 (id: 2147483647 rack: null)
2023-04-26 12:56:45,738 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-2, groupId=book-group] (Re-)joining group
2023-04-26 12:56:45,868 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-2, groupId=book-group] Request joining group due to: need to re-join with the given member-id
2023-04-26 12:56:45,869 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-2, groupId=book-group] (Re-)joining group
2023-04-26 12:56:46,003 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-2, groupId=book-group] Successfully joined group with generation Generation{generationId=173, memberId='consumer-book-group-2-94677559-7716-468a-8853-2487fe95f7b3', protocol='range'}
2023-04-26 12:56:46,003 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-2, groupId=book-group] Finished assignment for group at generation 173: {consumer-book-group-2-94677559-7716-468a-8853-2487fe95f7b3=Assignment(partitions=[my-topic-0])}
2023-04-26 12:56:46,024 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-2, groupId=book-group] Successfully synced group in generation Generation{generationId=173, memberId='consumer-book-group-2-94677559-7716-468a-8853-2487fe95f7b3', protocol='range'}
2023-04-26 12:56:46,025 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-2, groupId=book-group] Notifying assignor about the new Assignment(partitions=[my-topic-0])
2023-04-26 12:56:46,026 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-2, groupId=book-group] Adding newly assigned partitions: my-topic-0
2023-04-26 12:56:46,030 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-2, groupId=book-group] Setting offset for partition my-topic-0 to the committed offset FetchPosition{offset=417, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[wks-012:9092 (id: 0 rack: null)], epoch=0}}
2023-04-26 13:03:19,677 INFO org.apache.catalina.core.StandardService [Thread-21] Stopping service [Tomcat]
2023-04-26 13:03:19,680 WARN org.apache.catalina.loader.WebappClassLoaderBase [Thread-21] The web application [ROOT] appears to have started a thread named [cluster-ClusterId{value='6448d22eb2e73c199341b596', description='null'}-localhost:27017] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/jdk.internal.misc.Unsafe.park(Native Method)
 java.base@17.0.6/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:252)
 java.base@17.0.6/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1672)
 app//com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.waitForSignalOrTimeout(DefaultServerMonitor.java:300)
 app//com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.waitForNext(DefaultServerMonitor.java:281)
 app//com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:179)
 java.base@17.0.6/java.lang.Thread.run(Thread.java:833)
2023-04-26 13:03:19,682 WARN org.apache.catalina.loader.WebappClassLoaderBase [Thread-21] The web application [ROOT] appears to have started a thread named [cluster-rtt-ClusterId{value='6448d22eb2e73c199341b596', description='null'}-localhost:27017] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/java.lang.Thread.sleep(Native Method)
 app//com.mongodb.internal.connection.DefaultServerMonitor.waitForNext(DefaultServerMonitor.java:443)
 app//com.mongodb.internal.connection.DefaultServerMonitor.access$1500(DefaultServerMonitor.java:64)
 app//com.mongodb.internal.connection.DefaultServerMonitor$RoundTripTimeRunnable.run(DefaultServerMonitor.java:415)
 java.base@17.0.6/java.lang.Thread.run(Thread.java:833)
2023-04-26 13:03:19,685 WARN org.apache.catalina.loader.WebappClassLoaderBase [Thread-21] The web application [ROOT] appears to have started a thread named [MaintenanceTimer-3-thread-1] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/jdk.internal.misc.Unsafe.park(Native Method)
 java.base@17.0.6/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:252)
 java.base@17.0.6/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1672)
 java.base@17.0.6/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182)
 java.base@17.0.6/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1062)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1122)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
 java.base@17.0.6/java.lang.Thread.run(Thread.java:833)
2023-04-26 13:03:19,688 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-2, groupId=book-group] Revoke previously assigned partitions my-topic-0
2023-04-26 13:03:19,688 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-2, groupId=book-group] Member consumer-book-group-2-94677559-7716-468a-8853-2487fe95f7b3 sending LeaveGroup request to coordinator wks-012:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2023-04-26 13:03:19,693 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-2, groupId=book-group] Resetting generation due to: consumer pro-actively leaving the group
2023-04-26 13:03:19,697 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-2, groupId=book-group] Request joining group due to: consumer pro-actively leaving the group
2023-04-26 13:03:19,697 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-2, groupId=book-group] Unsubscribed all topics or patterns and assigned partitions
2023-04-26 13:03:19,698 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-2, groupId=book-group] Resetting generation due to: consumer pro-actively leaving the group
2023-04-26 13:03:19,698 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-2, groupId=book-group] Request joining group due to: consumer pro-actively leaving the group
2023-04-26 13:03:19,780 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Metrics scheduler closed
2023-04-26 13:03:19,781 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-04-26 13:03:19,781 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Metrics reporters closed
2023-04-26 13:03:19,787 INFO org.apache.kafka.common.utils.AppInfoParser [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] App info kafka.consumer for consumer-book-group-2 unregistered
2023-04-26 13:03:19,794 INFO org.spark_project.jetty.server.AbstractConnector [Thread-21] Stopped Spark@6e33aeda{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2023-04-26 13:03:19,796 INFO org.apache.spark.ui.SparkUI [Thread-21] Stopped Spark web UI at http://192.168.1.125:4040
2023-04-26 13:03:19,800 INFO org.apache.spark.MapOutputTrackerMasterEndpoint [dispatcher-event-loop-2] MapOutputTrackerMasterEndpoint stopped!
2023-04-26 13:03:19,823 INFO org.apache.spark.storage.memory.MemoryStore [Thread-21] MemoryStore cleared
2023-04-26 13:03:19,823 INFO org.apache.spark.storage.BlockManager [Thread-21] BlockManager stopped
2023-04-26 13:03:19,824 INFO org.apache.spark.storage.BlockManagerMaster [Thread-21] BlockManagerMaster stopped
2023-04-26 13:03:19,824 INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [dispatcher-event-loop-3] OutputCommitCoordinator stopped!
2023-04-26 13:03:19,847 INFO org.apache.spark.SparkContext [Thread-21] Successfully stopped SparkContext
2023-04-26 13:03:19,848 INFO org.apache.spark.SparkContext [Thread-21] SparkContext already stopped.
2023-04-26 13:03:19,849 INFO com.zaxxer.hikari.HikariDataSource [Thread-21] HikariPool-2 - Shutdown initiated...
2023-04-26 13:03:19,852 INFO com.zaxxer.hikari.HikariDataSource [Thread-21] HikariPool-2 - Shutdown completed.
2023-04-26 13:03:20,184 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] Starting SpringBootSecurityJwtMongodbApplication using Java 17.0.6 on wks-012 with PID 109060 (/home/inferyx/git/SpringApplicationWithSecurity/target/classes started by inferyx in /home/inferyx/git/SpringApplicationWithSecurity)
2023-04-26 13:03:20,185 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] No active profile set, falling back to 1 default profile: "default"
2023-04-26 13:03:21,138 INFO org.apache.catalina.core.StandardService [restartedMain] Starting service [Tomcat]
2023-04-26 13:03:21,157 INFO org.apache.catalina.core.StandardEngine [restartedMain] Starting Servlet engine: [Apache Tomcat/9.0.65]
2023-04-26 13:03:21,237 INFO org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] [restartedMain] Initializing Spring embedded WebApplicationContext
2023-04-26 13:03:21,337 INFO org.mongodb.driver.client [restartedMain] MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync|spring-boot", "version": "4.6.1"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "5.15.0-69-generic"}, "platform": "Java/Eclipse Adoptium/17.0.6+10"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.Jep395RecordCodecProvider@22d7de23]}, clusterSettings={hosts=[localhost:27017], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='30000 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, sendBufferSize=0}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, sendBufferSize=0}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=JAVA_LEGACY, serverApi=null, autoEncryptionSettings=null, contextProvider=null}
2023-04-26 13:03:21,341 INFO org.mongodb.driver.connection [cluster-ClusterId{value='6448d3c1b2e73c199341b597', description='null'}-localhost:27017] Opened connection [connectionId{localValue:6, serverValue:91}] to localhost:27017
2023-04-26 13:03:21,341 INFO org.mongodb.driver.cluster [cluster-ClusterId{value='6448d3c1b2e73c199341b597', description='null'}-localhost:27017] Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=6, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1177894}
2023-04-26 13:03:21,344 INFO org.mongodb.driver.connection [cluster-rtt-ClusterId{value='6448d3c1b2e73c199341b597', description='null'}-localhost:27017] Opened connection [connectionId{localValue:7, serverValue:92}] to localhost:27017
2023-04-26 13:03:21,623 INFO org.hibernate.jpa.internal.util.LogHelper [restartedMain] HHH000204: Processing PersistenceUnitInfo [name: default]
2023-04-26 13:03:21,650 INFO com.zaxxer.hikari.HikariDataSource [restartedMain] HikariPool-3 - Starting...
2023-04-26 13:03:21,658 INFO com.zaxxer.hikari.HikariDataSource [restartedMain] HikariPool-3 - Start completed.
2023-04-26 13:03:21,659 INFO org.hibernate.dialect.Dialect [restartedMain] HHH000400: Using dialect: org.hibernate.dialect.MySQL5Dialect
2023-04-26 13:03:21,838 INFO org.hibernate.engine.transaction.jta.platform.internal.JtaPlatformInitiator [restartedMain] HHH000490: Using JtaPlatform implementation: [org.hibernate.engine.transaction.jta.platform.internal.NoJtaPlatform]
2023-04-26 13:03:22,224 INFO org.apache.spark.SparkContext [restartedMain] Running Spark version 2.4.5
2023-04-26 13:03:22,226 INFO org.apache.spark.SparkContext [restartedMain] Submitted application: MyAppName
2023-04-26 13:03:22,228 INFO org.apache.spark.SecurityManager [restartedMain] Changing view acls to: inferyx
2023-04-26 13:03:22,228 INFO org.apache.spark.SecurityManager [restartedMain] Changing modify acls to: inferyx
2023-04-26 13:03:22,229 INFO org.apache.spark.SecurityManager [restartedMain] Changing view acls groups to: 
2023-04-26 13:03:22,229 INFO org.apache.spark.SecurityManager [restartedMain] Changing modify acls groups to: 
2023-04-26 13:03:22,229 INFO org.apache.spark.SecurityManager [restartedMain] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(inferyx); groups with view permissions: Set(); users  with modify permissions: Set(inferyx); groups with modify permissions: Set()
2023-04-26 13:03:22,316 INFO org.apache.spark.util.Utils [restartedMain] Successfully started service 'sparkDriver' on port 41657.
2023-04-26 13:03:22,321 INFO org.apache.spark.SparkEnv [restartedMain] Registering MapOutputTracker
2023-04-26 13:03:22,323 INFO org.apache.spark.SparkEnv [restartedMain] Registering BlockManagerMaster
2023-04-26 13:03:22,324 INFO org.apache.spark.storage.BlockManagerMasterEndpoint [restartedMain] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2023-04-26 13:03:22,324 INFO org.apache.spark.storage.BlockManagerMasterEndpoint [restartedMain] BlockManagerMasterEndpoint up
2023-04-26 13:03:22,326 INFO org.apache.spark.storage.DiskBlockManager [restartedMain] Created local directory at /tmp/blockmgr-d4c6d0cb-9e9f-4e26-a274-80682595423a
2023-04-26 13:03:22,327 INFO org.apache.spark.storage.memory.MemoryStore [restartedMain] MemoryStore started with capacity 998.4 MB
2023-04-26 13:03:22,329 INFO org.apache.spark.SparkEnv [restartedMain] Registering OutputCommitCoordinator
2023-04-26 13:03:22,338 INFO org.spark_project.jetty.server.Server [restartedMain] jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2023-04-26 13:03:22,340 INFO org.spark_project.jetty.server.Server [restartedMain] Started @714002ms
2023-04-26 13:03:22,342 INFO org.spark_project.jetty.server.AbstractConnector [restartedMain] Started ServerConnector@7c489bd1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2023-04-26 13:03:22,343 INFO org.apache.spark.util.Utils [restartedMain] Successfully started service 'SparkUI' on port 4040.
2023-04-26 13:03:22,344 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@37ffe9a4{/jobs,null,AVAILABLE,@Spark}
2023-04-26 13:03:22,345 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@67cb83b2{/jobs/json,null,AVAILABLE,@Spark}
2023-04-26 13:03:22,347 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@21be5a19{/jobs/job,null,AVAILABLE,@Spark}
2023-04-26 13:03:22,348 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@1910cbec{/jobs/job/json,null,AVAILABLE,@Spark}
2023-04-26 13:03:22,349 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@51eaa3a3{/stages,null,AVAILABLE,@Spark}
2023-04-26 13:03:22,352 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@6a077c3f{/stages/json,null,AVAILABLE,@Spark}
2023-04-26 13:03:22,353 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@393a9df4{/stages/stage,null,AVAILABLE,@Spark}
2023-04-26 13:03:22,354 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@266a6e94{/stages/stage/json,null,AVAILABLE,@Spark}
2023-04-26 13:03:22,355 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@22193641{/stages/pool,null,AVAILABLE,@Spark}
2023-04-26 13:03:22,357 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@732e50b9{/stages/pool/json,null,AVAILABLE,@Spark}
2023-04-26 13:03:22,358 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@1e96a615{/storage,null,AVAILABLE,@Spark}
2023-04-26 13:03:22,359 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@6b7d659d{/storage/json,null,AVAILABLE,@Spark}
2023-04-26 13:03:22,359 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@3dfd8d{/storage/rdd,null,AVAILABLE,@Spark}
2023-04-26 13:03:22,360 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@117deab{/storage/rdd/json,null,AVAILABLE,@Spark}
2023-04-26 13:03:22,361 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@2a9a7692{/environment,null,AVAILABLE,@Spark}
2023-04-26 13:03:22,362 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@2a4bafd1{/environment/json,null,AVAILABLE,@Spark}
2023-04-26 13:03:22,363 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@45f5fe80{/executors,null,AVAILABLE,@Spark}
2023-04-26 13:03:22,363 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@7aa88213{/executors/json,null,AVAILABLE,@Spark}
2023-04-26 13:03:22,364 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@37f4532{/executors/threadDump,null,AVAILABLE,@Spark}
2023-04-26 13:03:22,365 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@74bd47ab{/executors/threadDump/json,null,AVAILABLE,@Spark}
2023-04-26 13:03:22,366 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@1ed6b642{/static,null,AVAILABLE,@Spark}
2023-04-26 13:03:22,367 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@20501791{/,null,AVAILABLE,@Spark}
2023-04-26 13:03:22,368 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@597f907c{/api,null,AVAILABLE,@Spark}
2023-04-26 13:03:22,369 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@6caedeef{/jobs/job/kill,null,AVAILABLE,@Spark}
2023-04-26 13:03:22,371 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@4ce2a074{/stages/stage/kill,null,AVAILABLE,@Spark}
2023-04-26 13:03:22,371 INFO org.apache.spark.ui.SparkUI [restartedMain] Bound SparkUI to 0.0.0.0, and started at http://192.168.1.125:4040
2023-04-26 13:03:22,419 INFO org.apache.spark.executor.Executor [restartedMain] Starting executor ID driver on host localhost
2023-04-26 13:03:22,427 INFO org.apache.spark.util.Utils [restartedMain] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41505.
2023-04-26 13:03:22,428 INFO org.apache.spark.network.netty.NettyBlockTransferService [restartedMain] Server created on 192.168.1.125:41505
2023-04-26 13:03:22,428 INFO org.apache.spark.storage.BlockManager [restartedMain] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2023-04-26 13:03:22,428 INFO org.apache.spark.storage.BlockManagerMaster [restartedMain] Registering BlockManager BlockManagerId(driver, 192.168.1.125, 41505, None)
2023-04-26 13:03:22,429 INFO org.apache.spark.storage.BlockManagerMasterEndpoint [dispatcher-event-loop-2] Registering block manager 192.168.1.125:41505 with 998.4 MB RAM, BlockManagerId(driver, 192.168.1.125, 41505, None)
2023-04-26 13:03:22,430 INFO org.apache.spark.storage.BlockManagerMaster [restartedMain] Registered BlockManager BlockManagerId(driver, 192.168.1.125, 41505, None)
2023-04-26 13:03:22,430 INFO org.apache.spark.storage.BlockManager [restartedMain] Initialized BlockManager: BlockManagerId(driver, 192.168.1.125, 41505, None)
2023-04-26 13:03:22,432 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@52c24612{/metrics/json,null,AVAILABLE,@Spark}
2023-04-26 13:03:24,786 INFO org.apache.kafka.clients.consumer.ConsumerConfig [restartedMain] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-book-group-3
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = book-group
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2023-04-26 13:03:24,791 INFO org.apache.kafka.common.utils.AppInfoParser [restartedMain] Kafka version: 3.1.1
2023-04-26 13:03:24,791 INFO org.apache.kafka.common.utils.AppInfoParser [restartedMain] Kafka commitId: 97671528ba54a138
2023-04-26 13:03:24,792 INFO org.apache.kafka.common.utils.AppInfoParser [restartedMain] Kafka startTimeMs: 1682494404791
2023-04-26 13:03:24,792 INFO org.apache.kafka.clients.consumer.KafkaConsumer [restartedMain] [Consumer clientId=consumer-book-group-3, groupId=book-group] Subscribed to topic(s): my-topic
2023-04-26 13:03:24,799 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-3, groupId=book-group] Resetting the last seen epoch of partition my-topic-0 to 0 since the associated topicId changed from null to QqwJ2XdJQbi2BDXQbzT51g
2023-04-26 13:03:24,799 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-3, groupId=book-group] Cluster ID: 6CnV1ouTQjGsP-r9Rd2WRA
2023-04-26 13:03:24,812 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] Started SpringBootSecurityJwtMongodbApplication in 4.722 seconds (JVM running for 716.474)
2023-04-26 13:03:24,815 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] This is a info message
2023-04-26 13:03:24,816 WARN com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] This is a warn message
2023-04-26 13:03:24,830 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-3, groupId=book-group] Discovered group coordinator wks-012:9092 (id: 2147483647 rack: null)
2023-04-26 13:03:24,831 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-3, groupId=book-group] (Re-)joining group
2023-04-26 13:03:24,835 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-3, groupId=book-group] Request joining group due to: need to re-join with the given member-id
2023-04-26 13:03:24,835 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-3, groupId=book-group] (Re-)joining group
2023-04-26 13:03:24,838 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-3, groupId=book-group] Successfully joined group with generation Generation{generationId=175, memberId='consumer-book-group-3-a48de859-588b-45be-a05d-279aff6b3de8', protocol='range'}
2023-04-26 13:03:24,839 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-3, groupId=book-group] Finished assignment for group at generation 175: {consumer-book-group-3-a48de859-588b-45be-a05d-279aff6b3de8=Assignment(partitions=[my-topic-0])}
2023-04-26 13:03:24,871 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-3, groupId=book-group] Successfully synced group in generation Generation{generationId=175, memberId='consumer-book-group-3-a48de859-588b-45be-a05d-279aff6b3de8', protocol='range'}
2023-04-26 13:03:24,871 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-3, groupId=book-group] Notifying assignor about the new Assignment(partitions=[my-topic-0])
2023-04-26 13:03:24,871 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-3, groupId=book-group] Adding newly assigned partitions: my-topic-0
2023-04-26 13:03:24,874 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-3, groupId=book-group] Setting offset for partition my-topic-0 to the committed offset FetchPosition{offset=417, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[wks-012:9092 (id: 0 rack: null)], epoch=0}}
2023-04-26 13:03:27,223 INFO org.apache.catalina.core.StandardService [Thread-38] Stopping service [Tomcat]
2023-04-26 13:03:27,227 WARN org.apache.catalina.loader.WebappClassLoaderBase [Thread-38] The web application [ROOT] appears to have started a thread named [cluster-ClusterId{value='6448d3c1b2e73c199341b597', description='null'}-localhost:27017] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/jdk.internal.misc.Unsafe.park(Native Method)
 java.base@17.0.6/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:252)
 java.base@17.0.6/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1672)
 app//com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.waitForSignalOrTimeout(DefaultServerMonitor.java:300)
 app//com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.waitForNext(DefaultServerMonitor.java:281)
 app//com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:179)
 java.base@17.0.6/java.lang.Thread.run(Thread.java:833)
2023-04-26 13:03:27,228 WARN org.apache.catalina.loader.WebappClassLoaderBase [Thread-38] The web application [ROOT] appears to have started a thread named [cluster-rtt-ClusterId{value='6448d3c1b2e73c199341b597', description='null'}-localhost:27017] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/java.lang.Thread.sleep(Native Method)
 app//com.mongodb.internal.connection.DefaultServerMonitor.waitForNext(DefaultServerMonitor.java:443)
 app//com.mongodb.internal.connection.DefaultServerMonitor.access$1500(DefaultServerMonitor.java:64)
 app//com.mongodb.internal.connection.DefaultServerMonitor$RoundTripTimeRunnable.run(DefaultServerMonitor.java:415)
 java.base@17.0.6/java.lang.Thread.run(Thread.java:833)
2023-04-26 13:03:27,229 WARN org.apache.catalina.loader.WebappClassLoaderBase [Thread-38] The web application [ROOT] appears to have started a thread named [MaintenanceTimer-4-thread-1] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/jdk.internal.misc.Unsafe.park(Native Method)
 java.base@17.0.6/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:252)
 java.base@17.0.6/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1672)
 java.base@17.0.6/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182)
 java.base@17.0.6/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1062)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1122)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
 java.base@17.0.6/java.lang.Thread.run(Thread.java:833)
2023-04-26 13:03:27,237 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-3, groupId=book-group] Revoke previously assigned partitions my-topic-0
2023-04-26 13:03:27,237 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-3, groupId=book-group] Member consumer-book-group-3-a48de859-588b-45be-a05d-279aff6b3de8 sending LeaveGroup request to coordinator wks-012:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2023-04-26 13:03:27,238 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-3, groupId=book-group] Resetting generation due to: consumer pro-actively leaving the group
2023-04-26 13:03:27,238 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-3, groupId=book-group] Request joining group due to: consumer pro-actively leaving the group
2023-04-26 13:03:27,238 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-3, groupId=book-group] Unsubscribed all topics or patterns and assigned partitions
2023-04-26 13:03:27,239 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-3, groupId=book-group] Resetting generation due to: consumer pro-actively leaving the group
2023-04-26 13:03:27,239 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-3, groupId=book-group] Request joining group due to: consumer pro-actively leaving the group
2023-04-26 13:03:27,513 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Metrics scheduler closed
2023-04-26 13:03:27,514 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-04-26 13:03:27,514 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Metrics reporters closed
2023-04-26 13:03:27,518 INFO org.apache.kafka.common.utils.AppInfoParser [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] App info kafka.consumer for consumer-book-group-3 unregistered
2023-04-26 13:03:27,524 INFO org.spark_project.jetty.server.AbstractConnector [Thread-38] Stopped Spark@7c489bd1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2023-04-26 13:03:27,525 INFO org.apache.spark.ui.SparkUI [Thread-38] Stopped Spark web UI at http://192.168.1.125:4040
2023-04-26 13:03:27,528 INFO org.apache.spark.MapOutputTrackerMasterEndpoint [dispatcher-event-loop-2] MapOutputTrackerMasterEndpoint stopped!
2023-04-26 13:03:27,588 INFO org.apache.spark.storage.memory.MemoryStore [Thread-38] MemoryStore cleared
2023-04-26 13:03:27,588 INFO org.apache.spark.storage.BlockManager [Thread-38] BlockManager stopped
2023-04-26 13:03:27,589 INFO org.apache.spark.storage.BlockManagerMaster [Thread-38] BlockManagerMaster stopped
2023-04-26 13:03:27,594 INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [dispatcher-event-loop-3] OutputCommitCoordinator stopped!
2023-04-26 13:03:27,605 INFO org.apache.spark.SparkContext [Thread-38] Successfully stopped SparkContext
2023-04-26 13:03:27,606 INFO org.apache.spark.SparkContext [Thread-38] SparkContext already stopped.
2023-04-26 13:03:27,608 INFO com.zaxxer.hikari.HikariDataSource [Thread-38] HikariPool-3 - Shutdown initiated...
2023-04-26 13:03:27,623 INFO com.zaxxer.hikari.HikariDataSource [Thread-38] HikariPool-3 - Shutdown completed.
2023-04-26 13:03:28,103 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] Starting SpringBootSecurityJwtMongodbApplication using Java 17.0.6 on wks-012 with PID 109060 (/home/inferyx/git/SpringApplicationWithSecurity/target/classes started by inferyx in /home/inferyx/git/SpringApplicationWithSecurity)
2023-04-26 13:03:28,113 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] No active profile set, falling back to 1 default profile: "default"
2023-04-26 13:03:29,217 INFO org.apache.catalina.core.StandardService [restartedMain] Starting service [Tomcat]
2023-04-26 13:03:29,217 INFO org.apache.catalina.core.StandardEngine [restartedMain] Starting Servlet engine: [Apache Tomcat/9.0.65]
2023-04-26 13:03:29,275 INFO org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] [restartedMain] Initializing Spring embedded WebApplicationContext
2023-04-26 13:03:29,324 INFO org.mongodb.driver.client [restartedMain] MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync|spring-boot", "version": "4.6.1"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "5.15.0-69-generic"}, "platform": "Java/Eclipse Adoptium/17.0.6+10"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.Jep395RecordCodecProvider@22d7de23]}, clusterSettings={hosts=[localhost:27017], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='30000 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, sendBufferSize=0}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, sendBufferSize=0}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=JAVA_LEGACY, serverApi=null, autoEncryptionSettings=null, contextProvider=null}
2023-04-26 13:03:29,477 INFO org.mongodb.driver.connection [cluster-ClusterId{value='6448d3c9b2e73c199341b598', description='null'}-localhost:27017] Opened connection [connectionId{localValue:8, serverValue:93}] to localhost:27017
2023-04-26 13:03:29,477 INFO org.mongodb.driver.connection [cluster-rtt-ClusterId{value='6448d3c9b2e73c199341b598', description='null'}-localhost:27017] Opened connection [connectionId{localValue:9, serverValue:94}] to localhost:27017
2023-04-26 13:03:29,477 INFO org.mongodb.driver.cluster [cluster-ClusterId{value='6448d3c9b2e73c199341b598', description='null'}-localhost:27017] Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=6, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=152363348}
2023-04-26 13:03:29,562 INFO org.hibernate.jpa.internal.util.LogHelper [restartedMain] HHH000204: Processing PersistenceUnitInfo [name: default]
2023-04-26 13:03:29,584 INFO com.zaxxer.hikari.HikariDataSource [restartedMain] HikariPool-4 - Starting...
2023-04-26 13:03:29,590 INFO com.zaxxer.hikari.HikariDataSource [restartedMain] HikariPool-4 - Start completed.
2023-04-26 13:03:29,591 INFO org.hibernate.dialect.Dialect [restartedMain] HHH000400: Using dialect: org.hibernate.dialect.MySQL5Dialect
2023-04-26 13:03:29,878 INFO org.hibernate.engine.transaction.jta.platform.internal.JtaPlatformInitiator [restartedMain] HHH000490: Using JtaPlatform implementation: [org.hibernate.engine.transaction.jta.platform.internal.NoJtaPlatform]
2023-04-26 13:03:30,207 INFO org.apache.spark.SparkContext [restartedMain] Running Spark version 2.4.5
2023-04-26 13:03:30,208 INFO org.apache.spark.SparkContext [restartedMain] Submitted application: MyAppName
2023-04-26 13:03:30,210 INFO org.apache.spark.SecurityManager [restartedMain] Changing view acls to: inferyx
2023-04-26 13:03:30,211 INFO org.apache.spark.SecurityManager [restartedMain] Changing modify acls to: inferyx
2023-04-26 13:03:30,211 INFO org.apache.spark.SecurityManager [restartedMain] Changing view acls groups to: 
2023-04-26 13:03:30,211 INFO org.apache.spark.SecurityManager [restartedMain] Changing modify acls groups to: 
2023-04-26 13:03:30,212 INFO org.apache.spark.SecurityManager [restartedMain] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(inferyx); groups with view permissions: Set(); users  with modify permissions: Set(inferyx); groups with modify permissions: Set()
2023-04-26 13:03:30,287 INFO org.apache.spark.util.Utils [restartedMain] Successfully started service 'sparkDriver' on port 38427.
2023-04-26 13:03:30,293 INFO org.apache.spark.SparkEnv [restartedMain] Registering MapOutputTracker
2023-04-26 13:03:30,294 INFO org.apache.spark.SparkEnv [restartedMain] Registering BlockManagerMaster
2023-04-26 13:03:30,295 INFO org.apache.spark.storage.BlockManagerMasterEndpoint [restartedMain] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2023-04-26 13:03:30,296 INFO org.apache.spark.storage.BlockManagerMasterEndpoint [restartedMain] BlockManagerMasterEndpoint up
2023-04-26 13:03:30,297 INFO org.apache.spark.storage.DiskBlockManager [restartedMain] Created local directory at /tmp/blockmgr-1173ccc1-8c51-40ee-ab3c-c35a7e139ed4
2023-04-26 13:03:30,298 INFO org.apache.spark.storage.memory.MemoryStore [restartedMain] MemoryStore started with capacity 998.4 MB
2023-04-26 13:03:30,300 INFO org.apache.spark.SparkEnv [restartedMain] Registering OutputCommitCoordinator
2023-04-26 13:03:30,308 INFO org.spark_project.jetty.server.Server [restartedMain] jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2023-04-26 13:03:30,310 INFO org.spark_project.jetty.server.Server [restartedMain] Started @721972ms
2023-04-26 13:03:30,311 INFO org.spark_project.jetty.server.AbstractConnector [restartedMain] Started ServerConnector@2c919bf6{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2023-04-26 13:03:30,312 INFO org.apache.spark.util.Utils [restartedMain] Successfully started service 'SparkUI' on port 4040.
2023-04-26 13:03:30,313 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@73e9fb03{/jobs,null,AVAILABLE,@Spark}
2023-04-26 13:03:30,314 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@20cdecd8{/jobs/json,null,AVAILABLE,@Spark}
2023-04-26 13:03:30,315 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@48067373{/jobs/job,null,AVAILABLE,@Spark}
2023-04-26 13:03:30,316 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@68e81fae{/jobs/job/json,null,AVAILABLE,@Spark}
2023-04-26 13:03:30,317 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@5465d053{/stages,null,AVAILABLE,@Spark}
2023-04-26 13:03:30,318 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@1b815d2b{/stages/json,null,AVAILABLE,@Spark}
2023-04-26 13:03:30,319 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@606728c3{/stages/stage,null,AVAILABLE,@Spark}
2023-04-26 13:03:30,320 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@48da5e0e{/stages/stage/json,null,AVAILABLE,@Spark}
2023-04-26 13:03:30,320 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@3247e80e{/stages/pool,null,AVAILABLE,@Spark}
2023-04-26 13:03:30,321 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@233e9368{/stages/pool/json,null,AVAILABLE,@Spark}
2023-04-26 13:03:30,322 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@7a4696fc{/storage,null,AVAILABLE,@Spark}
2023-04-26 13:03:30,323 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@60768d94{/storage/json,null,AVAILABLE,@Spark}
2023-04-26 13:03:30,324 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@6dfc0e0c{/storage/rdd,null,AVAILABLE,@Spark}
2023-04-26 13:03:30,326 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@3a0e7508{/storage/rdd/json,null,AVAILABLE,@Spark}
2023-04-26 13:03:30,327 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@7431f5c3{/environment,null,AVAILABLE,@Spark}
2023-04-26 13:03:30,329 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@207fdcfc{/environment/json,null,AVAILABLE,@Spark}
2023-04-26 13:03:30,330 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@2d7e2a91{/executors,null,AVAILABLE,@Spark}
2023-04-26 13:03:30,331 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@6491bc03{/executors/json,null,AVAILABLE,@Spark}
2023-04-26 13:03:30,332 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@1f75c2bf{/executors/threadDump,null,AVAILABLE,@Spark}
2023-04-26 13:03:30,333 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@41b36591{/executors/threadDump/json,null,AVAILABLE,@Spark}
2023-04-26 13:03:30,334 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@1d45e594{/static,null,AVAILABLE,@Spark}
2023-04-26 13:03:30,335 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@74dd239b{/,null,AVAILABLE,@Spark}
2023-04-26 13:03:30,336 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@450c7785{/api,null,AVAILABLE,@Spark}
2023-04-26 13:03:30,337 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@2d923260{/jobs/job/kill,null,AVAILABLE,@Spark}
2023-04-26 13:03:30,337 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@30ce1db2{/stages/stage/kill,null,AVAILABLE,@Spark}
2023-04-26 13:03:30,338 INFO org.apache.spark.ui.SparkUI [restartedMain] Bound SparkUI to 0.0.0.0, and started at http://192.168.1.125:4040
2023-04-26 13:03:30,370 INFO org.apache.spark.executor.Executor [restartedMain] Starting executor ID driver on host localhost
2023-04-26 13:03:30,376 INFO org.apache.spark.util.Utils [restartedMain] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39727.
2023-04-26 13:03:30,376 INFO org.apache.spark.network.netty.NettyBlockTransferService [restartedMain] Server created on 192.168.1.125:39727
2023-04-26 13:03:30,376 INFO org.apache.spark.storage.BlockManager [restartedMain] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2023-04-26 13:03:30,377 INFO org.apache.spark.storage.BlockManagerMaster [restartedMain] Registering BlockManager BlockManagerId(driver, 192.168.1.125, 39727, None)
2023-04-26 13:03:30,377 INFO org.apache.spark.storage.BlockManagerMasterEndpoint [dispatcher-event-loop-2] Registering block manager 192.168.1.125:39727 with 998.4 MB RAM, BlockManagerId(driver, 192.168.1.125, 39727, None)
2023-04-26 13:03:30,378 INFO org.apache.spark.storage.BlockManagerMaster [restartedMain] Registered BlockManager BlockManagerId(driver, 192.168.1.125, 39727, None)
2023-04-26 13:03:30,378 INFO org.apache.spark.storage.BlockManager [restartedMain] Initialized BlockManager: BlockManagerId(driver, 192.168.1.125, 39727, None)
2023-04-26 13:03:30,380 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@7ac5c239{/metrics/json,null,AVAILABLE,@Spark}
2023-04-26 13:03:32,444 INFO org.apache.kafka.clients.consumer.ConsumerConfig [restartedMain] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-book-group-4
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = book-group
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2023-04-26 13:03:32,449 INFO org.apache.kafka.common.utils.AppInfoParser [restartedMain] Kafka version: 3.1.1
2023-04-26 13:03:32,450 INFO org.apache.kafka.common.utils.AppInfoParser [restartedMain] Kafka commitId: 97671528ba54a138
2023-04-26 13:03:32,450 INFO org.apache.kafka.common.utils.AppInfoParser [restartedMain] Kafka startTimeMs: 1682494412449
2023-04-26 13:03:32,450 INFO org.apache.kafka.clients.consumer.KafkaConsumer [restartedMain] [Consumer clientId=consumer-book-group-4, groupId=book-group] Subscribed to topic(s): my-topic
2023-04-26 13:03:32,457 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-4, groupId=book-group] Resetting the last seen epoch of partition my-topic-0 to 0 since the associated topicId changed from null to QqwJ2XdJQbi2BDXQbzT51g
2023-04-26 13:03:32,457 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-4, groupId=book-group] Cluster ID: 6CnV1ouTQjGsP-r9Rd2WRA
2023-04-26 13:03:32,457 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-4, groupId=book-group] Discovered group coordinator wks-012:9092 (id: 2147483647 rack: null)
2023-04-26 13:03:32,461 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-4, groupId=book-group] (Re-)joining group
2023-04-26 13:03:32,466 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-4, groupId=book-group] Request joining group due to: need to re-join with the given member-id
2023-04-26 13:03:32,466 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-4, groupId=book-group] (Re-)joining group
2023-04-26 13:03:32,469 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-4, groupId=book-group] Successfully joined group with generation Generation{generationId=177, memberId='consumer-book-group-4-f68ab789-beab-4354-aa72-58f6829724c4', protocol='range'}
2023-04-26 13:03:32,469 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-4, groupId=book-group] Finished assignment for group at generation 177: {consumer-book-group-4-f68ab789-beab-4354-aa72-58f6829724c4=Assignment(partitions=[my-topic-0])}
2023-04-26 13:03:32,472 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-4, groupId=book-group] Successfully synced group in generation Generation{generationId=177, memberId='consumer-book-group-4-f68ab789-beab-4354-aa72-58f6829724c4', protocol='range'}
2023-04-26 13:03:32,473 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-4, groupId=book-group] Notifying assignor about the new Assignment(partitions=[my-topic-0])
2023-04-26 13:03:32,473 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-4, groupId=book-group] Adding newly assigned partitions: my-topic-0
2023-04-26 13:03:32,476 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] Started SpringBootSecurityJwtMongodbApplication in 4.531 seconds (JVM running for 724.138)
2023-04-26 13:03:32,481 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] This is a info message
2023-04-26 13:03:32,482 WARN com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] This is a warn message
2023-04-26 13:03:32,485 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-4, groupId=book-group] Setting offset for partition my-topic-0 to the committed offset FetchPosition{offset=417, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[wks-012:9092 (id: 0 rack: null)], epoch=0}}
2023-04-26 13:03:35,099 INFO org.apache.catalina.core.StandardService [Thread-54] Stopping service [Tomcat]
2023-04-26 13:03:35,112 WARN org.apache.catalina.loader.WebappClassLoaderBase [Thread-54] The web application [ROOT] appears to have started a thread named [cluster-ClusterId{value='6448d3c9b2e73c199341b598', description='null'}-localhost:27017] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/jdk.internal.misc.Unsafe.park(Native Method)
 java.base@17.0.6/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:252)
 java.base@17.0.6/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1672)
 app//com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.waitForSignalOrTimeout(DefaultServerMonitor.java:300)
 app//com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.waitForNext(DefaultServerMonitor.java:281)
 app//com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:179)
 java.base@17.0.6/java.lang.Thread.run(Thread.java:833)
2023-04-26 13:03:35,113 WARN org.apache.catalina.loader.WebappClassLoaderBase [Thread-54] The web application [ROOT] appears to have started a thread named [cluster-rtt-ClusterId{value='6448d3c9b2e73c199341b598', description='null'}-localhost:27017] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/java.lang.Thread.sleep(Native Method)
 app//com.mongodb.internal.connection.DefaultServerMonitor.waitForNext(DefaultServerMonitor.java:443)
 app//com.mongodb.internal.connection.DefaultServerMonitor.access$1500(DefaultServerMonitor.java:64)
 app//com.mongodb.internal.connection.DefaultServerMonitor$RoundTripTimeRunnable.run(DefaultServerMonitor.java:415)
 java.base@17.0.6/java.lang.Thread.run(Thread.java:833)
2023-04-26 13:03:35,114 WARN org.apache.catalina.loader.WebappClassLoaderBase [Thread-54] The web application [ROOT] appears to have started a thread named [MaintenanceTimer-5-thread-1] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/jdk.internal.misc.Unsafe.park(Native Method)
 java.base@17.0.6/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:252)
 java.base@17.0.6/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1672)
 java.base@17.0.6/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182)
 java.base@17.0.6/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1062)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1122)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
 java.base@17.0.6/java.lang.Thread.run(Thread.java:833)
2023-04-26 13:03:35,119 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-4, groupId=book-group] Revoke previously assigned partitions my-topic-0
2023-04-26 13:03:35,120 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-4, groupId=book-group] Member consumer-book-group-4-f68ab789-beab-4354-aa72-58f6829724c4 sending LeaveGroup request to coordinator wks-012:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2023-04-26 13:03:35,123 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-4, groupId=book-group] Resetting generation due to: consumer pro-actively leaving the group
2023-04-26 13:03:35,123 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-4, groupId=book-group] Request joining group due to: consumer pro-actively leaving the group
2023-04-26 13:03:35,123 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-4, groupId=book-group] Unsubscribed all topics or patterns and assigned partitions
2023-04-26 13:03:35,124 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-4, groupId=book-group] Resetting generation due to: consumer pro-actively leaving the group
2023-04-26 13:03:35,124 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-4, groupId=book-group] Request joining group due to: consumer pro-actively leaving the group
2023-04-26 13:03:35,266 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Metrics scheduler closed
2023-04-26 13:03:35,267 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-04-26 13:03:35,267 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Metrics reporters closed
2023-04-26 13:03:35,275 INFO org.apache.kafka.common.utils.AppInfoParser [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] App info kafka.consumer for consumer-book-group-4 unregistered
2023-04-26 13:03:35,281 INFO org.spark_project.jetty.server.AbstractConnector [Thread-54] Stopped Spark@2c919bf6{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2023-04-26 13:03:35,294 INFO org.apache.spark.ui.SparkUI [Thread-54] Stopped Spark web UI at http://192.168.1.125:4040
2023-04-26 13:03:35,299 INFO org.apache.spark.MapOutputTrackerMasterEndpoint [dispatcher-event-loop-1] MapOutputTrackerMasterEndpoint stopped!
2023-04-26 13:03:35,391 INFO org.apache.spark.storage.memory.MemoryStore [Thread-54] MemoryStore cleared
2023-04-26 13:03:35,391 INFO org.apache.spark.storage.BlockManager [Thread-54] BlockManager stopped
2023-04-26 13:03:35,392 INFO org.apache.spark.storage.BlockManagerMaster [Thread-54] BlockManagerMaster stopped
2023-04-26 13:03:35,392 INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [dispatcher-event-loop-3] OutputCommitCoordinator stopped!
2023-04-26 13:03:35,430 INFO org.apache.spark.SparkContext [Thread-54] Successfully stopped SparkContext
2023-04-26 13:03:35,431 INFO org.apache.spark.SparkContext [Thread-54] SparkContext already stopped.
2023-04-26 13:03:35,433 INFO com.zaxxer.hikari.HikariDataSource [Thread-54] HikariPool-4 - Shutdown initiated...
2023-04-26 13:03:35,454 INFO com.zaxxer.hikari.HikariDataSource [Thread-54] HikariPool-4 - Shutdown completed.
2023-04-26 13:03:36,005 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] Starting SpringBootSecurityJwtMongodbApplication using Java 17.0.6 on wks-012 with PID 109060 (/home/inferyx/git/SpringApplicationWithSecurity/target/classes started by inferyx in /home/inferyx/git/SpringApplicationWithSecurity)
2023-04-26 13:03:36,005 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] No active profile set, falling back to 1 default profile: "default"
2023-04-26 13:03:37,164 INFO org.apache.catalina.core.StandardService [restartedMain] Starting service [Tomcat]
2023-04-26 13:03:37,164 INFO org.apache.catalina.core.StandardEngine [restartedMain] Starting Servlet engine: [Apache Tomcat/9.0.65]
2023-04-26 13:03:37,218 INFO org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] [restartedMain] Initializing Spring embedded WebApplicationContext
2023-04-26 13:03:37,265 INFO org.mongodb.driver.client [restartedMain] MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync|spring-boot", "version": "4.6.1"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "5.15.0-69-generic"}, "platform": "Java/Eclipse Adoptium/17.0.6+10"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.Jep395RecordCodecProvider@22d7de23]}, clusterSettings={hosts=[localhost:27017], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='30000 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, sendBufferSize=0}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, sendBufferSize=0}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=JAVA_LEGACY, serverApi=null, autoEncryptionSettings=null, contextProvider=null}
2023-04-26 13:03:37,523 INFO org.hibernate.jpa.internal.util.LogHelper [restartedMain] HHH000204: Processing PersistenceUnitInfo [name: default]
2023-04-26 13:03:37,550 INFO com.zaxxer.hikari.HikariDataSource [restartedMain] HikariPool-5 - Starting...
2023-04-26 13:03:37,556 INFO com.zaxxer.hikari.HikariDataSource [restartedMain] HikariPool-5 - Start completed.
2023-04-26 13:03:37,557 INFO org.hibernate.dialect.Dialect [restartedMain] HHH000400: Using dialect: org.hibernate.dialect.MySQL5Dialect
2023-04-26 13:03:37,622 INFO org.mongodb.driver.connection [cluster-rtt-ClusterId{value='6448d3d1b2e73c199341b599', description='null'}-localhost:27017] Opened connection [connectionId{localValue:11, serverValue:95}] to localhost:27017
2023-04-26 13:03:37,622 INFO org.mongodb.driver.connection [cluster-ClusterId{value='6448d3d1b2e73c199341b599', description='null'}-localhost:27017] Opened connection [connectionId{localValue:10, serverValue:96}] to localhost:27017
2023-04-26 13:03:37,622 INFO org.mongodb.driver.cluster [cluster-ClusterId{value='6448d3d1b2e73c199341b599', description='null'}-localhost:27017] Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=6, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=355633237}
2023-04-26 13:03:37,775 INFO org.hibernate.engine.transaction.jta.platform.internal.JtaPlatformInitiator [restartedMain] HHH000490: Using JtaPlatform implementation: [org.hibernate.engine.transaction.jta.platform.internal.NoJtaPlatform]
2023-04-26 13:03:38,066 INFO org.apache.spark.SparkContext [restartedMain] Running Spark version 2.4.5
2023-04-26 13:03:38,067 INFO org.apache.spark.SparkContext [restartedMain] Submitted application: MyAppName
2023-04-26 13:03:38,070 INFO org.apache.spark.SecurityManager [restartedMain] Changing view acls to: inferyx
2023-04-26 13:03:38,070 INFO org.apache.spark.SecurityManager [restartedMain] Changing modify acls to: inferyx
2023-04-26 13:03:38,071 INFO org.apache.spark.SecurityManager [restartedMain] Changing view acls groups to: 
2023-04-26 13:03:38,071 INFO org.apache.spark.SecurityManager [restartedMain] Changing modify acls groups to: 
2023-04-26 13:03:38,071 INFO org.apache.spark.SecurityManager [restartedMain] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(inferyx); groups with view permissions: Set(); users  with modify permissions: Set(inferyx); groups with modify permissions: Set()
2023-04-26 13:03:38,141 INFO org.apache.spark.util.Utils [restartedMain] Successfully started service 'sparkDriver' on port 40823.
2023-04-26 13:03:38,147 INFO org.apache.spark.SparkEnv [restartedMain] Registering MapOutputTracker
2023-04-26 13:03:38,148 INFO org.apache.spark.SparkEnv [restartedMain] Registering BlockManagerMaster
2023-04-26 13:03:38,149 INFO org.apache.spark.storage.BlockManagerMasterEndpoint [restartedMain] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2023-04-26 13:03:38,149 INFO org.apache.spark.storage.BlockManagerMasterEndpoint [restartedMain] BlockManagerMasterEndpoint up
2023-04-26 13:03:38,150 INFO org.apache.spark.storage.DiskBlockManager [restartedMain] Created local directory at /tmp/blockmgr-440f73cf-1e53-468a-bbdd-2a4b70f43097
2023-04-26 13:03:38,151 INFO org.apache.spark.storage.memory.MemoryStore [restartedMain] MemoryStore started with capacity 998.4 MB
2023-04-26 13:03:38,154 INFO org.apache.spark.SparkEnv [restartedMain] Registering OutputCommitCoordinator
2023-04-26 13:03:38,162 INFO org.spark_project.jetty.server.Server [restartedMain] jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2023-04-26 13:03:38,166 INFO org.spark_project.jetty.server.Server [restartedMain] Started @729828ms
2023-04-26 13:03:38,167 INFO org.spark_project.jetty.server.AbstractConnector [restartedMain] Started ServerConnector@73563d90{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2023-04-26 13:03:38,168 INFO org.apache.spark.util.Utils [restartedMain] Successfully started service 'SparkUI' on port 4040.
2023-04-26 13:03:38,169 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@30067116{/jobs,null,AVAILABLE,@Spark}
2023-04-26 13:03:38,170 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@6a5f68cc{/jobs/json,null,AVAILABLE,@Spark}
2023-04-26 13:03:38,170 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@6d80995{/jobs/job,null,AVAILABLE,@Spark}
2023-04-26 13:03:38,171 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@44201e63{/jobs/job/json,null,AVAILABLE,@Spark}
2023-04-26 13:03:38,172 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@47ad4930{/stages,null,AVAILABLE,@Spark}
2023-04-26 13:03:38,172 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@56254001{/stages/json,null,AVAILABLE,@Spark}
2023-04-26 13:03:38,173 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@10f77ff1{/stages/stage,null,AVAILABLE,@Spark}
2023-04-26 13:03:38,175 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@54e0a88a{/stages/stage/json,null,AVAILABLE,@Spark}
2023-04-26 13:03:38,176 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@79a99fef{/stages/pool,null,AVAILABLE,@Spark}
2023-04-26 13:03:38,177 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@5f0fd93{/stages/pool/json,null,AVAILABLE,@Spark}
2023-04-26 13:03:38,178 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@2e53022b{/storage,null,AVAILABLE,@Spark}
2023-04-26 13:03:38,178 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@7f264b93{/storage/json,null,AVAILABLE,@Spark}
2023-04-26 13:03:38,179 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@4a45f57f{/storage/rdd,null,AVAILABLE,@Spark}
2023-04-26 13:03:38,180 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@48db85c1{/storage/rdd/json,null,AVAILABLE,@Spark}
2023-04-26 13:03:38,180 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@7eaf84ab{/environment,null,AVAILABLE,@Spark}
2023-04-26 13:03:38,181 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@3398323e{/environment/json,null,AVAILABLE,@Spark}
2023-04-26 13:03:38,182 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@5613c73f{/executors,null,AVAILABLE,@Spark}
2023-04-26 13:03:38,183 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@42f2a5b1{/executors/json,null,AVAILABLE,@Spark}
2023-04-26 13:03:38,186 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@40faeb98{/executors/threadDump,null,AVAILABLE,@Spark}
2023-04-26 13:03:38,187 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@3281287b{/executors/threadDump/json,null,AVAILABLE,@Spark}
2023-04-26 13:03:38,189 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@7ae33b2b{/static,null,AVAILABLE,@Spark}
2023-04-26 13:03:38,190 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@2d6bc6eb{/,null,AVAILABLE,@Spark}
2023-04-26 13:03:38,191 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@3a5a4ef5{/api,null,AVAILABLE,@Spark}
2023-04-26 13:03:38,192 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@62fa2249{/jobs/job/kill,null,AVAILABLE,@Spark}
2023-04-26 13:03:38,194 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@11d5fc63{/stages/stage/kill,null,AVAILABLE,@Spark}
2023-04-26 13:03:38,194 INFO org.apache.spark.ui.SparkUI [restartedMain] Bound SparkUI to 0.0.0.0, and started at http://192.168.1.125:4040
2023-04-26 13:03:38,232 INFO org.apache.spark.executor.Executor [restartedMain] Starting executor ID driver on host localhost
2023-04-26 13:03:38,238 INFO org.apache.spark.util.Utils [restartedMain] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43969.
2023-04-26 13:03:38,238 INFO org.apache.spark.network.netty.NettyBlockTransferService [restartedMain] Server created on 192.168.1.125:43969
2023-04-26 13:03:38,239 INFO org.apache.spark.storage.BlockManager [restartedMain] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2023-04-26 13:03:38,239 INFO org.apache.spark.storage.BlockManagerMaster [restartedMain] Registering BlockManager BlockManagerId(driver, 192.168.1.125, 43969, None)
2023-04-26 13:03:38,240 INFO org.apache.spark.storage.BlockManagerMasterEndpoint [dispatcher-event-loop-2] Registering block manager 192.168.1.125:43969 with 998.4 MB RAM, BlockManagerId(driver, 192.168.1.125, 43969, None)
2023-04-26 13:03:38,240 INFO org.apache.spark.storage.BlockManagerMaster [restartedMain] Registered BlockManager BlockManagerId(driver, 192.168.1.125, 43969, None)
2023-04-26 13:03:38,241 INFO org.apache.spark.storage.BlockManager [restartedMain] Initialized BlockManager: BlockManagerId(driver, 192.168.1.125, 43969, None)
2023-04-26 13:03:38,242 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@76adcabb{/metrics/json,null,AVAILABLE,@Spark}
2023-04-26 13:03:41,713 INFO org.apache.kafka.clients.consumer.ConsumerConfig [restartedMain] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-book-group-5
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = book-group
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2023-04-26 13:03:41,718 INFO org.apache.kafka.common.utils.AppInfoParser [restartedMain] Kafka version: 3.1.1
2023-04-26 13:03:41,718 INFO org.apache.kafka.common.utils.AppInfoParser [restartedMain] Kafka commitId: 97671528ba54a138
2023-04-26 13:03:41,719 INFO org.apache.kafka.common.utils.AppInfoParser [restartedMain] Kafka startTimeMs: 1682494421718
2023-04-26 13:03:41,719 INFO org.apache.kafka.clients.consumer.KafkaConsumer [restartedMain] [Consumer clientId=consumer-book-group-5, groupId=book-group] Subscribed to topic(s): my-topic
2023-04-26 13:03:41,724 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-5, groupId=book-group] Resetting the last seen epoch of partition my-topic-0 to 0 since the associated topicId changed from null to QqwJ2XdJQbi2BDXQbzT51g
2023-04-26 13:03:41,725 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-5, groupId=book-group] Cluster ID: 6CnV1ouTQjGsP-r9Rd2WRA
2023-04-26 13:03:41,725 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-5, groupId=book-group] Discovered group coordinator wks-012:9092 (id: 2147483647 rack: null)
2023-04-26 13:03:41,729 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-5, groupId=book-group] (Re-)joining group
2023-04-26 13:03:41,744 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] Started SpringBootSecurityJwtMongodbApplication in 5.961 seconds (JVM running for 733.406)
2023-04-26 13:03:41,747 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] This is a info message
2023-04-26 13:03:41,748 WARN com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] This is a warn message
2023-04-26 13:03:41,775 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-5, groupId=book-group] Request joining group due to: need to re-join with the given member-id
2023-04-26 13:03:41,776 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-5, groupId=book-group] (Re-)joining group
2023-04-26 13:03:41,779 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-5, groupId=book-group] Successfully joined group with generation Generation{generationId=179, memberId='consumer-book-group-5-bde47c0b-1727-49f6-a674-d93a98df9028', protocol='range'}
2023-04-26 13:03:41,779 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-5, groupId=book-group] Finished assignment for group at generation 179: {consumer-book-group-5-bde47c0b-1727-49f6-a674-d93a98df9028=Assignment(partitions=[my-topic-0])}
2023-04-26 13:03:41,783 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-5, groupId=book-group] Successfully synced group in generation Generation{generationId=179, memberId='consumer-book-group-5-bde47c0b-1727-49f6-a674-d93a98df9028', protocol='range'}
2023-04-26 13:03:41,784 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-5, groupId=book-group] Notifying assignor about the new Assignment(partitions=[my-topic-0])
2023-04-26 13:03:41,784 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-5, groupId=book-group] Adding newly assigned partitions: my-topic-0
2023-04-26 13:03:41,787 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-5, groupId=book-group] Setting offset for partition my-topic-0 to the committed offset FetchPosition{offset=417, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[wks-012:9092 (id: 0 rack: null)], epoch=0}}
2023-04-26 13:03:48,204 INFO org.apache.catalina.core.StandardService [Thread-70] Stopping service [Tomcat]
2023-04-26 13:03:48,209 WARN org.apache.catalina.loader.WebappClassLoaderBase [Thread-70] The web application [ROOT] appears to have started a thread named [cluster-ClusterId{value='6448d3d1b2e73c199341b599', description='null'}-localhost:27017] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/jdk.internal.misc.Unsafe.park(Native Method)
 java.base@17.0.6/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:252)
 java.base@17.0.6/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1672)
 app//com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.waitForSignalOrTimeout(DefaultServerMonitor.java:300)
 app//com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.waitForNext(DefaultServerMonitor.java:281)
 app//com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:179)
 java.base@17.0.6/java.lang.Thread.run(Thread.java:833)
2023-04-26 13:03:48,213 WARN org.apache.catalina.loader.WebappClassLoaderBase [Thread-70] The web application [ROOT] appears to have started a thread named [cluster-rtt-ClusterId{value='6448d3d1b2e73c199341b599', description='null'}-localhost:27017] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/java.lang.Thread.sleep(Native Method)
 app//com.mongodb.internal.connection.DefaultServerMonitor.waitForNext(DefaultServerMonitor.java:443)
 app//com.mongodb.internal.connection.DefaultServerMonitor.access$1500(DefaultServerMonitor.java:64)
 app//com.mongodb.internal.connection.DefaultServerMonitor$RoundTripTimeRunnable.run(DefaultServerMonitor.java:415)
 java.base@17.0.6/java.lang.Thread.run(Thread.java:833)
2023-04-26 13:03:48,215 WARN org.apache.catalina.loader.WebappClassLoaderBase [Thread-70] The web application [ROOT] appears to have started a thread named [MaintenanceTimer-6-thread-1] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/jdk.internal.misc.Unsafe.park(Native Method)
 java.base@17.0.6/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:252)
 java.base@17.0.6/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1672)
 java.base@17.0.6/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182)
 java.base@17.0.6/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1062)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1122)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
 java.base@17.0.6/java.lang.Thread.run(Thread.java:833)
2023-04-26 13:03:48,224 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-5, groupId=book-group] Revoke previously assigned partitions my-topic-0
2023-04-26 13:03:48,224 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-5, groupId=book-group] Member consumer-book-group-5-bde47c0b-1727-49f6-a674-d93a98df9028 sending LeaveGroup request to coordinator wks-012:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2023-04-26 13:03:48,225 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-5, groupId=book-group] Resetting generation due to: consumer pro-actively leaving the group
2023-04-26 13:03:48,225 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-5, groupId=book-group] Request joining group due to: consumer pro-actively leaving the group
2023-04-26 13:03:48,225 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-5, groupId=book-group] Unsubscribed all topics or patterns and assigned partitions
2023-04-26 13:03:48,227 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-5, groupId=book-group] Resetting generation due to: consumer pro-actively leaving the group
2023-04-26 13:03:48,227 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-5, groupId=book-group] Request joining group due to: consumer pro-actively leaving the group
2023-04-26 13:03:48,263 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Metrics scheduler closed
2023-04-26 13:03:48,264 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-04-26 13:03:48,264 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Metrics reporters closed
2023-04-26 13:03:48,268 INFO org.apache.kafka.common.utils.AppInfoParser [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] App info kafka.consumer for consumer-book-group-5 unregistered
2023-04-26 13:03:48,276 INFO org.spark_project.jetty.server.AbstractConnector [Thread-70] Stopped Spark@73563d90{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2023-04-26 13:03:48,278 INFO org.apache.spark.ui.SparkUI [Thread-70] Stopped Spark web UI at http://192.168.1.125:4040
2023-04-26 13:03:48,280 INFO org.apache.spark.MapOutputTrackerMasterEndpoint [dispatcher-event-loop-2] MapOutputTrackerMasterEndpoint stopped!
2023-04-26 13:03:48,348 INFO org.apache.spark.storage.memory.MemoryStore [Thread-70] MemoryStore cleared
2023-04-26 13:03:48,350 INFO org.apache.spark.storage.BlockManager [Thread-70] BlockManager stopped
2023-04-26 13:03:48,352 INFO org.apache.spark.storage.BlockManagerMaster [Thread-70] BlockManagerMaster stopped
2023-04-26 13:03:48,352 INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [dispatcher-event-loop-3] OutputCommitCoordinator stopped!
2023-04-26 13:03:48,382 INFO org.apache.spark.SparkContext [Thread-70] Successfully stopped SparkContext
2023-04-26 13:03:48,382 INFO org.apache.spark.SparkContext [Thread-70] SparkContext already stopped.
2023-04-26 13:03:48,386 INFO com.zaxxer.hikari.HikariDataSource [Thread-70] HikariPool-5 - Shutdown initiated...
2023-04-26 13:03:48,391 INFO com.zaxxer.hikari.HikariDataSource [Thread-70] HikariPool-5 - Shutdown completed.
2023-04-26 13:03:48,930 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] Starting SpringBootSecurityJwtMongodbApplication using Java 17.0.6 on wks-012 with PID 109060 (/home/inferyx/git/SpringApplicationWithSecurity/target/classes started by inferyx in /home/inferyx/git/SpringApplicationWithSecurity)
2023-04-26 13:03:48,931 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] No active profile set, falling back to 1 default profile: "default"
2023-04-26 13:03:50,032 INFO org.apache.catalina.core.StandardService [restartedMain] Starting service [Tomcat]
2023-04-26 13:03:50,032 INFO org.apache.catalina.core.StandardEngine [restartedMain] Starting Servlet engine: [Apache Tomcat/9.0.65]
2023-04-26 13:03:50,084 INFO org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] [restartedMain] Initializing Spring embedded WebApplicationContext
2023-04-26 13:03:50,140 INFO org.mongodb.driver.client [restartedMain] MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync|spring-boot", "version": "4.6.1"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "5.15.0-69-generic"}, "platform": "Java/Eclipse Adoptium/17.0.6+10"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.Jep395RecordCodecProvider@22d7de23]}, clusterSettings={hosts=[localhost:27017], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='30000 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, sendBufferSize=0}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, sendBufferSize=0}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=JAVA_LEGACY, serverApi=null, autoEncryptionSettings=null, contextProvider=null}
2023-04-26 13:03:50,222 INFO org.mongodb.driver.connection [cluster-ClusterId{value='6448d3deb2e73c199341b59a', description='null'}-localhost:27017] Opened connection [connectionId{localValue:12, serverValue:97}] to localhost:27017
2023-04-26 13:03:50,223 INFO org.mongodb.driver.cluster [cluster-ClusterId{value='6448d3deb2e73c199341b59a', description='null'}-localhost:27017] Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=6, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=81579445}
2023-04-26 13:03:50,224 INFO org.mongodb.driver.connection [cluster-rtt-ClusterId{value='6448d3deb2e73c199341b59a', description='null'}-localhost:27017] Opened connection [connectionId{localValue:13, serverValue:98}] to localhost:27017
2023-04-26 13:03:50,416 INFO org.hibernate.jpa.internal.util.LogHelper [restartedMain] HHH000204: Processing PersistenceUnitInfo [name: default]
2023-04-26 13:03:50,441 INFO com.zaxxer.hikari.HikariDataSource [restartedMain] HikariPool-6 - Starting...
2023-04-26 13:03:50,448 INFO com.zaxxer.hikari.HikariDataSource [restartedMain] HikariPool-6 - Start completed.
2023-04-26 13:03:50,448 INFO org.hibernate.dialect.Dialect [restartedMain] HHH000400: Using dialect: org.hibernate.dialect.MySQL5Dialect
2023-04-26 13:03:50,720 INFO org.hibernate.engine.transaction.jta.platform.internal.JtaPlatformInitiator [restartedMain] HHH000490: Using JtaPlatform implementation: [org.hibernate.engine.transaction.jta.platform.internal.NoJtaPlatform]
2023-04-26 13:03:51,014 INFO org.apache.spark.SparkContext [restartedMain] Running Spark version 2.4.5
2023-04-26 13:03:51,015 INFO org.apache.spark.SparkContext [restartedMain] Submitted application: MyAppName
2023-04-26 13:03:51,017 INFO org.apache.spark.SecurityManager [restartedMain] Changing view acls to: inferyx
2023-04-26 13:03:51,018 INFO org.apache.spark.SecurityManager [restartedMain] Changing modify acls to: inferyx
2023-04-26 13:03:51,018 INFO org.apache.spark.SecurityManager [restartedMain] Changing view acls groups to: 
2023-04-26 13:03:51,019 INFO org.apache.spark.SecurityManager [restartedMain] Changing modify acls groups to: 
2023-04-26 13:03:51,019 INFO org.apache.spark.SecurityManager [restartedMain] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(inferyx); groups with view permissions: Set(); users  with modify permissions: Set(inferyx); groups with modify permissions: Set()
2023-04-26 13:03:51,103 INFO org.apache.spark.util.Utils [restartedMain] Successfully started service 'sparkDriver' on port 42335.
2023-04-26 13:03:51,107 INFO org.apache.spark.SparkEnv [restartedMain] Registering MapOutputTracker
2023-04-26 13:03:51,108 INFO org.apache.spark.SparkEnv [restartedMain] Registering BlockManagerMaster
2023-04-26 13:03:51,111 INFO org.apache.spark.storage.BlockManagerMasterEndpoint [restartedMain] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2023-04-26 13:03:51,111 INFO org.apache.spark.storage.BlockManagerMasterEndpoint [restartedMain] BlockManagerMasterEndpoint up
2023-04-26 13:03:51,112 INFO org.apache.spark.storage.DiskBlockManager [restartedMain] Created local directory at /tmp/blockmgr-d3613175-6e6b-4c7d-acaa-1bf4943c0210
2023-04-26 13:03:51,113 INFO org.apache.spark.storage.memory.MemoryStore [restartedMain] MemoryStore started with capacity 998.4 MB
2023-04-26 13:03:51,117 INFO org.apache.spark.SparkEnv [restartedMain] Registering OutputCommitCoordinator
2023-04-26 13:03:51,123 INFO org.spark_project.jetty.server.Server [restartedMain] jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2023-04-26 13:03:51,125 INFO org.spark_project.jetty.server.Server [restartedMain] Started @742787ms
2023-04-26 13:03:51,126 INFO org.spark_project.jetty.server.AbstractConnector [restartedMain] Started ServerConnector@355aa3f6{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2023-04-26 13:03:51,127 INFO org.apache.spark.util.Utils [restartedMain] Successfully started service 'SparkUI' on port 4040.
2023-04-26 13:03:51,127 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@6ad8c255{/jobs,null,AVAILABLE,@Spark}
2023-04-26 13:03:51,128 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@3bf8d5d1{/jobs/json,null,AVAILABLE,@Spark}
2023-04-26 13:03:51,130 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@7fffc9be{/jobs/job,null,AVAILABLE,@Spark}
2023-04-26 13:03:51,130 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@1fda90a4{/jobs/job/json,null,AVAILABLE,@Spark}
2023-04-26 13:03:51,131 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@333c3360{/stages,null,AVAILABLE,@Spark}
2023-04-26 13:03:51,134 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@4e973e8b{/stages/json,null,AVAILABLE,@Spark}
2023-04-26 13:03:51,134 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@4fa4acde{/stages/stage,null,AVAILABLE,@Spark}
2023-04-26 13:03:51,135 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@4304a3e{/stages/stage/json,null,AVAILABLE,@Spark}
2023-04-26 13:03:51,136 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@5645ceeb{/stages/pool,null,AVAILABLE,@Spark}
2023-04-26 13:03:51,136 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@2151c1ce{/stages/pool/json,null,AVAILABLE,@Spark}
2023-04-26 13:03:51,137 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@4bf95b66{/storage,null,AVAILABLE,@Spark}
2023-04-26 13:03:51,138 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@4d9e899d{/storage/json,null,AVAILABLE,@Spark}
2023-04-26 13:03:51,139 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@35d42453{/storage/rdd,null,AVAILABLE,@Spark}
2023-04-26 13:03:51,139 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@1a24cc96{/storage/rdd/json,null,AVAILABLE,@Spark}
2023-04-26 13:03:51,139 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@1b8796df{/environment,null,AVAILABLE,@Spark}
2023-04-26 13:03:51,140 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@22ec2653{/environment/json,null,AVAILABLE,@Spark}
2023-04-26 13:03:51,140 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@368e78e1{/executors,null,AVAILABLE,@Spark}
2023-04-26 13:03:51,141 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@a7242da{/executors/json,null,AVAILABLE,@Spark}
2023-04-26 13:03:51,141 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@2d640c25{/executors/threadDump,null,AVAILABLE,@Spark}
2023-04-26 13:03:51,142 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@6e797964{/executors/threadDump/json,null,AVAILABLE,@Spark}
2023-04-26 13:03:51,142 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@2b754d3a{/static,null,AVAILABLE,@Spark}
2023-04-26 13:03:51,144 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@3309be1c{/,null,AVAILABLE,@Spark}
2023-04-26 13:03:51,144 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@3e81997c{/api,null,AVAILABLE,@Spark}
2023-04-26 13:03:51,145 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@1a0be744{/jobs/job/kill,null,AVAILABLE,@Spark}
2023-04-26 13:03:51,146 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@648c5347{/stages/stage/kill,null,AVAILABLE,@Spark}
2023-04-26 13:03:51,147 INFO org.apache.spark.ui.SparkUI [restartedMain] Bound SparkUI to 0.0.0.0, and started at http://192.168.1.125:4040
2023-04-26 13:03:51,185 INFO org.apache.spark.executor.Executor [restartedMain] Starting executor ID driver on host localhost
2023-04-26 13:03:51,189 INFO org.apache.spark.util.Utils [restartedMain] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33319.
2023-04-26 13:03:51,190 INFO org.apache.spark.network.netty.NettyBlockTransferService [restartedMain] Server created on 192.168.1.125:33319
2023-04-26 13:03:51,190 INFO org.apache.spark.storage.BlockManager [restartedMain] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2023-04-26 13:03:51,190 INFO org.apache.spark.storage.BlockManagerMaster [restartedMain] Registering BlockManager BlockManagerId(driver, 192.168.1.125, 33319, None)
2023-04-26 13:03:51,191 INFO org.apache.spark.storage.BlockManagerMasterEndpoint [dispatcher-event-loop-2] Registering block manager 192.168.1.125:33319 with 998.4 MB RAM, BlockManagerId(driver, 192.168.1.125, 33319, None)
2023-04-26 13:03:51,192 INFO org.apache.spark.storage.BlockManagerMaster [restartedMain] Registered BlockManager BlockManagerId(driver, 192.168.1.125, 33319, None)
2023-04-26 13:03:51,192 INFO org.apache.spark.storage.BlockManager [restartedMain] Initialized BlockManager: BlockManagerId(driver, 192.168.1.125, 33319, None)
2023-04-26 13:03:51,193 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@420982bc{/metrics/json,null,AVAILABLE,@Spark}
2023-04-26 13:03:53,220 INFO org.apache.kafka.clients.consumer.ConsumerConfig [restartedMain] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-book-group-6
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = book-group
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2023-04-26 13:03:53,227 INFO org.apache.kafka.common.utils.AppInfoParser [restartedMain] Kafka version: 3.1.1
2023-04-26 13:03:53,227 INFO org.apache.kafka.common.utils.AppInfoParser [restartedMain] Kafka commitId: 97671528ba54a138
2023-04-26 13:03:53,227 INFO org.apache.kafka.common.utils.AppInfoParser [restartedMain] Kafka startTimeMs: 1682494433227
2023-04-26 13:03:53,228 INFO org.apache.kafka.clients.consumer.KafkaConsumer [restartedMain] [Consumer clientId=consumer-book-group-6, groupId=book-group] Subscribed to topic(s): my-topic
2023-04-26 13:03:53,234 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-6, groupId=book-group] Resetting the last seen epoch of partition my-topic-0 to 0 since the associated topicId changed from null to QqwJ2XdJQbi2BDXQbzT51g
2023-04-26 13:03:53,234 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-6, groupId=book-group] Cluster ID: 6CnV1ouTQjGsP-r9Rd2WRA
2023-04-26 13:03:53,244 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] Started SpringBootSecurityJwtMongodbApplication in 4.568 seconds (JVM running for 744.906)
2023-04-26 13:03:53,247 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] This is a info message
2023-04-26 13:03:53,248 WARN com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] This is a warn message
2023-04-26 13:03:53,267 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-6, groupId=book-group] Discovered group coordinator wks-012:9092 (id: 2147483647 rack: null)
2023-04-26 13:03:53,268 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-6, groupId=book-group] (Re-)joining group
2023-04-26 13:03:53,329 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-6, groupId=book-group] Request joining group due to: need to re-join with the given member-id
2023-04-26 13:03:53,329 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-6, groupId=book-group] (Re-)joining group
2023-04-26 13:03:53,332 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-6, groupId=book-group] Successfully joined group with generation Generation{generationId=181, memberId='consumer-book-group-6-e16f70b0-9ff5-4775-9ef5-61f9cc3285dd', protocol='range'}
2023-04-26 13:03:53,332 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-6, groupId=book-group] Finished assignment for group at generation 181: {consumer-book-group-6-e16f70b0-9ff5-4775-9ef5-61f9cc3285dd=Assignment(partitions=[my-topic-0])}
2023-04-26 13:03:55,002 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-6, groupId=book-group] Successfully synced group in generation Generation{generationId=181, memberId='consumer-book-group-6-e16f70b0-9ff5-4775-9ef5-61f9cc3285dd', protocol='range'}
2023-04-26 13:03:55,002 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-6, groupId=book-group] Notifying assignor about the new Assignment(partitions=[my-topic-0])
2023-04-26 13:03:55,003 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-6, groupId=book-group] Adding newly assigned partitions: my-topic-0
2023-04-26 13:03:55,005 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-6, groupId=book-group] Setting offset for partition my-topic-0 to the committed offset FetchPosition{offset=417, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[wks-012:9092 (id: 0 rack: null)], epoch=0}}
2023-04-26 13:04:36,790 INFO org.apache.catalina.core.StandardService [Thread-87] Stopping service [Tomcat]
2023-04-26 13:04:36,794 WARN org.apache.catalina.loader.WebappClassLoaderBase [Thread-87] The web application [ROOT] appears to have started a thread named [cluster-ClusterId{value='6448d3deb2e73c199341b59a', description='null'}-localhost:27017] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/jdk.internal.misc.Unsafe.park(Native Method)
 java.base@17.0.6/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:252)
 java.base@17.0.6/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1672)
 app//com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.waitForSignalOrTimeout(DefaultServerMonitor.java:300)
 app//com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.waitForNext(DefaultServerMonitor.java:281)
 app//com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:179)
 java.base@17.0.6/java.lang.Thread.run(Thread.java:833)
2023-04-26 13:04:36,795 WARN org.apache.catalina.loader.WebappClassLoaderBase [Thread-87] The web application [ROOT] appears to have started a thread named [cluster-rtt-ClusterId{value='6448d3deb2e73c199341b59a', description='null'}-localhost:27017] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/java.lang.Thread.sleep(Native Method)
 app//com.mongodb.internal.connection.DefaultServerMonitor.waitForNext(DefaultServerMonitor.java:443)
 app//com.mongodb.internal.connection.DefaultServerMonitor.access$1500(DefaultServerMonitor.java:64)
 app//com.mongodb.internal.connection.DefaultServerMonitor$RoundTripTimeRunnable.run(DefaultServerMonitor.java:415)
 java.base@17.0.6/java.lang.Thread.run(Thread.java:833)
2023-04-26 13:04:36,796 WARN org.apache.catalina.loader.WebappClassLoaderBase [Thread-87] The web application [ROOT] appears to have started a thread named [MaintenanceTimer-7-thread-1] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base@17.0.6/jdk.internal.misc.Unsafe.park(Native Method)
 java.base@17.0.6/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:252)
 java.base@17.0.6/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1672)
 java.base@17.0.6/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182)
 java.base@17.0.6/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1062)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1122)
 java.base@17.0.6/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
 java.base@17.0.6/java.lang.Thread.run(Thread.java:833)
2023-04-26 13:04:36,801 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-6, groupId=book-group] Revoke previously assigned partitions my-topic-0
2023-04-26 13:04:36,801 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-6, groupId=book-group] Member consumer-book-group-6-e16f70b0-9ff5-4775-9ef5-61f9cc3285dd sending LeaveGroup request to coordinator wks-012:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2023-04-26 13:04:36,802 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-6, groupId=book-group] Resetting generation due to: consumer pro-actively leaving the group
2023-04-26 13:04:36,802 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-6, groupId=book-group] Request joining group due to: consumer pro-actively leaving the group
2023-04-26 13:04:36,802 INFO org.apache.kafka.clients.consumer.KafkaConsumer [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-6, groupId=book-group] Unsubscribed all topics or patterns and assigned partitions
2023-04-26 13:04:36,803 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-6, groupId=book-group] Resetting generation due to: consumer pro-actively leaving the group
2023-04-26 13:04:36,803 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-6, groupId=book-group] Request joining group due to: consumer pro-actively leaving the group
2023-04-26 13:04:36,829 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Metrics scheduler closed
2023-04-26 13:04:36,830 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-04-26 13:04:36,830 INFO org.apache.kafka.common.metrics.Metrics [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] Metrics reporters closed
2023-04-26 13:04:36,834 INFO org.apache.kafka.common.utils.AppInfoParser [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] App info kafka.consumer for consumer-book-group-6 unregistered
2023-04-26 13:04:36,841 INFO org.spark_project.jetty.server.AbstractConnector [Thread-87] Stopped Spark@355aa3f6{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2023-04-26 13:04:36,843 INFO org.apache.spark.ui.SparkUI [Thread-87] Stopped Spark web UI at http://192.168.1.125:4040
2023-04-26 13:04:37,043 INFO org.apache.spark.MapOutputTrackerMasterEndpoint [dispatcher-event-loop-2] MapOutputTrackerMasterEndpoint stopped!
2023-04-26 13:04:37,095 INFO org.apache.spark.storage.memory.MemoryStore [Thread-87] MemoryStore cleared
2023-04-26 13:04:37,095 INFO org.apache.spark.storage.BlockManager [Thread-87] BlockManager stopped
2023-04-26 13:04:37,096 INFO org.apache.spark.storage.BlockManagerMaster [Thread-87] BlockManagerMaster stopped
2023-04-26 13:04:37,097 INFO org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [dispatcher-event-loop-3] OutputCommitCoordinator stopped!
2023-04-26 13:04:37,109 INFO org.apache.spark.SparkContext [Thread-87] Successfully stopped SparkContext
2023-04-26 13:04:37,109 INFO org.apache.spark.SparkContext [Thread-87] SparkContext already stopped.
2023-04-26 13:04:37,111 INFO com.zaxxer.hikari.HikariDataSource [Thread-87] HikariPool-6 - Shutdown initiated...
2023-04-26 13:04:37,116 INFO com.zaxxer.hikari.HikariDataSource [Thread-87] HikariPool-6 - Shutdown completed.
2023-04-26 13:04:37,471 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] Starting SpringBootSecurityJwtMongodbApplication using Java 17.0.6 on wks-012 with PID 109060 (/home/inferyx/git/SpringApplicationWithSecurity/target/classes started by inferyx in /home/inferyx/git/SpringApplicationWithSecurity)
2023-04-26 13:04:37,471 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] No active profile set, falling back to 1 default profile: "default"
2023-04-26 13:04:38,510 INFO org.apache.catalina.core.StandardService [restartedMain] Starting service [Tomcat]
2023-04-26 13:04:38,510 INFO org.apache.catalina.core.StandardEngine [restartedMain] Starting Servlet engine: [Apache Tomcat/9.0.65]
2023-04-26 13:04:38,557 INFO org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] [restartedMain] Initializing Spring embedded WebApplicationContext
2023-04-26 13:04:38,617 INFO org.mongodb.driver.client [restartedMain] MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync|spring-boot", "version": "4.6.1"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "5.15.0-69-generic"}, "platform": "Java/Eclipse Adoptium/17.0.6+10"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.Jep395RecordCodecProvider@22d7de23]}, clusterSettings={hosts=[localhost:27017], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='30000 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, sendBufferSize=0}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, sendBufferSize=0}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=JAVA_LEGACY, serverApi=null, autoEncryptionSettings=null, contextProvider=null}
2023-04-26 13:04:38,620 INFO org.mongodb.driver.connection [cluster-ClusterId{value='6448d40eb2e73c199341b59b', description='null'}-localhost:27017] Opened connection [connectionId{localValue:14, serverValue:99}] to localhost:27017
2023-04-26 13:04:38,620 INFO org.mongodb.driver.cluster [cluster-ClusterId{value='6448d40eb2e73c199341b59b', description='null'}-localhost:27017] Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=6, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1386575}
2023-04-26 13:04:38,625 INFO org.mongodb.driver.connection [cluster-rtt-ClusterId{value='6448d40eb2e73c199341b59b', description='null'}-localhost:27017] Opened connection [connectionId{localValue:15, serverValue:100}] to localhost:27017
2023-04-26 13:04:38,904 INFO org.hibernate.jpa.internal.util.LogHelper [restartedMain] HHH000204: Processing PersistenceUnitInfo [name: default]
2023-04-26 13:04:38,937 INFO com.zaxxer.hikari.HikariDataSource [restartedMain] HikariPool-7 - Starting...
2023-04-26 13:04:38,943 INFO com.zaxxer.hikari.HikariDataSource [restartedMain] HikariPool-7 - Start completed.
2023-04-26 13:04:38,944 INFO org.hibernate.dialect.Dialect [restartedMain] HHH000400: Using dialect: org.hibernate.dialect.MySQL5Dialect
2023-04-26 13:04:39,101 INFO org.hibernate.engine.transaction.jta.platform.internal.JtaPlatformInitiator [restartedMain] HHH000490: Using JtaPlatform implementation: [org.hibernate.engine.transaction.jta.platform.internal.NoJtaPlatform]
2023-04-26 13:04:39,408 INFO org.apache.spark.SparkContext [restartedMain] Running Spark version 2.4.5
2023-04-26 13:04:39,409 INFO org.apache.spark.SparkContext [restartedMain] Submitted application: MyAppName
2023-04-26 13:04:39,410 INFO org.apache.spark.SecurityManager [restartedMain] Changing view acls to: inferyx
2023-04-26 13:04:39,411 INFO org.apache.spark.SecurityManager [restartedMain] Changing modify acls to: inferyx
2023-04-26 13:04:39,411 INFO org.apache.spark.SecurityManager [restartedMain] Changing view acls groups to: 
2023-04-26 13:04:39,411 INFO org.apache.spark.SecurityManager [restartedMain] Changing modify acls groups to: 
2023-04-26 13:04:39,412 INFO org.apache.spark.SecurityManager [restartedMain] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(inferyx); groups with view permissions: Set(); users  with modify permissions: Set(inferyx); groups with modify permissions: Set()
2023-04-26 13:04:39,486 INFO org.apache.spark.util.Utils [restartedMain] Successfully started service 'sparkDriver' on port 33199.
2023-04-26 13:04:39,490 INFO org.apache.spark.SparkEnv [restartedMain] Registering MapOutputTracker
2023-04-26 13:04:39,491 INFO org.apache.spark.SparkEnv [restartedMain] Registering BlockManagerMaster
2023-04-26 13:04:39,492 INFO org.apache.spark.storage.BlockManagerMasterEndpoint [restartedMain] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2023-04-26 13:04:39,492 INFO org.apache.spark.storage.BlockManagerMasterEndpoint [restartedMain] BlockManagerMasterEndpoint up
2023-04-26 13:04:39,493 INFO org.apache.spark.storage.DiskBlockManager [restartedMain] Created local directory at /tmp/blockmgr-ec766b63-81c9-4660-b744-05426f6600e5
2023-04-26 13:04:39,494 INFO org.apache.spark.storage.memory.MemoryStore [restartedMain] MemoryStore started with capacity 998.4 MB
2023-04-26 13:04:39,498 INFO org.apache.spark.SparkEnv [restartedMain] Registering OutputCommitCoordinator
2023-04-26 13:04:39,505 INFO org.spark_project.jetty.server.Server [restartedMain] jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2023-04-26 13:04:39,507 INFO org.spark_project.jetty.server.Server [restartedMain] Started @791169ms
2023-04-26 13:04:39,508 INFO org.spark_project.jetty.server.AbstractConnector [restartedMain] Started ServerConnector@62cf39b{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2023-04-26 13:04:39,508 INFO org.apache.spark.util.Utils [restartedMain] Successfully started service 'SparkUI' on port 4040.
2023-04-26 13:04:39,509 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@6b9ee9d6{/jobs,null,AVAILABLE,@Spark}
2023-04-26 13:04:39,509 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@1d59a456{/jobs/json,null,AVAILABLE,@Spark}
2023-04-26 13:04:39,510 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@385a2d76{/jobs/job,null,AVAILABLE,@Spark}
2023-04-26 13:04:39,511 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@598ba1cd{/jobs/job/json,null,AVAILABLE,@Spark}
2023-04-26 13:04:39,511 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@45c73373{/stages,null,AVAILABLE,@Spark}
2023-04-26 13:04:39,512 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@15ec5489{/stages/json,null,AVAILABLE,@Spark}
2023-04-26 13:04:39,512 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@7cb4fadf{/stages/stage,null,AVAILABLE,@Spark}
2023-04-26 13:04:39,513 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@78b5b94d{/stages/stage/json,null,AVAILABLE,@Spark}
2023-04-26 13:04:39,514 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@4955183{/stages/pool,null,AVAILABLE,@Spark}
2023-04-26 13:04:39,514 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@73aa1584{/stages/pool/json,null,AVAILABLE,@Spark}
2023-04-26 13:04:39,514 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@28136b2c{/storage,null,AVAILABLE,@Spark}
2023-04-26 13:04:39,515 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@7d39c0b2{/storage/json,null,AVAILABLE,@Spark}
2023-04-26 13:04:39,516 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@530e9fdf{/storage/rdd,null,AVAILABLE,@Spark}
2023-04-26 13:04:39,516 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@70aba60f{/storage/rdd/json,null,AVAILABLE,@Spark}
2023-04-26 13:04:39,517 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@4352f93d{/environment,null,AVAILABLE,@Spark}
2023-04-26 13:04:39,518 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@70dd9db5{/environment/json,null,AVAILABLE,@Spark}
2023-04-26 13:04:39,518 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@570a1416{/executors,null,AVAILABLE,@Spark}
2023-04-26 13:04:39,519 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@7a3c4b26{/executors/json,null,AVAILABLE,@Spark}
2023-04-26 13:04:39,519 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@2e73ce4d{/executors/threadDump,null,AVAILABLE,@Spark}
2023-04-26 13:04:39,520 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@325fde03{/executors/threadDump/json,null,AVAILABLE,@Spark}
2023-04-26 13:04:39,520 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@3572c6fa{/static,null,AVAILABLE,@Spark}
2023-04-26 13:04:39,521 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@2d8fde09{/,null,AVAILABLE,@Spark}
2023-04-26 13:04:39,522 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@1dbe6c53{/api,null,AVAILABLE,@Spark}
2023-04-26 13:04:39,522 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@72bb5fe6{/jobs/job/kill,null,AVAILABLE,@Spark}
2023-04-26 13:04:39,523 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@2117b82e{/stages/stage/kill,null,AVAILABLE,@Spark}
2023-04-26 13:04:39,523 INFO org.apache.spark.ui.SparkUI [restartedMain] Bound SparkUI to 0.0.0.0, and started at http://192.168.1.125:4040
2023-04-26 13:04:39,553 INFO org.apache.spark.executor.Executor [restartedMain] Starting executor ID driver on host localhost
2023-04-26 13:04:39,557 INFO org.apache.spark.util.Utils [restartedMain] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44401.
2023-04-26 13:04:39,557 INFO org.apache.spark.network.netty.NettyBlockTransferService [restartedMain] Server created on 192.168.1.125:44401
2023-04-26 13:04:39,558 INFO org.apache.spark.storage.BlockManager [restartedMain] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2023-04-26 13:04:39,558 INFO org.apache.spark.storage.BlockManagerMaster [restartedMain] Registering BlockManager BlockManagerId(driver, 192.168.1.125, 44401, None)
2023-04-26 13:04:39,558 INFO org.apache.spark.storage.BlockManagerMasterEndpoint [dispatcher-event-loop-2] Registering block manager 192.168.1.125:44401 with 998.4 MB RAM, BlockManagerId(driver, 192.168.1.125, 44401, None)
2023-04-26 13:04:39,559 INFO org.apache.spark.storage.BlockManagerMaster [restartedMain] Registered BlockManager BlockManagerId(driver, 192.168.1.125, 44401, None)
2023-04-26 13:04:39,559 INFO org.apache.spark.storage.BlockManager [restartedMain] Initialized BlockManager: BlockManagerId(driver, 192.168.1.125, 44401, None)
2023-04-26 13:04:39,560 INFO org.spark_project.jetty.server.handler.ContextHandler [restartedMain] Started o.s.j.s.ServletContextHandler@21c31197{/metrics/json,null,AVAILABLE,@Spark}
2023-04-26 13:04:42,146 INFO org.apache.kafka.clients.consumer.ConsumerConfig [restartedMain] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-book-group-7
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = book-group
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2023-04-26 13:04:42,153 INFO org.apache.kafka.common.utils.AppInfoParser [restartedMain] Kafka version: 3.1.1
2023-04-26 13:04:42,154 INFO org.apache.kafka.common.utils.AppInfoParser [restartedMain] Kafka commitId: 97671528ba54a138
2023-04-26 13:04:42,154 INFO org.apache.kafka.common.utils.AppInfoParser [restartedMain] Kafka startTimeMs: 1682494482153
2023-04-26 13:04:42,155 INFO org.apache.kafka.clients.consumer.KafkaConsumer [restartedMain] [Consumer clientId=consumer-book-group-7, groupId=book-group] Subscribed to topic(s): my-topic
2023-04-26 13:04:42,165 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-7, groupId=book-group] Resetting the last seen epoch of partition my-topic-0 to 0 since the associated topicId changed from null to QqwJ2XdJQbi2BDXQbzT51g
2023-04-26 13:04:42,166 INFO org.apache.kafka.clients.Metadata [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-7, groupId=book-group] Cluster ID: 6CnV1ouTQjGsP-r9Rd2WRA
2023-04-26 13:04:42,167 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-7, groupId=book-group] Discovered group coordinator wks-012:9092 (id: 2147483647 rack: null)
2023-04-26 13:04:42,168 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-7, groupId=book-group] (Re-)joining group
2023-04-26 13:04:42,173 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-7, groupId=book-group] Request joining group due to: need to re-join with the given member-id
2023-04-26 13:04:42,174 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-7, groupId=book-group] (Re-)joining group
2023-04-26 13:04:42,177 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-7, groupId=book-group] Successfully joined group with generation Generation{generationId=183, memberId='consumer-book-group-7-a21b525f-2498-4c8d-b45c-2108c9ddedff', protocol='range'}
2023-04-26 13:04:42,178 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-7, groupId=book-group] Finished assignment for group at generation 183: {consumer-book-group-7-a21b525f-2498-4c8d-b45c-2108c9ddedff=Assignment(partitions=[my-topic-0])}
2023-04-26 13:04:42,182 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] Started SpringBootSecurityJwtMongodbApplication in 4.819 seconds (JVM running for 793.844)
2023-04-26 13:04:42,182 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-7, groupId=book-group] Successfully synced group in generation Generation{generationId=183, memberId='consumer-book-group-7-a21b525f-2498-4c8d-b45c-2108c9ddedff', protocol='range'}
2023-04-26 13:04:42,186 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-7, groupId=book-group] Notifying assignor about the new Assignment(partitions=[my-topic-0])
2023-04-26 13:04:42,186 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-7, groupId=book-group] Adding newly assigned partitions: my-topic-0
2023-04-26 13:04:42,189 INFO com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] This is a info message
2023-04-26 13:04:42,190 WARN com.example.spring.jwt.mongodb.SpringBootSecurityJwtMongodbApplication [restartedMain] This is a warn message
2023-04-26 13:04:42,228 INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-7, groupId=book-group] Setting offset for partition my-topic-0 to the committed offset FetchPosition{offset=417, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[wks-012:9092 (id: 0 rack: null)], epoch=0}}
2023-04-26 13:13:42,224 INFO org.apache.kafka.clients.NetworkClient [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] [Consumer clientId=consumer-book-group-7, groupId=book-group] Node -1 disconnected.
2023-04-26 13:25:05,930 INFO org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/] [http-nio-8080-exec-1] Initializing Spring DispatcherServlet 'dispatcherServlet'
2023-04-26 13:25:05,981 INFO org.mongodb.driver.connection [http-nio-8080-exec-1] Opened connection [connectionId{localValue:16, serverValue:101}] to localhost:27017
2023-04-26 13:25:06,582 INFO org.apache.spark.sql.internal.SharedState [http-nio-8080-exec-1] Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/inferyx/git/SpringApplicationWithSecurity/spark-warehouse').
2023-04-26 13:25:06,584 INFO org.apache.spark.sql.internal.SharedState [http-nio-8080-exec-1] Warehouse path is 'file:/home/inferyx/git/SpringApplicationWithSecurity/spark-warehouse'.
2023-04-26 13:25:06,651 INFO org.spark_project.jetty.server.handler.ContextHandler [http-nio-8080-exec-1] Started o.s.j.s.ServletContextHandler@665e0cf6{/SQL,null,AVAILABLE,@Spark}
2023-04-26 13:25:06,652 INFO org.spark_project.jetty.server.handler.ContextHandler [http-nio-8080-exec-1] Started o.s.j.s.ServletContextHandler@4c0273d2{/SQL/json,null,AVAILABLE,@Spark}
2023-04-26 13:25:06,653 INFO org.spark_project.jetty.server.handler.ContextHandler [http-nio-8080-exec-1] Started o.s.j.s.ServletContextHandler@5c552d20{/SQL/execution,null,AVAILABLE,@Spark}
2023-04-26 13:25:06,654 INFO org.spark_project.jetty.server.handler.ContextHandler [http-nio-8080-exec-1] Started o.s.j.s.ServletContextHandler@15c627cd{/SQL/execution/json,null,AVAILABLE,@Spark}
2023-04-26 13:25:06,682 INFO org.spark_project.jetty.server.handler.ContextHandler [http-nio-8080-exec-1] Started o.s.j.s.ServletContextHandler@3db9ff2b{/static/sql,null,AVAILABLE,@Spark}
2023-04-26 13:25:08,170 INFO org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef [http-nio-8080-exec-1] Registered StateStoreCoordinator endpoint
2023-04-26 13:25:09,364 INFO org.apache.spark.sql.execution.datasources.InMemoryFileIndex [http-nio-8080-exec-1] It took 410 ms to list leaf files for 1 paths.
2023-04-26 13:25:09,761 INFO org.apache.spark.sql.execution.datasources.InMemoryFileIndex [http-nio-8080-exec-1] It took 2 ms to list leaf files for 1 paths.
2023-04-26 13:25:14,875 INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy [http-nio-8080-exec-1] Pruning directories with: 
2023-04-26 13:25:14,885 INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy [http-nio-8080-exec-1] Post-Scan Filters: (length(trim(value#0, None)) > 0)
2023-04-26 13:25:14,905 INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy [http-nio-8080-exec-1] Output Data Schema: struct<value: string>
2023-04-26 13:25:14,921 INFO org.apache.spark.sql.execution.FileSourceScanExec [http-nio-8080-exec-1] Pushed Filters: 
2023-04-26 13:25:16,106 INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [http-nio-8080-exec-1] Code generated in 414.203087 ms
2023-04-26 13:25:17,208 INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [http-nio-8080-exec-1] Code generated in 48.40995 ms
2023-04-26 13:25:17,612 INFO org.apache.spark.storage.memory.MemoryStore [http-nio-8080-exec-1] Block broadcast_0 stored as values in memory (estimated size 112.7 KB, free 998.3 MB)
2023-04-26 13:25:18,152 INFO org.apache.spark.storage.memory.MemoryStore [http-nio-8080-exec-1] Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.7 KB, free 998.3 MB)
2023-04-26 13:25:18,167 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-3] Added broadcast_0_piece0 in memory on 192.168.1.125:44401 (size: 20.7 KB, free: 998.4 MB)
2023-04-26 13:25:18,197 INFO org.apache.spark.SparkContext [http-nio-8080-exec-1] Created broadcast 0 from csv at SparkController.java:55
2023-04-26 13:25:18,279 INFO org.apache.spark.sql.execution.FileSourceScanExec [http-nio-8080-exec-1] Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2023-04-26 13:25:19,162 INFO org.apache.spark.SparkContext [http-nio-8080-exec-1] Starting job: csv at SparkController.java:55
2023-04-26 13:25:19,283 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Got job 0 (csv at SparkController.java:55) with 1 output partitions
2023-04-26 13:25:19,284 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Final stage: ResultStage 0 (csv at SparkController.java:55)
2023-04-26 13:25:19,285 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Parents of final stage: List()
2023-04-26 13:25:19,286 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Missing parents: List()
2023-04-26 13:25:19,353 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at SparkController.java:55), which has no missing parents
2023-04-26 13:25:19,682 INFO org.apache.spark.storage.memory.MemoryStore [dag-scheduler-event-loop] Block broadcast_1 stored as values in memory (estimated size 8.9 KB, free 998.3 MB)
2023-04-26 13:25:19,685 INFO org.apache.spark.storage.memory.MemoryStore [dag-scheduler-event-loop] Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KB, free 998.3 MB)
2023-04-26 13:25:19,687 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-0] Added broadcast_1_piece0 in memory on 192.168.1.125:44401 (size: 4.6 KB, free: 998.4 MB)
2023-04-26 13:25:19,687 INFO org.apache.spark.SparkContext [dag-scheduler-event-loop] Created broadcast 1 from broadcast at DAGScheduler.scala:1163
2023-04-26 13:25:19,735 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at SparkController.java:55) (first 15 tasks are for partitions Vector(0))
2023-04-26 13:25:19,741 INFO org.apache.spark.scheduler.TaskSchedulerImpl [dag-scheduler-event-loop] Adding task set 0.0 with 1 tasks
2023-04-26 13:25:19,970 INFO org.apache.spark.scheduler.TaskSetManager [dispatcher-event-loop-1] Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 8266 bytes)
2023-04-26 13:25:20,051 INFO org.apache.spark.executor.Executor [Executor task launch worker for task 0] Running task 0.0 in stage 0.0 (TID 0)
2023-04-26 13:25:20,328 INFO org.apache.spark.sql.execution.datasources.FileScanRDD [Executor task launch worker for task 0] Reading File path: file:///home/inferyx/Documents/Files/addresses.csv, range: 0-328, partition values: [empty row]
2023-04-26 13:25:20,354 INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [Executor task launch worker for task 0] Code generated in 18.421701 ms
2023-04-26 13:25:20,567 INFO org.apache.spark.executor.Executor [Executor task launch worker for task 0] Finished task 0.0 in stage 0.0 (TID 0). 1307 bytes result sent to driver
2023-04-26 13:25:20,585 INFO org.apache.spark.scheduler.TaskSetManager [task-result-getter-0] Finished task 0.0 in stage 0.0 (TID 0) in 665 ms on localhost (executor driver) (1/1)
2023-04-26 13:25:20,604 INFO org.apache.spark.scheduler.TaskSchedulerImpl [task-result-getter-0] Removed TaskSet 0.0, whose tasks have all completed, from pool 
2023-04-26 13:25:20,667 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] ResultStage 0 (csv at SparkController.java:55) finished in 1.110 s
2023-04-26 13:25:20,716 INFO org.apache.spark.scheduler.DAGScheduler [http-nio-8080-exec-1] Job 0 finished: csv at SparkController.java:55, took 1.551664 s
2023-04-26 13:25:20,908 INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy [http-nio-8080-exec-1] Pruning directories with: 
2023-04-26 13:25:20,909 INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy [http-nio-8080-exec-1] Post-Scan Filters: 
2023-04-26 13:25:20,910 INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy [http-nio-8080-exec-1] Output Data Schema: struct<value: string>
2023-04-26 13:25:20,911 INFO org.apache.spark.sql.execution.FileSourceScanExec [http-nio-8080-exec-1] Pushed Filters: 
2023-04-26 13:25:20,928 INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [http-nio-8080-exec-1] Code generated in 11.680904 ms
2023-04-26 13:25:20,986 INFO org.apache.spark.storage.memory.MemoryStore [http-nio-8080-exec-1] Block broadcast_2 stored as values in memory (estimated size 112.7 KB, free 998.1 MB)
2023-04-26 13:25:21,005 INFO org.apache.spark.storage.memory.MemoryStore [http-nio-8080-exec-1] Block broadcast_2_piece0 stored as bytes in memory (estimated size 20.7 KB, free 998.1 MB)
2023-04-26 13:25:21,006 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-0] Added broadcast_2_piece0 in memory on 192.168.1.125:44401 (size: 20.7 KB, free: 998.4 MB)
2023-04-26 13:25:21,007 INFO org.apache.spark.SparkContext [http-nio-8080-exec-1] Created broadcast 2 from csv at SparkController.java:55
2023-04-26 13:25:21,008 INFO org.apache.spark.sql.execution.FileSourceScanExec [http-nio-8080-exec-1] Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2023-04-26 13:25:21,188 INFO org.apache.spark.SparkContext [http-nio-8080-exec-1] Starting job: csv at SparkController.java:55
2023-04-26 13:25:21,190 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Got job 1 (csv at SparkController.java:55) with 1 output partitions
2023-04-26 13:25:21,191 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Final stage: ResultStage 1 (csv at SparkController.java:55)
2023-04-26 13:25:21,191 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Parents of final stage: List()
2023-04-26 13:25:21,191 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Missing parents: List()
2023-04-26 13:25:21,192 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Submitting ResultStage 1 (MapPartitionsRDD[9] at csv at SparkController.java:55), which has no missing parents
2023-04-26 13:25:21,199 INFO org.apache.spark.storage.memory.MemoryStore [dag-scheduler-event-loop] Block broadcast_3 stored as values in memory (estimated size 13.9 KB, free 998.1 MB)
2023-04-26 13:25:21,204 INFO org.apache.spark.storage.memory.MemoryStore [dag-scheduler-event-loop] Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.5 KB, free 998.1 MB)
2023-04-26 13:25:21,205 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-1] Added broadcast_3_piece0 in memory on 192.168.1.125:44401 (size: 7.5 KB, free: 998.3 MB)
2023-04-26 13:25:21,206 INFO org.apache.spark.SparkContext [dag-scheduler-event-loop] Created broadcast 3 from broadcast at DAGScheduler.scala:1163
2023-04-26 13:25:21,207 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at csv at SparkController.java:55) (first 15 tasks are for partitions Vector(0))
2023-04-26 13:25:21,207 INFO org.apache.spark.scheduler.TaskSchedulerImpl [dag-scheduler-event-loop] Adding task set 1.0 with 1 tasks
2023-04-26 13:25:21,208 INFO org.apache.spark.scheduler.TaskSetManager [dispatcher-event-loop-2] Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 8266 bytes)
2023-04-26 13:25:21,209 INFO org.apache.spark.executor.Executor [Executor task launch worker for task 1] Running task 0.0 in stage 1.0 (TID 1)
2023-04-26 13:25:21,252 INFO org.apache.spark.sql.execution.datasources.FileScanRDD [Executor task launch worker for task 1] Reading File path: file:///home/inferyx/Documents/Files/addresses.csv, range: 0-328, partition values: [empty row]
2023-04-26 13:25:21,306 INFO org.apache.spark.executor.Executor [Executor task launch worker for task 1] Finished task 0.0 in stage 1.0 (TID 1). 1493 bytes result sent to driver
2023-04-26 13:25:21,310 INFO org.apache.spark.scheduler.TaskSetManager [task-result-getter-1] Finished task 0.0 in stage 1.0 (TID 1) in 102 ms on localhost (executor driver) (1/1)
2023-04-26 13:25:21,311 INFO org.apache.spark.scheduler.TaskSchedulerImpl [task-result-getter-1] Removed TaskSet 1.0, whose tasks have all completed, from pool 
2023-04-26 13:25:21,314 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] ResultStage 1 (csv at SparkController.java:55) finished in 0.118 s
2023-04-26 13:25:21,317 INFO org.apache.spark.scheduler.DAGScheduler [http-nio-8080-exec-1] Job 1 finished: csv at SparkController.java:55, took 0.127678 s
2023-04-26 13:25:21,414 INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy [http-nio-8080-exec-1] Pruning directories with: 
2023-04-26 13:25:21,416 INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy [http-nio-8080-exec-1] Post-Scan Filters: 
2023-04-26 13:25:21,417 INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy [http-nio-8080-exec-1] Output Data Schema: struct<John: string, Doe: string, 120 jefferson st.: string, Riverside: string,  NJ: string ... 1 more field>
2023-04-26 13:25:21,418 INFO org.apache.spark.sql.execution.FileSourceScanExec [http-nio-8080-exec-1] Pushed Filters: 
2023-04-26 13:25:21,496 INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [http-nio-8080-exec-1] Code generated in 33.095648 ms
2023-04-26 13:25:21,553 INFO org.apache.spark.storage.memory.MemoryStore [http-nio-8080-exec-1] Block broadcast_4 stored as values in memory (estimated size 112.7 KB, free 998.0 MB)
2023-04-26 13:25:21,570 INFO org.apache.spark.storage.memory.MemoryStore [http-nio-8080-exec-1] Block broadcast_4_piece0 stored as bytes in memory (estimated size 20.7 KB, free 998.0 MB)
2023-04-26 13:25:21,571 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-1] Added broadcast_4_piece0 in memory on 192.168.1.125:44401 (size: 20.7 KB, free: 998.3 MB)
2023-04-26 13:25:21,572 INFO org.apache.spark.SparkContext [http-nio-8080-exec-1] Created broadcast 4 from show at SparkController.java:58
2023-04-26 13:25:21,577 INFO org.apache.spark.sql.execution.FileSourceScanExec [http-nio-8080-exec-1] Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2023-04-26 13:25:21,659 INFO org.apache.spark.SparkContext [http-nio-8080-exec-1] Starting job: show at SparkController.java:58
2023-04-26 13:25:21,662 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Got job 2 (show at SparkController.java:58) with 1 output partitions
2023-04-26 13:25:21,663 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Final stage: ResultStage 2 (show at SparkController.java:58)
2023-04-26 13:25:21,663 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Parents of final stage: List()
2023-04-26 13:25:21,664 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Missing parents: List()
2023-04-26 13:25:21,664 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Submitting ResultStage 2 (MapPartitionsRDD[13] at show at SparkController.java:58), which has no missing parents
2023-04-26 13:25:21,670 INFO org.apache.spark.storage.memory.MemoryStore [dag-scheduler-event-loop] Block broadcast_5 stored as values in memory (estimated size 10.4 KB, free 998.0 MB)
2023-04-26 13:25:21,675 INFO org.apache.spark.storage.memory.MemoryStore [dag-scheduler-event-loop] Block broadcast_5_piece0 stored as bytes in memory (estimated size 5.7 KB, free 998.0 MB)
2023-04-26 13:25:21,677 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-2] Added broadcast_5_piece0 in memory on 192.168.1.125:44401 (size: 5.7 KB, free: 998.3 MB)
2023-04-26 13:25:21,678 INFO org.apache.spark.SparkContext [dag-scheduler-event-loop] Created broadcast 5 from broadcast at DAGScheduler.scala:1163
2023-04-26 13:25:21,680 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[13] at show at SparkController.java:58) (first 15 tasks are for partitions Vector(0))
2023-04-26 13:25:21,680 INFO org.apache.spark.scheduler.TaskSchedulerImpl [dag-scheduler-event-loop] Adding task set 2.0 with 1 tasks
2023-04-26 13:25:21,683 INFO org.apache.spark.scheduler.TaskSetManager [dispatcher-event-loop-3] Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 8266 bytes)
2023-04-26 13:25:21,683 INFO org.apache.spark.executor.Executor [Executor task launch worker for task 2] Running task 0.0 in stage 2.0 (TID 2)
2023-04-26 13:25:21,695 INFO org.apache.spark.sql.execution.datasources.FileScanRDD [Executor task launch worker for task 2] Reading File path: file:///home/inferyx/Documents/Files/addresses.csv, range: 0-328, partition values: [empty row]
2023-04-26 13:25:21,735 INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [Executor task launch worker for task 2] Code generated in 32.318768 ms
2023-04-26 13:25:21,834 INFO org.apache.spark.executor.Executor [Executor task launch worker for task 2] Finished task 0.0 in stage 2.0 (TID 2). 1677 bytes result sent to driver
2023-04-26 13:25:21,836 INFO org.apache.spark.scheduler.TaskSetManager [task-result-getter-2] Finished task 0.0 in stage 2.0 (TID 2) in 154 ms on localhost (executor driver) (1/1)
2023-04-26 13:25:21,837 INFO org.apache.spark.scheduler.TaskSchedulerImpl [task-result-getter-2] Removed TaskSet 2.0, whose tasks have all completed, from pool 
2023-04-26 13:25:21,838 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] ResultStage 2 (show at SparkController.java:58) finished in 0.172 s
2023-04-26 13:25:21,839 INFO org.apache.spark.scheduler.DAGScheduler [http-nio-8080-exec-1] Job 2 finished: show at SparkController.java:58, took 0.178672 s
2023-04-26 13:25:21,894 INFO com.example.spring.jwt.mongodb.controllers.SparkController [http-nio-8080-exec-1] Reading Csv File
2023-04-26 13:25:22,040 INFO org.apache.spark.sql.execution.datasources.InMemoryFileIndex [http-nio-8080-exec-1] It took 1 ms to list leaf files for 1 paths.
2023-04-26 13:25:22,183 INFO org.apache.spark.sql.execution.datasources.InMemoryFileIndex [http-nio-8080-exec-1] It took 2 ms to list leaf files for 1 paths.
2023-04-26 13:25:22,281 INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy [http-nio-8080-exec-1] Pruning directories with: 
2023-04-26 13:25:22,282 INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy [http-nio-8080-exec-1] Post-Scan Filters: (length(trim(value#53, None)) > 0)
2023-04-26 13:25:22,284 INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy [http-nio-8080-exec-1] Output Data Schema: struct<value: string>
2023-04-26 13:25:22,284 INFO org.apache.spark.sql.execution.FileSourceScanExec [http-nio-8080-exec-1] Pushed Filters: 
2023-04-26 13:25:22,360 INFO org.apache.spark.storage.memory.MemoryStore [http-nio-8080-exec-1] Block broadcast_6 stored as values in memory (estimated size 112.8 KB, free 997.8 MB)
2023-04-26 13:25:22,380 INFO org.apache.spark.storage.memory.MemoryStore [http-nio-8080-exec-1] Block broadcast_6_piece0 stored as bytes in memory (estimated size 20.7 KB, free 997.8 MB)
2023-04-26 13:25:22,382 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-2] Added broadcast_6_piece0 in memory on 192.168.1.125:44401 (size: 20.7 KB, free: 998.3 MB)
2023-04-26 13:25:22,383 INFO org.apache.spark.SparkContext [http-nio-8080-exec-1] Created broadcast 6 from csv at SparkController.java:66
2023-04-26 13:25:22,385 INFO org.apache.spark.sql.execution.FileSourceScanExec [http-nio-8080-exec-1] Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2023-04-26 13:25:22,461 INFO org.apache.spark.SparkContext [http-nio-8080-exec-1] Starting job: csv at SparkController.java:66
2023-04-26 13:25:22,463 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Got job 3 (csv at SparkController.java:66) with 1 output partitions
2023-04-26 13:25:22,464 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Final stage: ResultStage 3 (csv at SparkController.java:66)
2023-04-26 13:25:22,464 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Parents of final stage: List()
2023-04-26 13:25:22,465 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Missing parents: List()
2023-04-26 13:25:22,466 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Submitting ResultStage 3 (MapPartitionsRDD[17] at csv at SparkController.java:66), which has no missing parents
2023-04-26 13:25:22,469 INFO org.apache.spark.storage.memory.MemoryStore [dag-scheduler-event-loop] Block broadcast_7 stored as values in memory (estimated size 8.9 KB, free 997.8 MB)
2023-04-26 13:25:22,476 INFO org.apache.spark.storage.memory.MemoryStore [dag-scheduler-event-loop] Block broadcast_7_piece0 stored as bytes in memory (estimated size 4.6 KB, free 997.8 MB)
2023-04-26 13:25:22,477 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-3] Added broadcast_7_piece0 in memory on 192.168.1.125:44401 (size: 4.6 KB, free: 998.3 MB)
2023-04-26 13:25:22,478 INFO org.apache.spark.SparkContext [dag-scheduler-event-loop] Created broadcast 7 from broadcast at DAGScheduler.scala:1163
2023-04-26 13:25:22,480 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[17] at csv at SparkController.java:66) (first 15 tasks are for partitions Vector(0))
2023-04-26 13:25:22,480 INFO org.apache.spark.scheduler.TaskSchedulerImpl [dag-scheduler-event-loop] Adding task set 3.0 with 1 tasks
2023-04-26 13:25:22,482 INFO org.apache.spark.scheduler.TaskSetManager [dispatcher-event-loop-0] Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 8263 bytes)
2023-04-26 13:25:22,482 INFO org.apache.spark.executor.Executor [Executor task launch worker for task 3] Running task 0.0 in stage 3.0 (TID 3)
2023-04-26 13:25:22,486 INFO org.apache.spark.sql.execution.datasources.FileScanRDD [Executor task launch worker for task 3] Reading File path: file:///home/inferyx/Documents/Files/output.psv, range: 0-340, partition values: [empty row]
2023-04-26 13:25:22,518 INFO org.apache.spark.executor.Executor [Executor task launch worker for task 3] Finished task 0.0 in stage 3.0 (TID 3). 1315 bytes result sent to driver
2023-04-26 13:25:22,520 INFO org.apache.spark.scheduler.TaskSetManager [task-result-getter-3] Finished task 0.0 in stage 3.0 (TID 3) in 39 ms on localhost (executor driver) (1/1)
2023-04-26 13:25:22,520 INFO org.apache.spark.scheduler.TaskSchedulerImpl [task-result-getter-3] Removed TaskSet 3.0, whose tasks have all completed, from pool 
2023-04-26 13:25:22,521 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] ResultStage 3 (csv at SparkController.java:66) finished in 0.054 s
2023-04-26 13:25:22,522 INFO org.apache.spark.scheduler.DAGScheduler [http-nio-8080-exec-1] Job 3 finished: csv at SparkController.java:66, took 0.060424 s
2023-04-26 13:25:22,570 INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy [http-nio-8080-exec-1] Pruning directories with: 
2023-04-26 13:25:22,571 INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy [http-nio-8080-exec-1] Post-Scan Filters: 
2023-04-26 13:25:22,571 INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy [http-nio-8080-exec-1] Output Data Schema: struct<value: string>
2023-04-26 13:25:22,572 INFO org.apache.spark.sql.execution.FileSourceScanExec [http-nio-8080-exec-1] Pushed Filters: 
2023-04-26 13:25:22,650 INFO org.apache.spark.storage.memory.MemoryStore [http-nio-8080-exec-1] Block broadcast_8 stored as values in memory (estimated size 112.8 KB, free 997.7 MB)
2023-04-26 13:25:22,673 INFO org.apache.spark.storage.memory.MemoryStore [http-nio-8080-exec-1] Block broadcast_8_piece0 stored as bytes in memory (estimated size 20.7 KB, free 997.7 MB)
2023-04-26 13:25:22,675 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-3] Added broadcast_8_piece0 in memory on 192.168.1.125:44401 (size: 20.7 KB, free: 998.3 MB)
2023-04-26 13:25:22,677 INFO org.apache.spark.SparkContext [http-nio-8080-exec-1] Created broadcast 8 from csv at SparkController.java:66
2023-04-26 13:25:22,678 INFO org.apache.spark.sql.execution.FileSourceScanExec [http-nio-8080-exec-1] Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2023-04-26 13:25:22,713 INFO org.apache.spark.SparkContext [http-nio-8080-exec-1] Starting job: csv at SparkController.java:66
2023-04-26 13:25:22,715 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Got job 4 (csv at SparkController.java:66) with 1 output partitions
2023-04-26 13:25:22,715 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Final stage: ResultStage 4 (csv at SparkController.java:66)
2023-04-26 13:25:22,715 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Parents of final stage: List()
2023-04-26 13:25:22,716 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Missing parents: List()
2023-04-26 13:25:22,717 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Submitting ResultStage 4 (MapPartitionsRDD[23] at csv at SparkController.java:66), which has no missing parents
2023-04-26 13:25:22,723 INFO org.apache.spark.storage.memory.MemoryStore [dag-scheduler-event-loop] Block broadcast_9 stored as values in memory (estimated size 13.9 KB, free 997.7 MB)
2023-04-26 13:25:22,729 INFO org.apache.spark.storage.memory.MemoryStore [dag-scheduler-event-loop] Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.6 KB, free 997.7 MB)
2023-04-26 13:25:22,730 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-0] Added broadcast_9_piece0 in memory on 192.168.1.125:44401 (size: 7.6 KB, free: 998.3 MB)
2023-04-26 13:25:22,731 INFO org.apache.spark.SparkContext [dag-scheduler-event-loop] Created broadcast 9 from broadcast at DAGScheduler.scala:1163
2023-04-26 13:25:22,732 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[23] at csv at SparkController.java:66) (first 15 tasks are for partitions Vector(0))
2023-04-26 13:25:22,732 INFO org.apache.spark.scheduler.TaskSchedulerImpl [dag-scheduler-event-loop] Adding task set 4.0 with 1 tasks
2023-04-26 13:25:22,734 INFO org.apache.spark.scheduler.TaskSetManager [dispatcher-event-loop-1] Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, PROCESS_LOCAL, 8263 bytes)
2023-04-26 13:25:22,735 INFO org.apache.spark.executor.Executor [Executor task launch worker for task 4] Running task 0.0 in stage 4.0 (TID 4)
2023-04-26 13:25:22,743 INFO org.apache.spark.sql.execution.datasources.FileScanRDD [Executor task launch worker for task 4] Reading File path: file:///home/inferyx/Documents/Files/output.psv, range: 0-340, partition values: [empty row]
2023-04-26 13:25:22,758 INFO org.apache.spark.executor.Executor [Executor task launch worker for task 4] Finished task 0.0 in stage 4.0 (TID 4). 1586 bytes result sent to driver
2023-04-26 13:25:22,816 INFO org.apache.spark.scheduler.TaskSetManager [task-result-getter-0] Finished task 0.0 in stage 4.0 (TID 4) in 82 ms on localhost (executor driver) (1/1)
2023-04-26 13:25:22,817 INFO org.apache.spark.scheduler.TaskSchedulerImpl [task-result-getter-0] Removed TaskSet 4.0, whose tasks have all completed, from pool 
2023-04-26 13:25:22,820 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] ResultStage 4 (csv at SparkController.java:66) finished in 0.101 s
2023-04-26 13:25:22,823 INFO org.apache.spark.scheduler.DAGScheduler [http-nio-8080-exec-1] Job 4 finished: csv at SparkController.java:66, took 0.109433 s
2023-04-26 13:25:23,152 INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy [http-nio-8080-exec-1] Pruning directories with: 
2023-04-26 13:25:23,154 INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy [http-nio-8080-exec-1] Post-Scan Filters: 
2023-04-26 13:25:23,155 INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy [http-nio-8080-exec-1] Output Data Schema: struct< John: string, Doe: string, 120 jefferson st.: string, Riverside: string,  NJ: string ... 1 more field>
2023-04-26 13:25:23,155 INFO org.apache.spark.sql.execution.FileSourceScanExec [http-nio-8080-exec-1] Pushed Filters: 
2023-04-26 13:25:23,176 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 58
2023-04-26 13:25:23,177 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 39
2023-04-26 13:25:23,178 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 67
2023-04-26 13:25:23,178 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 75
2023-04-26 13:25:23,178 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 44
2023-04-26 13:25:23,290 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-2] Removed broadcast_5_piece0 on 192.168.1.125:44401 in memory (size: 5.7 KB, free: 998.3 MB)
2023-04-26 13:25:23,310 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 2
2023-04-26 13:25:23,311 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 146
2023-04-26 13:25:23,311 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 72
2023-04-26 13:25:23,315 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-1] Removed broadcast_1_piece0 on 192.168.1.125:44401 in memory (size: 4.6 KB, free: 998.3 MB)
2023-04-26 13:25:23,321 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-2] Removed broadcast_7_piece0 on 192.168.1.125:44401 in memory (size: 4.6 KB, free: 998.3 MB)
2023-04-26 13:25:23,331 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 113
2023-04-26 13:25:23,331 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 24
2023-04-26 13:25:23,331 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 47
2023-04-26 13:25:23,340 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-0] Removed broadcast_9_piece0 on 192.168.1.125:44401 in memory (size: 7.6 KB, free: 998.3 MB)
2023-04-26 13:25:23,346 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 38
2023-04-26 13:25:23,349 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 11
2023-04-26 13:25:23,349 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 148
2023-04-26 13:25:23,349 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 100
2023-04-26 13:25:23,349 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 4
2023-04-26 13:25:23,350 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 143
2023-04-26 13:25:23,350 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 98
2023-04-26 13:25:23,350 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 99
2023-04-26 13:25:23,350 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 23
2023-04-26 13:25:23,350 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 134
2023-04-26 13:25:23,350 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 50
2023-04-26 13:25:23,350 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 106
2023-04-26 13:25:23,351 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 34
2023-04-26 13:25:23,351 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 109
2023-04-26 13:25:23,351 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 26
2023-04-26 13:25:23,351 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 123
2023-04-26 13:25:23,352 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 37
2023-04-26 13:25:23,352 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 25
2023-04-26 13:25:23,352 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 3
2023-04-26 13:25:23,352 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 0
2023-04-26 13:25:23,352 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 79
2023-04-26 13:25:23,352 INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [http-nio-8080-exec-1] Code generated in 56.155271 ms
2023-04-26 13:25:23,353 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 137
2023-04-26 13:25:23,353 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 27
2023-04-26 13:25:23,353 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 54
2023-04-26 13:25:23,353 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 115
2023-04-26 13:25:23,354 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 147
2023-04-26 13:25:23,354 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 32
2023-04-26 13:25:23,354 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 145
2023-04-26 13:25:23,354 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 1
2023-04-26 13:25:23,355 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 43
2023-04-26 13:25:23,355 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 31
2023-04-26 13:25:23,355 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 57
2023-04-26 13:25:23,355 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 139
2023-04-26 13:25:23,355 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 131
2023-04-26 13:25:23,356 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 77
2023-04-26 13:25:23,356 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 141
2023-04-26 13:25:23,356 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 5
2023-04-26 13:25:23,356 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 112
2023-04-26 13:25:23,356 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 118
2023-04-26 13:25:23,360 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-1] Removed broadcast_0_piece0 on 192.168.1.125:44401 in memory (size: 20.7 KB, free: 998.3 MB)
2023-04-26 13:25:23,364 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 105
2023-04-26 13:25:23,364 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 86
2023-04-26 13:25:23,364 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 8
2023-04-26 13:25:23,364 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 117
2023-04-26 13:25:23,364 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 124
2023-04-26 13:25:23,364 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 40
2023-04-26 13:25:23,364 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 60
2023-04-26 13:25:23,364 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 53
2023-04-26 13:25:23,367 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-3] Removed broadcast_3_piece0 on 192.168.1.125:44401 in memory (size: 7.5 KB, free: 998.3 MB)
2023-04-26 13:25:23,370 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 91
2023-04-26 13:25:23,370 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 46
2023-04-26 13:25:23,370 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 87
2023-04-26 13:25:23,371 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 74
2023-04-26 13:25:23,371 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 138
2023-04-26 13:25:23,371 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 29
2023-04-26 13:25:23,375 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-0] Removed broadcast_4_piece0 on 192.168.1.125:44401 in memory (size: 20.7 KB, free: 998.3 MB)
2023-04-26 13:25:23,383 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 133
2023-04-26 13:25:23,384 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 51
2023-04-26 13:25:23,384 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 95
2023-04-26 13:25:23,384 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 144
2023-04-26 13:25:23,384 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 48
2023-04-26 13:25:23,384 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 6
2023-04-26 13:25:23,384 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 136
2023-04-26 13:25:23,385 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 17
2023-04-26 13:25:23,385 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 149
2023-04-26 13:25:23,385 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 41
2023-04-26 13:25:23,385 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 96
2023-04-26 13:25:23,385 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 59
2023-04-26 13:25:23,385 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 111
2023-04-26 13:25:23,385 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 83
2023-04-26 13:25:23,393 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-2] Removed broadcast_8_piece0 on 192.168.1.125:44401 in memory (size: 20.7 KB, free: 998.4 MB)
2023-04-26 13:25:23,397 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 101
2023-04-26 13:25:23,397 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 33
2023-04-26 13:25:23,397 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 14
2023-04-26 13:25:23,397 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 73
2023-04-26 13:25:23,397 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 151
2023-04-26 13:25:23,398 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 20
2023-04-26 13:25:23,398 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 68
2023-04-26 13:25:23,398 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 128
2023-04-26 13:25:23,398 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 94
2023-04-26 13:25:23,399 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 107
2023-04-26 13:25:23,399 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 122
2023-04-26 13:25:23,399 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 97
2023-04-26 13:25:23,400 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 62
2023-04-26 13:25:23,400 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 135
2023-04-26 13:25:23,400 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 36
2023-04-26 13:25:23,400 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 63
2023-04-26 13:25:23,400 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 7
2023-04-26 13:25:23,400 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 126
2023-04-26 13:25:23,400 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 140
2023-04-26 13:25:23,401 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 132
2023-04-26 13:25:23,401 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 13
2023-04-26 13:25:23,401 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 84
2023-04-26 13:25:23,401 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 16
2023-04-26 13:25:23,401 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 88
2023-04-26 13:25:23,401 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 102
2023-04-26 13:25:23,401 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 114
2023-04-26 13:25:23,401 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 82
2023-04-26 13:25:23,402 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 108
2023-04-26 13:25:23,402 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 12
2023-04-26 13:25:23,402 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 55
2023-04-26 13:25:23,402 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 85
2023-04-26 13:25:23,402 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 22
2023-04-26 13:25:23,402 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 49
2023-04-26 13:25:23,402 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 66
2023-04-26 13:25:23,406 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-3] Removed broadcast_6_piece0 on 192.168.1.125:44401 in memory (size: 20.7 KB, free: 998.4 MB)
2023-04-26 13:25:23,418 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 42
2023-04-26 13:25:23,418 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 30
2023-04-26 13:25:23,419 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 89
2023-04-26 13:25:23,419 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 103
2023-04-26 13:25:23,419 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 56
2023-04-26 13:25:23,419 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 116
2023-04-26 13:25:23,419 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 90
2023-04-26 13:25:23,419 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 15
2023-04-26 13:25:23,420 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 9
2023-04-26 13:25:23,420 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 18
2023-04-26 13:25:23,420 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 61
2023-04-26 13:25:23,420 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 142
2023-04-26 13:25:23,420 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 28
2023-04-26 13:25:23,420 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 70
2023-04-26 13:25:23,420 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 52
2023-04-26 13:25:23,420 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 19
2023-04-26 13:25:23,420 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 119
2023-04-26 13:25:23,421 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 71
2023-04-26 13:25:23,420 INFO org.apache.spark.storage.memory.MemoryStore [http-nio-8080-exec-1] Block broadcast_10 stored as values in memory (estimated size 112.8 KB, free 998.2 MB)
2023-04-26 13:25:23,423 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-1] Removed broadcast_2_piece0 on 192.168.1.125:44401 in memory (size: 20.7 KB, free: 998.4 MB)
2023-04-26 13:25:23,430 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 129
2023-04-26 13:25:23,431 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 93
2023-04-26 13:25:23,431 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 120
2023-04-26 13:25:23,432 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 35
2023-04-26 13:25:23,432 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 127
2023-04-26 13:25:23,432 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 80
2023-04-26 13:25:23,432 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 150
2023-04-26 13:25:23,432 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 10
2023-04-26 13:25:23,433 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 125
2023-04-26 13:25:23,433 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 130
2023-04-26 13:25:23,433 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 110
2023-04-26 13:25:23,433 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 121
2023-04-26 13:25:23,433 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 65
2023-04-26 13:25:23,433 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 69
2023-04-26 13:25:23,433 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 78
2023-04-26 13:25:23,434 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 92
2023-04-26 13:25:23,434 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 64
2023-04-26 13:25:23,434 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 76
2023-04-26 13:25:23,434 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 104
2023-04-26 13:25:23,434 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 21
2023-04-26 13:25:23,434 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 45
2023-04-26 13:25:23,434 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 81
2023-04-26 13:25:23,439 INFO org.apache.spark.storage.memory.MemoryStore [http-nio-8080-exec-1] Block broadcast_10_piece0 stored as bytes in memory (estimated size 20.7 KB, free 998.3 MB)
2023-04-26 13:25:23,441 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-3] Added broadcast_10_piece0 in memory on 192.168.1.125:44401 (size: 20.7 KB, free: 998.4 MB)
2023-04-26 13:25:23,443 INFO org.apache.spark.SparkContext [http-nio-8080-exec-1] Created broadcast 10 from show at SparkController.java:69
2023-04-26 13:25:23,445 INFO org.apache.spark.sql.execution.FileSourceScanExec [http-nio-8080-exec-1] Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2023-04-26 13:25:23,537 INFO org.apache.spark.SparkContext [http-nio-8080-exec-1] Starting job: show at SparkController.java:69
2023-04-26 13:25:23,538 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Got job 5 (show at SparkController.java:69) with 1 output partitions
2023-04-26 13:25:23,539 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Final stage: ResultStage 5 (show at SparkController.java:69)
2023-04-26 13:25:23,539 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Parents of final stage: List()
2023-04-26 13:25:23,539 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Missing parents: List()
2023-04-26 13:25:23,540 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Submitting ResultStage 5 (MapPartitionsRDD[27] at show at SparkController.java:69), which has no missing parents
2023-04-26 13:25:23,544 INFO org.apache.spark.storage.memory.MemoryStore [dag-scheduler-event-loop] Block broadcast_11 stored as values in memory (estimated size 12.6 KB, free 998.3 MB)
2023-04-26 13:25:23,546 INFO org.apache.spark.storage.memory.MemoryStore [dag-scheduler-event-loop] Block broadcast_11_piece0 stored as bytes in memory (estimated size 6.3 KB, free 998.3 MB)
2023-04-26 13:25:23,547 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-2] Added broadcast_11_piece0 in memory on 192.168.1.125:44401 (size: 6.3 KB, free: 998.4 MB)
2023-04-26 13:25:23,548 INFO org.apache.spark.SparkContext [dag-scheduler-event-loop] Created broadcast 11 from broadcast at DAGScheduler.scala:1163
2023-04-26 13:25:23,549 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[27] at show at SparkController.java:69) (first 15 tasks are for partitions Vector(0))
2023-04-26 13:25:23,549 INFO org.apache.spark.scheduler.TaskSchedulerImpl [dag-scheduler-event-loop] Adding task set 5.0 with 1 tasks
2023-04-26 13:25:23,552 INFO org.apache.spark.scheduler.TaskSetManager [dispatcher-event-loop-0] Starting task 0.0 in stage 5.0 (TID 5, localhost, executor driver, partition 0, PROCESS_LOCAL, 8263 bytes)
2023-04-26 13:25:23,553 INFO org.apache.spark.executor.Executor [Executor task launch worker for task 5] Running task 0.0 in stage 5.0 (TID 5)
2023-04-26 13:25:23,562 INFO org.apache.spark.sql.execution.datasources.FileScanRDD [Executor task launch worker for task 5] Reading File path: file:///home/inferyx/Documents/Files/output.psv, range: 0-340, partition values: [empty row]
2023-04-26 13:25:23,594 INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [Executor task launch worker for task 5] Code generated in 27.081323 ms
2023-04-26 13:25:23,607 INFO org.apache.spark.executor.Executor [Executor task launch worker for task 5] Finished task 0.0 in stage 5.0 (TID 5). 1695 bytes result sent to driver
2023-04-26 13:25:23,608 INFO org.apache.spark.scheduler.TaskSetManager [task-result-getter-1] Finished task 0.0 in stage 5.0 (TID 5) in 56 ms on localhost (executor driver) (1/1)
2023-04-26 13:25:23,609 INFO org.apache.spark.scheduler.TaskSchedulerImpl [task-result-getter-1] Removed TaskSet 5.0, whose tasks have all completed, from pool 
2023-04-26 13:25:23,611 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] ResultStage 5 (show at SparkController.java:69) finished in 0.070 s
2023-04-26 13:25:23,615 INFO org.apache.spark.scheduler.DAGScheduler [http-nio-8080-exec-1] Job 5 finished: show at SparkController.java:69, took 0.077122 s
2023-04-26 13:25:23,618 INFO com.example.spring.jwt.mongodb.controllers.SparkController [http-nio-8080-exec-1] Reading Psv File
2023-04-26 13:25:23,800 INFO org.apache.spark.sql.execution.datasources.InMemoryFileIndex [http-nio-8080-exec-1] It took 2 ms to list leaf files for 1 paths.
2023-04-26 13:25:23,946 INFO org.apache.spark.sql.execution.datasources.InMemoryFileIndex [http-nio-8080-exec-1] It took 2 ms to list leaf files for 1 paths.
2023-04-26 13:25:24,230 INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy [http-nio-8080-exec-1] Pruning directories with: 
2023-04-26 13:25:24,231 INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy [http-nio-8080-exec-1] Post-Scan Filters: (length(trim(value#106, None)) > 0)
2023-04-26 13:25:24,232 INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy [http-nio-8080-exec-1] Output Data Schema: struct<value: string>
2023-04-26 13:25:24,233 INFO org.apache.spark.sql.execution.FileSourceScanExec [http-nio-8080-exec-1] Pushed Filters: 
2023-04-26 13:25:24,298 INFO org.apache.spark.storage.memory.MemoryStore [http-nio-8080-exec-1] Block broadcast_12 stored as values in memory (estimated size 112.8 KB, free 998.1 MB)
2023-04-26 13:25:24,313 INFO org.apache.spark.storage.memory.MemoryStore [http-nio-8080-exec-1] Block broadcast_12_piece0 stored as bytes in memory (estimated size 20.7 KB, free 998.1 MB)
2023-04-26 13:25:24,315 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-1] Added broadcast_12_piece0 in memory on 192.168.1.125:44401 (size: 20.7 KB, free: 998.4 MB)
2023-04-26 13:25:24,316 INFO org.apache.spark.SparkContext [http-nio-8080-exec-1] Created broadcast 12 from csv at SparkController.java:76
2023-04-26 13:25:24,317 INFO org.apache.spark.sql.execution.FileSourceScanExec [http-nio-8080-exec-1] Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2023-04-26 13:25:24,396 INFO org.apache.spark.SparkContext [http-nio-8080-exec-1] Starting job: csv at SparkController.java:76
2023-04-26 13:25:24,398 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Got job 6 (csv at SparkController.java:76) with 1 output partitions
2023-04-26 13:25:24,399 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Final stage: ResultStage 6 (csv at SparkController.java:76)
2023-04-26 13:25:24,399 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Parents of final stage: List()
2023-04-26 13:25:24,399 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Missing parents: List()
2023-04-26 13:25:24,400 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Submitting ResultStage 6 (MapPartitionsRDD[31] at csv at SparkController.java:76), which has no missing parents
2023-04-26 13:25:24,405 INFO org.apache.spark.storage.memory.MemoryStore [dag-scheduler-event-loop] Block broadcast_13 stored as values in memory (estimated size 8.9 KB, free 998.1 MB)
2023-04-26 13:25:24,407 INFO org.apache.spark.storage.memory.MemoryStore [dag-scheduler-event-loop] Block broadcast_13_piece0 stored as bytes in memory (estimated size 4.6 KB, free 998.1 MB)
2023-04-26 13:25:24,410 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-3] Added broadcast_13_piece0 in memory on 192.168.1.125:44401 (size: 4.6 KB, free: 998.3 MB)
2023-04-26 13:25:24,410 INFO org.apache.spark.SparkContext [dag-scheduler-event-loop] Created broadcast 13 from broadcast at DAGScheduler.scala:1163
2023-04-26 13:25:24,413 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[31] at csv at SparkController.java:76) (first 15 tasks are for partitions Vector(0))
2023-04-26 13:25:24,413 INFO org.apache.spark.scheduler.TaskSchedulerImpl [dag-scheduler-event-loop] Adding task set 6.0 with 1 tasks
2023-04-26 13:25:24,416 INFO org.apache.spark.scheduler.TaskSetManager [dispatcher-event-loop-2] Starting task 0.0 in stage 6.0 (TID 6, localhost, executor driver, partition 0, PROCESS_LOCAL, 8268 bytes)
2023-04-26 13:25:24,417 INFO org.apache.spark.executor.Executor [Executor task launch worker for task 6] Running task 0.0 in stage 6.0 (TID 6)
2023-04-26 13:25:24,422 INFO org.apache.spark.sql.execution.datasources.FileScanRDD [Executor task launch worker for task 6] Reading File path: file:///home/inferyx/Documents/Files/mlb_players.tsv, range: 0-61049, partition values: [empty row]
2023-04-26 13:25:24,488 INFO org.apache.spark.executor.Executor [Executor task launch worker for task 6] Finished task 0.0 in stage 6.0 (TID 6). 1325 bytes result sent to driver
2023-04-26 13:25:24,489 INFO org.apache.spark.scheduler.TaskSetManager [task-result-getter-2] Finished task 0.0 in stage 6.0 (TID 6) in 74 ms on localhost (executor driver) (1/1)
2023-04-26 13:25:24,489 INFO org.apache.spark.scheduler.TaskSchedulerImpl [task-result-getter-2] Removed TaskSet 6.0, whose tasks have all completed, from pool 
2023-04-26 13:25:24,490 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] ResultStage 6 (csv at SparkController.java:76) finished in 0.088 s
2023-04-26 13:25:24,492 INFO org.apache.spark.scheduler.DAGScheduler [http-nio-8080-exec-1] Job 6 finished: csv at SparkController.java:76, took 0.094761 s
2023-04-26 13:25:24,527 INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy [http-nio-8080-exec-1] Pruning directories with: 
2023-04-26 13:25:24,528 INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy [http-nio-8080-exec-1] Post-Scan Filters: 
2023-04-26 13:25:24,529 INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy [http-nio-8080-exec-1] Output Data Schema: struct<value: string>
2023-04-26 13:25:24,530 INFO org.apache.spark.sql.execution.FileSourceScanExec [http-nio-8080-exec-1] Pushed Filters: 
2023-04-26 13:25:24,589 INFO org.apache.spark.storage.memory.MemoryStore [http-nio-8080-exec-1] Block broadcast_14 stored as values in memory (estimated size 112.8 KB, free 998.0 MB)
2023-04-26 13:25:24,605 INFO org.apache.spark.storage.memory.MemoryStore [http-nio-8080-exec-1] Block broadcast_14_piece0 stored as bytes in memory (estimated size 20.7 KB, free 998.0 MB)
2023-04-26 13:25:24,606 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-3] Added broadcast_14_piece0 in memory on 192.168.1.125:44401 (size: 20.7 KB, free: 998.3 MB)
2023-04-26 13:25:24,607 INFO org.apache.spark.SparkContext [http-nio-8080-exec-1] Created broadcast 14 from csv at SparkController.java:76
2023-04-26 13:25:24,608 INFO org.apache.spark.sql.execution.FileSourceScanExec [http-nio-8080-exec-1] Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2023-04-26 13:25:24,632 INFO org.apache.spark.SparkContext [http-nio-8080-exec-1] Starting job: csv at SparkController.java:76
2023-04-26 13:25:24,633 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Got job 7 (csv at SparkController.java:76) with 1 output partitions
2023-04-26 13:25:24,634 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Final stage: ResultStage 7 (csv at SparkController.java:76)
2023-04-26 13:25:24,634 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Parents of final stage: List()
2023-04-26 13:25:24,634 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Missing parents: List()
2023-04-26 13:25:24,635 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Submitting ResultStage 7 (MapPartitionsRDD[37] at csv at SparkController.java:76), which has no missing parents
2023-04-26 13:25:24,641 INFO org.apache.spark.storage.memory.MemoryStore [dag-scheduler-event-loop] Block broadcast_15 stored as values in memory (estimated size 14.0 KB, free 998.0 MB)
2023-04-26 13:25:24,644 INFO org.apache.spark.storage.memory.MemoryStore [dag-scheduler-event-loop] Block broadcast_15_piece0 stored as bytes in memory (estimated size 7.6 KB, free 998.0 MB)
2023-04-26 13:25:24,645 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-2] Added broadcast_15_piece0 in memory on 192.168.1.125:44401 (size: 7.6 KB, free: 998.3 MB)
2023-04-26 13:25:24,646 INFO org.apache.spark.SparkContext [dag-scheduler-event-loop] Created broadcast 15 from broadcast at DAGScheduler.scala:1163
2023-04-26 13:25:24,647 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[37] at csv at SparkController.java:76) (first 15 tasks are for partitions Vector(0))
2023-04-26 13:25:24,648 INFO org.apache.spark.scheduler.TaskSchedulerImpl [dag-scheduler-event-loop] Adding task set 7.0 with 1 tasks
2023-04-26 13:25:24,649 INFO org.apache.spark.scheduler.TaskSetManager [dispatcher-event-loop-0] Starting task 0.0 in stage 7.0 (TID 7, localhost, executor driver, partition 0, PROCESS_LOCAL, 8268 bytes)
2023-04-26 13:25:24,650 INFO org.apache.spark.executor.Executor [Executor task launch worker for task 7] Running task 0.0 in stage 7.0 (TID 7)
2023-04-26 13:25:24,659 INFO org.apache.spark.sql.execution.datasources.FileScanRDD [Executor task launch worker for task 7] Reading File path: file:///home/inferyx/Documents/Files/mlb_players.tsv, range: 0-61049, partition values: [empty row]
2023-04-26 13:25:24,793 INFO org.apache.spark.executor.Executor [Executor task launch worker for task 7] Finished task 0.0 in stage 7.0 (TID 7). 1637 bytes result sent to driver
2023-04-26 13:25:24,795 INFO org.apache.spark.scheduler.TaskSetManager [task-result-getter-3] Finished task 0.0 in stage 7.0 (TID 7) in 146 ms on localhost (executor driver) (1/1)
2023-04-26 13:25:24,795 INFO org.apache.spark.scheduler.TaskSchedulerImpl [task-result-getter-3] Removed TaskSet 7.0, whose tasks have all completed, from pool 
2023-04-26 13:25:24,796 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] ResultStage 7 (csv at SparkController.java:76) finished in 0.158 s
2023-04-26 13:25:24,798 INFO org.apache.spark.scheduler.DAGScheduler [http-nio-8080-exec-1] Job 7 finished: csv at SparkController.java:76, took 0.164981 s
2023-04-26 13:25:24,879 INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy [http-nio-8080-exec-1] Pruning directories with: 
2023-04-26 13:25:24,881 INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy [http-nio-8080-exec-1] Post-Scan Filters: 
2023-04-26 13:25:24,882 INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy [http-nio-8080-exec-1] Output Data Schema: struct<Name: string, " ""Team""": string, " ""Position""": string, " ""Height(inches)""": int, " ""Weight(lbs)""": string ... 1 more field>
2023-04-26 13:25:24,883 INFO org.apache.spark.sql.execution.FileSourceScanExec [http-nio-8080-exec-1] Pushed Filters: 
2023-04-26 13:25:24,945 INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [http-nio-8080-exec-1] Code generated in 30.885979 ms
2023-04-26 13:25:25,006 INFO org.apache.spark.storage.memory.MemoryStore [http-nio-8080-exec-1] Block broadcast_16 stored as values in memory (estimated size 112.8 KB, free 997.8 MB)
2023-04-26 13:25:25,023 INFO org.apache.spark.storage.memory.MemoryStore [http-nio-8080-exec-1] Block broadcast_16_piece0 stored as bytes in memory (estimated size 20.7 KB, free 997.8 MB)
2023-04-26 13:25:25,025 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-2] Added broadcast_16_piece0 in memory on 192.168.1.125:44401 (size: 20.7 KB, free: 998.3 MB)
2023-04-26 13:25:25,027 INFO org.apache.spark.SparkContext [http-nio-8080-exec-1] Created broadcast 16 from show at SparkController.java:79
2023-04-26 13:25:25,030 INFO org.apache.spark.sql.execution.FileSourceScanExec [http-nio-8080-exec-1] Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2023-04-26 13:25:25,096 INFO org.apache.spark.SparkContext [http-nio-8080-exec-1] Starting job: show at SparkController.java:79
2023-04-26 13:25:25,097 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Got job 8 (show at SparkController.java:79) with 1 output partitions
2023-04-26 13:25:25,098 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Final stage: ResultStage 8 (show at SparkController.java:79)
2023-04-26 13:25:25,098 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Parents of final stage: List()
2023-04-26 13:25:25,098 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Missing parents: List()
2023-04-26 13:25:25,099 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Submitting ResultStage 8 (MapPartitionsRDD[41] at show at SparkController.java:79), which has no missing parents
2023-04-26 13:25:25,102 INFO org.apache.spark.storage.memory.MemoryStore [dag-scheduler-event-loop] Block broadcast_17 stored as values in memory (estimated size 12.8 KB, free 997.8 MB)
2023-04-26 13:25:25,104 INFO org.apache.spark.storage.memory.MemoryStore [dag-scheduler-event-loop] Block broadcast_17_piece0 stored as bytes in memory (estimated size 6.5 KB, free 997.8 MB)
2023-04-26 13:25:25,105 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-0] Added broadcast_17_piece0 in memory on 192.168.1.125:44401 (size: 6.5 KB, free: 998.3 MB)
2023-04-26 13:25:25,106 INFO org.apache.spark.SparkContext [dag-scheduler-event-loop] Created broadcast 17 from broadcast at DAGScheduler.scala:1163
2023-04-26 13:25:25,107 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[41] at show at SparkController.java:79) (first 15 tasks are for partitions Vector(0))
2023-04-26 13:25:25,107 INFO org.apache.spark.scheduler.TaskSchedulerImpl [dag-scheduler-event-loop] Adding task set 8.0 with 1 tasks
2023-04-26 13:25:25,109 INFO org.apache.spark.scheduler.TaskSetManager [dispatcher-event-loop-1] Starting task 0.0 in stage 8.0 (TID 8, localhost, executor driver, partition 0, PROCESS_LOCAL, 8268 bytes)
2023-04-26 13:25:25,110 INFO org.apache.spark.executor.Executor [Executor task launch worker for task 8] Running task 0.0 in stage 8.0 (TID 8)
2023-04-26 13:25:25,117 INFO org.apache.spark.sql.execution.datasources.FileScanRDD [Executor task launch worker for task 8] Reading File path: file:///home/inferyx/Documents/Files/mlb_players.tsv, range: 0-61049, partition values: [empty row]
2023-04-26 13:25:25,138 INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [Executor task launch worker for task 8] Code generated in 17.520599 ms
2023-04-26 13:25:25,151 INFO org.apache.spark.executor.Executor [Executor task launch worker for task 8] Finished task 0.0 in stage 8.0 (TID 8). 2219 bytes result sent to driver
2023-04-26 13:25:25,152 INFO org.apache.spark.scheduler.TaskSetManager [task-result-getter-0] Finished task 0.0 in stage 8.0 (TID 8) in 44 ms on localhost (executor driver) (1/1)
2023-04-26 13:25:25,153 INFO org.apache.spark.scheduler.TaskSchedulerImpl [task-result-getter-0] Removed TaskSet 8.0, whose tasks have all completed, from pool 
2023-04-26 13:25:25,154 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] ResultStage 8 (show at SparkController.java:79) finished in 0.054 s
2023-04-26 13:25:25,155 INFO org.apache.spark.scheduler.DAGScheduler [http-nio-8080-exec-1] Job 8 finished: show at SparkController.java:79, took 0.058261 s
2023-04-26 13:25:25,162 INFO com.example.spring.jwt.mongodb.controllers.SparkController [http-nio-8080-exec-1] Reading Psv File
2023-04-26 13:25:25,359 INFO org.apache.spark.sql.execution.datasources.InMemoryFileIndex [http-nio-8080-exec-1] It took 1 ms to list leaf files for 1 paths.
2023-04-26 13:25:25,555 INFO org.apache.spark.storage.memory.MemoryStore [http-nio-8080-exec-1] Block broadcast_18 stored as values in memory (estimated size 168.0 KB, free 997.6 MB)
2023-04-26 13:25:25,570 INFO org.apache.spark.storage.memory.MemoryStore [http-nio-8080-exec-1] Block broadcast_18_piece0 stored as bytes in memory (estimated size 20.9 KB, free 997.6 MB)
2023-04-26 13:25:25,571 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-0] Added broadcast_18_piece0 in memory on 192.168.1.125:44401 (size: 20.9 KB, free: 998.3 MB)
2023-04-26 13:25:25,572 INFO org.apache.spark.SparkContext [http-nio-8080-exec-1] Created broadcast 18 from json at SparkController.java:87
2023-04-26 13:25:25,674 INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat [http-nio-8080-exec-1] Total input paths to process : 1
2023-04-26 13:25:25,695 INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat [http-nio-8080-exec-1] Total input paths to process : 1
2023-04-26 13:25:25,748 INFO org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat [http-nio-8080-exec-1] DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 308
2023-04-26 13:25:25,764 INFO org.apache.spark.SparkContext [http-nio-8080-exec-1] Starting job: json at SparkController.java:87
2023-04-26 13:25:25,765 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Got job 9 (json at SparkController.java:87) with 1 output partitions
2023-04-26 13:25:25,765 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Final stage: ResultStage 9 (json at SparkController.java:87)
2023-04-26 13:25:25,766 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Parents of final stage: List()
2023-04-26 13:25:25,766 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Missing parents: List()
2023-04-26 13:25:25,766 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Submitting ResultStage 9 (MapPartitionsRDD[44] at json at SparkController.java:87), which has no missing parents
2023-04-26 13:25:25,778 INFO org.apache.spark.storage.memory.MemoryStore [dag-scheduler-event-loop] Block broadcast_19 stored as values in memory (estimated size 6.0 KB, free 997.6 MB)
2023-04-26 13:25:25,782 INFO org.apache.spark.storage.memory.MemoryStore [dag-scheduler-event-loop] Block broadcast_19_piece0 stored as bytes in memory (estimated size 3.6 KB, free 997.6 MB)
2023-04-26 13:25:25,783 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-1] Added broadcast_19_piece0 in memory on 192.168.1.125:44401 (size: 3.6 KB, free: 998.3 MB)
2023-04-26 13:25:25,784 INFO org.apache.spark.SparkContext [dag-scheduler-event-loop] Created broadcast 19 from broadcast at DAGScheduler.scala:1163
2023-04-26 13:25:25,785 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[44] at json at SparkController.java:87) (first 15 tasks are for partitions Vector(0))
2023-04-26 13:25:25,785 INFO org.apache.spark.scheduler.TaskSchedulerImpl [dag-scheduler-event-loop] Adding task set 9.0 with 1 tasks
2023-04-26 13:25:25,823 INFO org.apache.spark.scheduler.TaskSetManager [dispatcher-event-loop-3] Starting task 0.0 in stage 9.0 (TID 9, localhost, executor driver, partition 0, PROCESS_LOCAL, 7982 bytes)
2023-04-26 13:25:25,825 INFO org.apache.spark.executor.Executor [Executor task launch worker for task 9] Running task 0.0 in stage 9.0 (TID 9)
2023-04-26 13:25:25,876 INFO org.apache.spark.rdd.BinaryFileRDD [Executor task launch worker for task 9] Input split: Paths:/home/inferyx/Documents/Files/sample.json:0+308
2023-04-26 13:25:26,098 INFO org.apache.spark.executor.Executor [Executor task launch worker for task 9] Finished task 0.0 in stage 9.0 (TID 9). 2005 bytes result sent to driver
2023-04-26 13:25:26,099 INFO org.apache.spark.scheduler.TaskSetManager [task-result-getter-1] Finished task 0.0 in stage 9.0 (TID 9) in 313 ms on localhost (executor driver) (1/1)
2023-04-26 13:25:26,100 INFO org.apache.spark.scheduler.TaskSchedulerImpl [task-result-getter-1] Removed TaskSet 9.0, whose tasks have all completed, from pool 
2023-04-26 13:25:26,100 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] ResultStage 9 (json at SparkController.java:87) finished in 0.329 s
2023-04-26 13:25:26,102 INFO org.apache.spark.scheduler.DAGScheduler [http-nio-8080-exec-1] Job 9 finished: json at SparkController.java:87, took 0.337396 s
2023-04-26 13:25:26,222 INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy [http-nio-8080-exec-1] Pruning directories with: 
2023-04-26 13:25:26,223 INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy [http-nio-8080-exec-1] Post-Scan Filters: 
2023-04-26 13:25:26,223 INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy [http-nio-8080-exec-1] Output Data Schema: struct<address: struct<city: string, postalCode: string, state: string, streetAddress: string ... 2 more fields>, age: bigint, firstName: string, gender: string, lastName: string ... 1 more field>
2023-04-26 13:25:26,224 INFO org.apache.spark.sql.execution.FileSourceScanExec [http-nio-8080-exec-1] Pushed Filters: 
2023-04-26 13:25:26,301 INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [http-nio-8080-exec-1] Code generated in 38.007756 ms
2023-04-26 13:25:26,355 INFO org.apache.spark.storage.memory.MemoryStore [http-nio-8080-exec-1] Block broadcast_20 stored as values in memory (estimated size 112.3 KB, free 997.5 MB)
2023-04-26 13:25:26,376 INFO org.apache.spark.storage.memory.MemoryStore [http-nio-8080-exec-1] Block broadcast_20_piece0 stored as bytes in memory (estimated size 20.7 KB, free 997.5 MB)
2023-04-26 13:25:26,378 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-1] Added broadcast_20_piece0 in memory on 192.168.1.125:44401 (size: 20.7 KB, free: 998.3 MB)
2023-04-26 13:25:26,380 INFO org.apache.spark.SparkContext [http-nio-8080-exec-1] Created broadcast 20 from show at SparkController.java:89
2023-04-26 13:25:26,385 INFO org.apache.spark.sql.execution.FileSourceScanExec [http-nio-8080-exec-1] Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2023-04-26 13:25:26,401 INFO org.apache.spark.SparkContext [http-nio-8080-exec-1] Starting job: show at SparkController.java:89
2023-04-26 13:25:26,402 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Got job 10 (show at SparkController.java:89) with 1 output partitions
2023-04-26 13:25:26,402 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Final stage: ResultStage 10 (show at SparkController.java:89)
2023-04-26 13:25:26,403 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Parents of final stage: List()
2023-04-26 13:25:26,403 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Missing parents: List()
2023-04-26 13:25:26,403 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Submitting ResultStage 10 (MapPartitionsRDD[48] at show at SparkController.java:89), which has no missing parents
2023-04-26 13:25:26,408 INFO org.apache.spark.storage.memory.MemoryStore [dag-scheduler-event-loop] Block broadcast_21 stored as values in memory (estimated size 16.4 KB, free 997.5 MB)
2023-04-26 13:25:26,415 INFO org.apache.spark.storage.memory.MemoryStore [dag-scheduler-event-loop] Block broadcast_21_piece0 stored as bytes in memory (estimated size 7.3 KB, free 997.5 MB)
2023-04-26 13:25:26,416 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-3] Added broadcast_21_piece0 in memory on 192.168.1.125:44401 (size: 7.3 KB, free: 998.2 MB)
2023-04-26 13:25:26,417 INFO org.apache.spark.SparkContext [dag-scheduler-event-loop] Created broadcast 21 from broadcast at DAGScheduler.scala:1163
2023-04-26 13:25:26,418 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[48] at show at SparkController.java:89) (first 15 tasks are for partitions Vector(0))
2023-04-26 13:25:26,418 INFO org.apache.spark.scheduler.TaskSchedulerImpl [dag-scheduler-event-loop] Adding task set 10.0 with 1 tasks
2023-04-26 13:25:26,419 INFO org.apache.spark.scheduler.TaskSetManager [dispatcher-event-loop-2] Starting task 0.0 in stage 10.0 (TID 10, localhost, executor driver, partition 0, PROCESS_LOCAL, 8264 bytes)
2023-04-26 13:25:26,420 INFO org.apache.spark.executor.Executor [Executor task launch worker for task 10] Running task 0.0 in stage 10.0 (TID 10)
2023-04-26 13:25:26,427 INFO org.apache.spark.sql.execution.datasources.FileScanRDD [Executor task launch worker for task 10] Reading File path: file:///home/inferyx/Documents/Files/sample.json, range: 0-308, partition values: [empty row]
2023-04-26 13:25:26,472 INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [Executor task launch worker for task 10] Code generated in 35.122915 ms
2023-04-26 13:25:26,505 INFO org.apache.spark.executor.Executor [Executor task launch worker for task 10] Finished task 0.0 in stage 10.0 (TID 10). 1321 bytes result sent to driver
2023-04-26 13:25:26,506 INFO org.apache.spark.scheduler.TaskSetManager [task-result-getter-2] Finished task 0.0 in stage 10.0 (TID 10) in 87 ms on localhost (executor driver) (1/1)
2023-04-26 13:25:26,506 INFO org.apache.spark.scheduler.TaskSchedulerImpl [task-result-getter-2] Removed TaskSet 10.0, whose tasks have all completed, from pool 
2023-04-26 13:25:26,508 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] ResultStage 10 (show at SparkController.java:89) finished in 0.103 s
2023-04-26 13:25:26,508 INFO org.apache.spark.scheduler.DAGScheduler [http-nio-8080-exec-1] Job 10 finished: show at SparkController.java:89, took 0.106916 s
2023-04-26 13:25:26,513 INFO com.example.spring.jwt.mongodb.controllers.SparkController [http-nio-8080-exec-1] Reading Json File
2023-04-26 13:25:27,740 INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [http-nio-8080-exec-1] Code generated in 22.014006 ms
2023-04-26 13:25:27,769 INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [http-nio-8080-exec-1] Code generated in 18.466279 ms
2023-04-26 13:25:27,805 INFO org.apache.spark.SparkContext [http-nio-8080-exec-1] Starting job: show at SparkController.java:106
2023-04-26 13:25:27,807 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Got job 11 (show at SparkController.java:106) with 1 output partitions
2023-04-26 13:25:27,807 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Final stage: ResultStage 11 (show at SparkController.java:106)
2023-04-26 13:25:27,808 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Parents of final stage: List()
2023-04-26 13:25:27,808 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Missing parents: List()
2023-04-26 13:25:27,809 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Submitting ResultStage 11 (MapPartitionsRDD[53] at show at SparkController.java:106), which has no missing parents
2023-04-26 13:25:27,818 INFO org.apache.spark.storage.memory.MemoryStore [dag-scheduler-event-loop] Block broadcast_22 stored as values in memory (estimated size 7.5 KB, free 997.5 MB)
2023-04-26 13:25:27,825 INFO org.apache.spark.storage.memory.MemoryStore [dag-scheduler-event-loop] Block broadcast_22_piece0 stored as bytes in memory (estimated size 3.5 KB, free 997.4 MB)
2023-04-26 13:25:27,826 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-3] Added broadcast_22_piece0 in memory on 192.168.1.125:44401 (size: 3.5 KB, free: 998.2 MB)
2023-04-26 13:25:27,829 INFO org.apache.spark.SparkContext [dag-scheduler-event-loop] Created broadcast 22 from broadcast at DAGScheduler.scala:1163
2023-04-26 13:25:27,830 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[53] at show at SparkController.java:106) (first 15 tasks are for partitions Vector(0))
2023-04-26 13:25:27,831 INFO org.apache.spark.scheduler.TaskSchedulerImpl [dag-scheduler-event-loop] Adding task set 11.0 with 1 tasks
2023-04-26 13:25:27,839 INFO org.apache.spark.scheduler.TaskSetManager [dispatcher-event-loop-2] Starting task 0.0 in stage 11.0 (TID 11, localhost, executor driver, partition 0, PROCESS_LOCAL, 8705 bytes)
2023-04-26 13:25:27,841 INFO org.apache.spark.executor.Executor [Executor task launch worker for task 11] Running task 0.0 in stage 11.0 (TID 11)
2023-04-26 13:25:27,854 INFO org.apache.spark.executor.Executor [Executor task launch worker for task 11] Finished task 0.0 in stage 11.0 (TID 11). 1841 bytes result sent to driver
2023-04-26 13:25:27,855 INFO org.apache.spark.scheduler.TaskSetManager [task-result-getter-3] Finished task 0.0 in stage 11.0 (TID 11) in 23 ms on localhost (executor driver) (1/1)
2023-04-26 13:25:27,855 INFO org.apache.spark.scheduler.TaskSchedulerImpl [task-result-getter-3] Removed TaskSet 11.0, whose tasks have all completed, from pool 
2023-04-26 13:25:27,856 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] ResultStage 11 (show at SparkController.java:106) finished in 0.044 s
2023-04-26 13:25:27,858 INFO org.apache.spark.scheduler.DAGScheduler [http-nio-8080-exec-1] Job 11 finished: show at SparkController.java:106, took 0.052063 s
2023-04-26 13:25:27,866 INFO org.apache.spark.SparkContext [http-nio-8080-exec-1] Starting job: show at SparkController.java:106
2023-04-26 13:25:27,868 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Got job 12 (show at SparkController.java:106) with 2 output partitions
2023-04-26 13:25:27,869 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Final stage: ResultStage 12 (show at SparkController.java:106)
2023-04-26 13:25:27,869 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Parents of final stage: List()
2023-04-26 13:25:27,869 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Missing parents: List()
2023-04-26 13:25:27,870 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Submitting ResultStage 12 (MapPartitionsRDD[53] at show at SparkController.java:106), which has no missing parents
2023-04-26 13:25:27,872 INFO org.apache.spark.storage.memory.MemoryStore [dag-scheduler-event-loop] Block broadcast_23 stored as values in memory (estimated size 7.5 KB, free 997.4 MB)
2023-04-26 13:25:27,880 INFO org.apache.spark.storage.memory.MemoryStore [dag-scheduler-event-loop] Block broadcast_23_piece0 stored as bytes in memory (estimated size 3.5 KB, free 997.4 MB)
2023-04-26 13:25:27,881 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-3] Added broadcast_23_piece0 in memory on 192.168.1.125:44401 (size: 3.5 KB, free: 998.2 MB)
2023-04-26 13:25:27,881 INFO org.apache.spark.SparkContext [dag-scheduler-event-loop] Created broadcast 23 from broadcast at DAGScheduler.scala:1163
2023-04-26 13:25:27,882 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Submitting 2 missing tasks from ResultStage 12 (MapPartitionsRDD[53] at show at SparkController.java:106) (first 15 tasks are for partitions Vector(1, 2))
2023-04-26 13:25:27,882 INFO org.apache.spark.scheduler.TaskSchedulerImpl [dag-scheduler-event-loop] Adding task set 12.0 with 2 tasks
2023-04-26 13:25:27,884 INFO org.apache.spark.scheduler.TaskSetManager [dispatcher-event-loop-2] Starting task 0.0 in stage 12.0 (TID 12, localhost, executor driver, partition 1, PROCESS_LOCAL, 8334 bytes)
2023-04-26 13:25:27,885 INFO org.apache.spark.scheduler.TaskSetManager [dispatcher-event-loop-2] Starting task 1.0 in stage 12.0 (TID 13, localhost, executor driver, partition 2, PROCESS_LOCAL, 8271 bytes)
2023-04-26 13:25:27,885 INFO org.apache.spark.executor.Executor [Executor task launch worker for task 12] Running task 0.0 in stage 12.0 (TID 12)
2023-04-26 13:25:27,886 INFO org.apache.spark.executor.Executor [Executor task launch worker for task 13] Running task 1.0 in stage 12.0 (TID 13)
2023-04-26 13:25:27,890 INFO org.apache.spark.executor.Executor [Executor task launch worker for task 12] Finished task 0.0 in stage 12.0 (TID 12). 1186 bytes result sent to driver
2023-04-26 13:25:27,890 INFO org.apache.spark.executor.Executor [Executor task launch worker for task 13] Finished task 1.0 in stage 12.0 (TID 13). 1069 bytes result sent to driver
2023-04-26 13:25:27,891 INFO org.apache.spark.scheduler.TaskSetManager [task-result-getter-0] Finished task 0.0 in stage 12.0 (TID 12) in 8 ms on localhost (executor driver) (1/2)
2023-04-26 13:25:27,895 INFO org.apache.spark.scheduler.TaskSetManager [task-result-getter-1] Finished task 1.0 in stage 12.0 (TID 13) in 11 ms on localhost (executor driver) (2/2)
2023-04-26 13:25:27,895 INFO org.apache.spark.scheduler.TaskSchedulerImpl [task-result-getter-1] Removed TaskSet 12.0, whose tasks have all completed, from pool 
2023-04-26 13:25:27,896 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] ResultStage 12 (show at SparkController.java:106) finished in 0.025 s
2023-04-26 13:25:27,897 INFO org.apache.spark.scheduler.DAGScheduler [http-nio-8080-exec-1] Job 12 finished: show at SparkController.java:106, took 0.029689 s
2023-04-26 13:25:27,901 INFO com.example.spring.jwt.mongodb.controllers.SparkController [http-nio-8080-exec-1] Reading Excel File
2023-04-26 13:25:30,732 INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy [http-nio-8080-exec-1] Pruning directories with: 
2023-04-26 13:25:30,732 INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy [http-nio-8080-exec-1] Post-Scan Filters: 
2023-04-26 13:25:30,733 INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy [http-nio-8080-exec-1] Output Data Schema: struct<John: string, Doe: string, 120 jefferson st.: string, Riverside: string,  NJ: string ... 1 more field>
2023-04-26 13:25:30,734 INFO org.apache.spark.sql.execution.FileSourceScanExec [http-nio-8080-exec-1] Pushed Filters: 
2023-04-26 13:25:30,743 INFO org.apache.spark.storage.memory.MemoryStore [http-nio-8080-exec-1] Block broadcast_24 stored as values in memory (estimated size 167.6 KB, free 997.3 MB)
2023-04-26 13:25:30,764 INFO org.apache.spark.storage.memory.MemoryStore [http-nio-8080-exec-1] Block broadcast_24_piece0 stored as bytes in memory (estimated size 20.9 KB, free 997.3 MB)
2023-04-26 13:25:30,765 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-0] Added broadcast_24_piece0 in memory on 192.168.1.125:44401 (size: 20.9 KB, free: 998.2 MB)
2023-04-26 13:25:30,767 INFO org.apache.spark.SparkContext [http-nio-8080-exec-1] Created broadcast 24 from jdbc at SparkController.java:125
2023-04-26 13:25:30,768 INFO org.apache.spark.sql.execution.FileSourceScanExec [http-nio-8080-exec-1] Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2023-04-26 13:25:30,847 INFO org.apache.spark.SparkContext [http-nio-8080-exec-1] Starting job: jdbc at SparkController.java:125
2023-04-26 13:25:30,848 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Got job 13 (jdbc at SparkController.java:125) with 1 output partitions
2023-04-26 13:25:30,848 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Final stage: ResultStage 13 (jdbc at SparkController.java:125)
2023-04-26 13:25:30,848 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Parents of final stage: List()
2023-04-26 13:25:30,849 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Missing parents: List()
2023-04-26 13:25:30,849 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Submitting ResultStage 13 (MapPartitionsRDD[58] at jdbc at SparkController.java:125), which has no missing parents
2023-04-26 13:25:30,853 INFO org.apache.spark.storage.memory.MemoryStore [dag-scheduler-event-loop] Block broadcast_25 stored as values in memory (estimated size 15.9 KB, free 997.2 MB)
2023-04-26 13:25:30,861 INFO org.apache.spark.storage.memory.MemoryStore [dag-scheduler-event-loop] Block broadcast_25_piece0 stored as bytes in memory (estimated size 8.5 KB, free 997.2 MB)
2023-04-26 13:25:30,861 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-1] Added broadcast_25_piece0 in memory on 192.168.1.125:44401 (size: 8.5 KB, free: 998.2 MB)
2023-04-26 13:25:30,862 INFO org.apache.spark.SparkContext [dag-scheduler-event-loop] Created broadcast 25 from broadcast at DAGScheduler.scala:1163
2023-04-26 13:25:30,863 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[58] at jdbc at SparkController.java:125) (first 15 tasks are for partitions Vector(0))
2023-04-26 13:25:30,863 INFO org.apache.spark.scheduler.TaskSchedulerImpl [dag-scheduler-event-loop] Adding task set 13.0 with 1 tasks
2023-04-26 13:25:30,864 INFO org.apache.spark.scheduler.TaskSetManager [dispatcher-event-loop-3] Starting task 0.0 in stage 13.0 (TID 14, localhost, executor driver, partition 0, PROCESS_LOCAL, 8266 bytes)
2023-04-26 13:25:30,865 INFO org.apache.spark.executor.Executor [Executor task launch worker for task 14] Running task 0.0 in stage 13.0 (TID 14)
2023-04-26 13:25:30,943 INFO org.apache.spark.sql.execution.datasources.FileScanRDD [Executor task launch worker for task 14] Reading File path: file:///home/inferyx/Documents/Files/addresses.csv, range: 0-328, partition values: [empty row]
2023-04-26 13:25:33,121 INFO org.apache.spark.executor.Executor [Executor task launch worker for task 14] Finished task 0.0 in stage 13.0 (TID 14). 1437 bytes result sent to driver
2023-04-26 13:25:33,123 INFO org.apache.spark.scheduler.TaskSetManager [task-result-getter-2] Finished task 0.0 in stage 13.0 (TID 14) in 2259 ms on localhost (executor driver) (1/1)
2023-04-26 13:25:33,123 INFO org.apache.spark.scheduler.TaskSchedulerImpl [task-result-getter-2] Removed TaskSet 13.0, whose tasks have all completed, from pool 
2023-04-26 13:25:33,124 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] ResultStage 13 (jdbc at SparkController.java:125) finished in 2.274 s
2023-04-26 13:25:33,126 INFO org.apache.spark.scheduler.DAGScheduler [http-nio-8080-exec-1] Job 13 finished: jdbc at SparkController.java:125, took 2.278154 s
2023-04-26 13:25:33,498 INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy [http-nio-8080-exec-1] Pruning directories with: 
2023-04-26 13:25:33,499 INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy [http-nio-8080-exec-1] Post-Scan Filters: 
2023-04-26 13:25:33,500 INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy [http-nio-8080-exec-1] Output Data Schema: struct<John: string, Doe: string, 120 jefferson st.: string, Riverside: string,  NJ: string ... 1 more field>
2023-04-26 13:25:33,501 INFO org.apache.spark.sql.execution.FileSourceScanExec [http-nio-8080-exec-1] Pushed Filters: 
2023-04-26 13:25:33,530 INFO org.apache.spark.storage.memory.MemoryStore [http-nio-8080-exec-1] Block broadcast_26 stored as values in memory (estimated size 167.6 KB, free 997.1 MB)
2023-04-26 13:25:33,547 INFO org.apache.spark.storage.memory.MemoryStore [http-nio-8080-exec-1] Block broadcast_26_piece0 stored as bytes in memory (estimated size 20.9 KB, free 997.0 MB)
2023-04-26 13:25:33,548 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-1] Added broadcast_26_piece0 in memory on 192.168.1.125:44401 (size: 20.9 KB, free: 998.2 MB)
2023-04-26 13:25:33,550 INFO org.apache.spark.SparkContext [http-nio-8080-exec-1] Created broadcast 26 from show at SparkController.java:127
2023-04-26 13:25:33,551 INFO org.apache.spark.sql.execution.FileSourceScanExec [http-nio-8080-exec-1] Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2023-04-26 13:25:33,566 INFO org.apache.spark.SparkContext [http-nio-8080-exec-1] Starting job: show at SparkController.java:127
2023-04-26 13:25:33,567 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Got job 14 (show at SparkController.java:127) with 1 output partitions
2023-04-26 13:25:33,567 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Final stage: ResultStage 14 (show at SparkController.java:127)
2023-04-26 13:25:33,567 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Parents of final stage: List()
2023-04-26 13:25:33,567 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Missing parents: List()
2023-04-26 13:25:33,568 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Submitting ResultStage 14 (MapPartitionsRDD[64] at show at SparkController.java:127), which has no missing parents
2023-04-26 13:25:33,572 INFO org.apache.spark.storage.memory.MemoryStore [dag-scheduler-event-loop] Block broadcast_27 stored as values in memory (estimated size 10.4 KB, free 997.0 MB)
2023-04-26 13:25:33,575 INFO org.apache.spark.storage.memory.MemoryStore [dag-scheduler-event-loop] Block broadcast_27_piece0 stored as bytes in memory (estimated size 5.7 KB, free 997.0 MB)
2023-04-26 13:25:33,578 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-3] Added broadcast_27_piece0 in memory on 192.168.1.125:44401 (size: 5.7 KB, free: 998.2 MB)
2023-04-26 13:25:33,585 INFO org.apache.spark.SparkContext [dag-scheduler-event-loop] Created broadcast 27 from broadcast at DAGScheduler.scala:1163
2023-04-26 13:25:33,586 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[64] at show at SparkController.java:127) (first 15 tasks are for partitions Vector(0))
2023-04-26 13:25:33,586 INFO org.apache.spark.scheduler.TaskSchedulerImpl [dag-scheduler-event-loop] Adding task set 14.0 with 1 tasks
2023-04-26 13:25:33,587 INFO org.apache.spark.scheduler.TaskSetManager [dispatcher-event-loop-2] Starting task 0.0 in stage 14.0 (TID 15, localhost, executor driver, partition 0, PROCESS_LOCAL, 8266 bytes)
2023-04-26 13:25:33,588 INFO org.apache.spark.executor.Executor [Executor task launch worker for task 15] Running task 0.0 in stage 14.0 (TID 15)
2023-04-26 13:25:33,596 INFO org.apache.spark.sql.execution.datasources.FileScanRDD [Executor task launch worker for task 15] Reading File path: file:///home/inferyx/Documents/Files/addresses.csv, range: 0-328, partition values: [empty row]
2023-04-26 13:25:33,604 INFO org.apache.spark.executor.Executor [Executor task launch worker for task 15] Finished task 0.0 in stage 14.0 (TID 15). 1677 bytes result sent to driver
2023-04-26 13:25:33,607 INFO org.apache.spark.scheduler.TaskSetManager [task-result-getter-3] Finished task 0.0 in stage 14.0 (TID 15) in 20 ms on localhost (executor driver) (1/1)
2023-04-26 13:25:33,608 INFO org.apache.spark.scheduler.TaskSchedulerImpl [task-result-getter-3] Removed TaskSet 14.0, whose tasks have all completed, from pool 
2023-04-26 13:25:33,609 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] ResultStage 14 (show at SparkController.java:127) finished in 0.039 s
2023-04-26 13:25:33,611 INFO org.apache.spark.scheduler.DAGScheduler [http-nio-8080-exec-1] Job 14 finished: show at SparkController.java:127, took 0.044102 s
2023-04-26 13:25:33,614 INFO com.example.spring.jwt.mongodb.controllers.SparkController [http-nio-8080-exec-1] writting Csv File
2023-04-26 13:25:33,997 INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy [http-nio-8080-exec-1] Pruning directories with: 
2023-04-26 13:25:33,998 INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy [http-nio-8080-exec-1] Post-Scan Filters: 
2023-04-26 13:25:33,999 INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy [http-nio-8080-exec-1] Output Data Schema: struct< John: string, Doe: string, 120 jefferson st.: string, Riverside: string,  NJ: string ... 1 more field>
2023-04-26 13:25:33,999 INFO org.apache.spark.sql.execution.FileSourceScanExec [http-nio-8080-exec-1] Pushed Filters: 
2023-04-26 13:25:34,011 INFO org.apache.spark.storage.memory.MemoryStore [http-nio-8080-exec-1] Block broadcast_28 stored as values in memory (estimated size 167.8 KB, free 996.9 MB)
2023-04-26 13:25:34,030 INFO org.apache.spark.storage.memory.MemoryStore [http-nio-8080-exec-1] Block broadcast_28_piece0 stored as bytes in memory (estimated size 20.9 KB, free 996.8 MB)
2023-04-26 13:25:34,031 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-3] Added broadcast_28_piece0 in memory on 192.168.1.125:44401 (size: 20.9 KB, free: 998.2 MB)
2023-04-26 13:25:34,033 INFO org.apache.spark.SparkContext [http-nio-8080-exec-1] Created broadcast 28 from jdbc at SparkController.java:134
2023-04-26 13:25:34,034 INFO org.apache.spark.sql.execution.FileSourceScanExec [http-nio-8080-exec-1] Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2023-04-26 13:25:34,056 INFO org.apache.spark.SparkContext [http-nio-8080-exec-1] Starting job: jdbc at SparkController.java:134
2023-04-26 13:25:34,058 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Got job 15 (jdbc at SparkController.java:134) with 1 output partitions
2023-04-26 13:25:34,058 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Final stage: ResultStage 15 (jdbc at SparkController.java:134)
2023-04-26 13:25:34,058 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Parents of final stage: List()
2023-04-26 13:25:34,059 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Missing parents: List()
2023-04-26 13:25:34,059 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Submitting ResultStage 15 (MapPartitionsRDD[69] at jdbc at SparkController.java:134), which has no missing parents
2023-04-26 13:25:34,062 INFO org.apache.spark.storage.memory.MemoryStore [dag-scheduler-event-loop] Block broadcast_29 stored as values in memory (estimated size 16.0 KB, free 996.8 MB)
2023-04-26 13:25:34,064 INFO org.apache.spark.storage.memory.MemoryStore [dag-scheduler-event-loop] Block broadcast_29_piece0 stored as bytes in memory (estimated size 8.6 KB, free 996.8 MB)
2023-04-26 13:25:34,064 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-2] Added broadcast_29_piece0 in memory on 192.168.1.125:44401 (size: 8.6 KB, free: 998.2 MB)
2023-04-26 13:25:34,065 INFO org.apache.spark.SparkContext [dag-scheduler-event-loop] Created broadcast 29 from broadcast at DAGScheduler.scala:1163
2023-04-26 13:25:34,066 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[69] at jdbc at SparkController.java:134) (first 15 tasks are for partitions Vector(0))
2023-04-26 13:25:34,066 INFO org.apache.spark.scheduler.TaskSchedulerImpl [dag-scheduler-event-loop] Adding task set 15.0 with 1 tasks
2023-04-26 13:25:34,067 INFO org.apache.spark.scheduler.TaskSetManager [dispatcher-event-loop-0] Starting task 0.0 in stage 15.0 (TID 16, localhost, executor driver, partition 0, PROCESS_LOCAL, 8263 bytes)
2023-04-26 13:25:34,068 INFO org.apache.spark.executor.Executor [Executor task launch worker for task 16] Running task 0.0 in stage 15.0 (TID 16)
2023-04-26 13:25:34,103 INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [Executor task launch worker for task 16] Code generated in 19.673223 ms
2023-04-26 13:25:34,200 INFO org.apache.spark.sql.execution.datasources.FileScanRDD [Executor task launch worker for task 16] Reading File path: file:///home/inferyx/Documents/Files/output.psv, range: 0-340, partition values: [empty row]
2023-04-26 13:25:34,724 INFO org.apache.spark.executor.Executor [Executor task launch worker for task 16] Finished task 0.0 in stage 15.0 (TID 16). 1394 bytes result sent to driver
2023-04-26 13:25:34,725 INFO org.apache.spark.scheduler.TaskSetManager [task-result-getter-0] Finished task 0.0 in stage 15.0 (TID 16) in 658 ms on localhost (executor driver) (1/1)
2023-04-26 13:25:34,726 INFO org.apache.spark.scheduler.TaskSchedulerImpl [task-result-getter-0] Removed TaskSet 15.0, whose tasks have all completed, from pool 
2023-04-26 13:25:34,727 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] ResultStage 15 (jdbc at SparkController.java:134) finished in 0.666 s
2023-04-26 13:25:34,728 INFO org.apache.spark.scheduler.DAGScheduler [http-nio-8080-exec-1] Job 15 finished: jdbc at SparkController.java:134, took 0.671094 s
2023-04-26 13:25:34,884 INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy [http-nio-8080-exec-1] Pruning directories with: 
2023-04-26 13:25:34,885 INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy [http-nio-8080-exec-1] Post-Scan Filters: 
2023-04-26 13:25:34,886 INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy [http-nio-8080-exec-1] Output Data Schema: struct< John: string, Doe: string, 120 jefferson st.: string, Riverside: string,  NJ: string ... 1 more field>
2023-04-26 13:25:34,886 INFO org.apache.spark.sql.execution.FileSourceScanExec [http-nio-8080-exec-1] Pushed Filters: 
2023-04-26 13:25:34,949 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 436
2023-04-26 13:25:34,948 INFO org.apache.spark.storage.memory.MemoryStore [http-nio-8080-exec-1] Block broadcast_30 stored as values in memory (estimated size 167.8 KB, free 996.7 MB)
2023-04-26 13:25:34,951 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-2] Removed broadcast_21_piece0 on 192.168.1.125:44401 in memory (size: 7.3 KB, free: 998.2 MB)
2023-04-26 13:25:34,955 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 445
2023-04-26 13:25:34,955 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 350
2023-04-26 13:25:34,955 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 276
2023-04-26 13:25:34,955 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 258
2023-04-26 13:25:34,955 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 293
2023-04-26 13:25:34,955 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 398
2023-04-26 13:25:34,955 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 240
2023-04-26 13:25:34,956 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 453
2023-04-26 13:25:34,956 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 418
2023-04-26 13:25:34,959 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-3] Removed broadcast_20_piece0 on 192.168.1.125:44401 in memory (size: 20.7 KB, free: 998.2 MB)
2023-04-26 13:25:34,967 INFO org.apache.spark.storage.memory.MemoryStore [http-nio-8080-exec-1] Block broadcast_30_piece0 stored as bytes in memory (estimated size 20.9 KB, free 996.8 MB)
2023-04-26 13:25:34,968 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-2] Added broadcast_30_piece0 in memory on 192.168.1.125:44401 (size: 20.9 KB, free: 998.2 MB)
2023-04-26 13:25:34,970 INFO org.apache.spark.SparkContext [http-nio-8080-exec-1] Created broadcast 30 from show at SparkController.java:136
2023-04-26 13:25:34,971 INFO org.apache.spark.sql.execution.FileSourceScanExec [http-nio-8080-exec-1] Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2023-04-26 13:25:34,973 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 443
2023-04-26 13:25:34,974 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 460
2023-04-26 13:25:34,974 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 169
2023-04-26 13:25:34,974 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 458
2023-04-26 13:25:34,979 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-3] Removed broadcast_23_piece0 on 192.168.1.125:44401 in memory (size: 3.5 KB, free: 998.2 MB)
2023-04-26 13:25:34,983 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 195
2023-04-26 13:25:34,983 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 333
2023-04-26 13:25:34,983 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 451
2023-04-26 13:25:34,983 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 165
2023-04-26 13:25:34,985 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-2] Removed broadcast_29_piece0 on 192.168.1.125:44401 in memory (size: 8.6 KB, free: 998.2 MB)
2023-04-26 13:25:34,990 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 221
2023-04-26 13:25:34,990 INFO org.apache.spark.SparkContext [http-nio-8080-exec-1] Starting job: show at SparkController.java:136
2023-04-26 13:25:34,991 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 260
2023-04-26 13:25:34,991 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 402
2023-04-26 13:25:34,991 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 219
2023-04-26 13:25:34,991 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 178
2023-04-26 13:25:34,991 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 160
2023-04-26 13:25:34,992 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 261
2023-04-26 13:25:34,992 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 429
2023-04-26 13:25:34,992 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Got job 16 (show at SparkController.java:136) with 1 output partitions
2023-04-26 13:25:34,993 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Final stage: ResultStage 16 (show at SparkController.java:136)
2023-04-26 13:25:34,994 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Parents of final stage: List()
2023-04-26 13:25:34,994 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Missing parents: List()
2023-04-26 13:25:34,994 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Submitting ResultStage 16 (MapPartitionsRDD[75] at show at SparkController.java:136), which has no missing parents
2023-04-26 13:25:34,994 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-0] Removed broadcast_13_piece0 on 192.168.1.125:44401 in memory (size: 4.6 KB, free: 998.2 MB)
2023-04-26 13:25:34,999 INFO org.apache.spark.storage.memory.MemoryStore [dag-scheduler-event-loop] Block broadcast_31 stored as values in memory (estimated size 12.6 KB, free 996.8 MB)
2023-04-26 13:25:35,001 INFO org.apache.spark.storage.memory.MemoryStore [dag-scheduler-event-loop] Block broadcast_31_piece0 stored as bytes in memory (estimated size 6.3 KB, free 996.8 MB)
2023-04-26 13:25:35,002 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 430
2023-04-26 13:25:35,002 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 349
2023-04-26 13:25:35,003 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 431
2023-04-26 13:25:35,003 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 164
2023-04-26 13:25:35,003 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 257
2023-04-26 13:25:35,003 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 197
2023-04-26 13:25:35,003 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 286
2023-04-26 13:25:35,003 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 283
2023-04-26 13:25:35,003 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 215
2023-04-26 13:25:35,003 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 322
2023-04-26 13:25:35,003 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 363
2023-04-26 13:25:35,004 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 317
2023-04-26 13:25:35,004 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 368
2023-04-26 13:25:35,004 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 242
2023-04-26 13:25:35,004 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 279
2023-04-26 13:25:35,004 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 391
2023-04-26 13:25:35,004 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 331
2023-04-26 13:25:35,004 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 295
2023-04-26 13:25:35,004 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 360
2023-04-26 13:25:35,004 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 379
2023-04-26 13:25:35,004 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 190
2023-04-26 13:25:35,006 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-2] Added broadcast_31_piece0 in memory on 192.168.1.125:44401 (size: 6.3 KB, free: 998.2 MB)
2023-04-26 13:25:35,006 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 416
2023-04-26 13:25:35,006 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 389
2023-04-26 13:25:35,007 INFO org.apache.spark.SparkContext [dag-scheduler-event-loop] Created broadcast 31 from broadcast at DAGScheduler.scala:1163
2023-04-26 13:25:35,007 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 442
2023-04-26 13:25:35,008 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 271
2023-04-26 13:25:35,008 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 355
2023-04-26 13:25:35,008 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 208
2023-04-26 13:25:35,008 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 357
2023-04-26 13:25:35,008 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 408
2023-04-26 13:25:35,009 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 399
2023-04-26 13:25:35,009 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 185
2023-04-26 13:25:35,009 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 211
2023-04-26 13:25:35,009 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Submitting 1 missing tasks from ResultStage 16 (MapPartitionsRDD[75] at show at SparkController.java:136) (first 15 tasks are for partitions Vector(0))
2023-04-26 13:25:35,009 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 290
2023-04-26 13:25:35,009 INFO org.apache.spark.scheduler.TaskSchedulerImpl [dag-scheduler-event-loop] Adding task set 16.0 with 1 tasks
2023-04-26 13:25:35,009 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 233
2023-04-26 13:25:35,010 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 213
2023-04-26 13:25:35,010 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 157
2023-04-26 13:25:35,010 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 364
2023-04-26 13:25:35,010 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 341
2023-04-26 13:25:35,010 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 269
2023-04-26 13:25:35,011 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 266
2023-04-26 13:25:35,011 INFO org.apache.spark.scheduler.TaskSetManager [dispatcher-event-loop-3] Starting task 0.0 in stage 16.0 (TID 17, localhost, executor driver, partition 0, PROCESS_LOCAL, 8263 bytes)
2023-04-26 13:25:35,011 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 325
2023-04-26 13:25:35,011 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 457
2023-04-26 13:25:35,011 INFO org.apache.spark.executor.Executor [Executor task launch worker for task 17] Running task 0.0 in stage 16.0 (TID 17)
2023-04-26 13:25:35,011 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 450
2023-04-26 13:25:35,012 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 241
2023-04-26 13:25:35,013 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 452
2023-04-26 13:25:35,013 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 369
2023-04-26 13:25:35,013 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 448
2023-04-26 13:25:35,013 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 253
2023-04-26 13:25:35,014 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 193
2023-04-26 13:25:35,018 INFO org.apache.spark.sql.execution.datasources.FileScanRDD [Executor task launch worker for task 17] Reading File path: file:///home/inferyx/Documents/Files/output.psv, range: 0-340, partition values: [empty row]
2023-04-26 13:25:35,021 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-0] Removed broadcast_28_piece0 on 192.168.1.125:44401 in memory (size: 20.9 KB, free: 998.2 MB)
2023-04-26 13:25:35,028 INFO org.apache.spark.executor.Executor [Executor task launch worker for task 17] Finished task 0.0 in stage 16.0 (TID 17). 1695 bytes result sent to driver
2023-04-26 13:25:35,028 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 236
2023-04-26 13:25:35,028 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 176
2023-04-26 13:25:35,029 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 218
2023-04-26 13:25:35,029 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 234
2023-04-26 13:25:35,029 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 378
2023-04-26 13:25:35,029 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 229
2023-04-26 13:25:35,029 INFO org.apache.spark.scheduler.TaskSetManager [task-result-getter-1] Finished task 0.0 in stage 16.0 (TID 17) in 19 ms on localhost (executor driver) (1/1)
2023-04-26 13:25:35,029 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 217
2023-04-26 13:25:35,029 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 447
2023-04-26 13:25:35,029 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 316
2023-04-26 13:25:35,029 INFO org.apache.spark.scheduler.TaskSchedulerImpl [task-result-getter-1] Removed TaskSet 16.0, whose tasks have all completed, from pool 
2023-04-26 13:25:35,029 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 371
2023-04-26 13:25:35,030 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 223
2023-04-26 13:25:35,030 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 403
2023-04-26 13:25:35,030 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 205
2023-04-26 13:25:35,030 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 409
2023-04-26 13:25:35,030 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 334
2023-04-26 13:25:35,030 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 356
2023-04-26 13:25:35,030 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] ResultStage 16 (show at SparkController.java:136) finished in 0.034 s
2023-04-26 13:25:35,030 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 467
2023-04-26 13:25:35,033 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-0] Removed broadcast_10_piece0 on 192.168.1.125:44401 in memory (size: 20.7 KB, free: 998.2 MB)
2023-04-26 13:25:35,033 INFO org.apache.spark.scheduler.DAGScheduler [http-nio-8080-exec-1] Job 16 finished: show at SparkController.java:136, took 0.041554 s
2023-04-26 13:25:35,036 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 324
2023-04-26 13:25:35,036 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 244
2023-04-26 13:25:35,037 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 353
2023-04-26 13:25:35,037 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 250
2023-04-26 13:25:35,037 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 305
2023-04-26 13:25:35,037 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 392
2023-04-26 13:25:35,037 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 168
2023-04-26 13:25:35,042 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-3] Removed broadcast_15_piece0 on 192.168.1.125:44401 in memory (size: 7.6 KB, free: 998.2 MB)
2023-04-26 13:25:35,042 INFO com.example.spring.jwt.mongodb.controllers.SparkController [http-nio-8080-exec-1] writting Psv File
2023-04-26 13:25:35,049 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 347
2023-04-26 13:25:35,049 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 225
2023-04-26 13:25:35,049 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 300
2023-04-26 13:25:35,049 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 235
2023-04-26 13:25:35,050 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 170
2023-04-26 13:25:35,050 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 188
2023-04-26 13:25:35,050 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 186
2023-04-26 13:25:35,050 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 440
2023-04-26 13:25:35,050 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 461
2023-04-26 13:25:35,050 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 180
2023-04-26 13:25:35,050 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 209
2023-04-26 13:25:35,050 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 183
2023-04-26 13:25:35,050 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 358
2023-04-26 13:25:35,050 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 274
2023-04-26 13:25:35,050 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 394
2023-04-26 13:25:35,050 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 285
2023-04-26 13:25:35,051 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 395
2023-04-26 13:25:35,051 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 332
2023-04-26 13:25:35,051 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 373
2023-04-26 13:25:35,053 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-0] Removed broadcast_14_piece0 on 192.168.1.125:44401 in memory (size: 20.7 KB, free: 998.2 MB)
2023-04-26 13:25:35,062 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 372
2023-04-26 13:25:35,062 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 336
2023-04-26 13:25:35,062 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 405
2023-04-26 13:25:35,062 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 175
2023-04-26 13:25:35,063 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 171
2023-04-26 13:25:35,063 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 254
2023-04-26 13:25:35,063 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 311
2023-04-26 13:25:35,068 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 201
2023-04-26 13:25:35,068 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 310
2023-04-26 13:25:35,069 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 163
2023-04-26 13:25:35,069 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 309
2023-04-26 13:25:35,069 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 346
2023-04-26 13:25:35,069 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 153
2023-04-26 13:25:35,069 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 237
2023-04-26 13:25:35,069 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 202
2023-04-26 13:25:35,070 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 162
2023-04-26 13:25:35,070 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 194
2023-04-26 13:25:35,070 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 256
2023-04-26 13:25:35,070 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 340
2023-04-26 13:25:35,070 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 354
2023-04-26 13:25:35,070 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 319
2023-04-26 13:25:35,070 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 230
2023-04-26 13:25:35,073 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-2] Removed broadcast_17_piece0 on 192.168.1.125:44401 in memory (size: 6.5 KB, free: 998.2 MB)
2023-04-26 13:25:35,082 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 161
2023-04-26 13:25:35,082 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 321
2023-04-26 13:25:35,082 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 330
2023-04-26 13:25:35,083 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 231
2023-04-26 13:25:35,083 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 282
2023-04-26 13:25:35,083 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 380
2023-04-26 13:25:35,083 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 468
2023-04-26 13:25:35,083 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 179
2023-04-26 13:25:35,083 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 301
2023-04-26 13:25:35,084 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 338
2023-04-26 13:25:35,084 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 277
2023-04-26 13:25:35,084 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 406
2023-04-26 13:25:35,084 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 412
2023-04-26 13:25:35,084 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 294
2023-04-26 13:25:35,084 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 243
2023-04-26 13:25:35,084 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 199
2023-04-26 13:25:35,084 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 314
2023-04-26 13:25:35,085 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 397
2023-04-26 13:25:35,085 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 407
2023-04-26 13:25:35,085 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 411
2023-04-26 13:25:35,085 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 417
2023-04-26 13:25:35,085 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 444
2023-04-26 13:25:35,085 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 446
2023-04-26 13:25:35,085 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 154
2023-04-26 13:25:35,085 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 425
2023-04-26 13:25:35,085 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 455
2023-04-26 13:25:35,085 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 207
2023-04-26 13:25:35,086 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 239
2023-04-26 13:25:35,086 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 420
2023-04-26 13:25:35,086 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 424
2023-04-26 13:25:35,086 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 312
2023-04-26 13:25:35,086 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 423
2023-04-26 13:25:35,089 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-1] Removed broadcast_12_piece0 on 192.168.1.125:44401 in memory (size: 20.7 KB, free: 998.3 MB)
2023-04-26 13:25:35,102 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 210
2023-04-26 13:25:35,102 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 415
2023-04-26 13:25:35,102 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 438
2023-04-26 13:25:35,103 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 247
2023-04-26 13:25:35,103 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 437
2023-04-26 13:25:35,103 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 245
2023-04-26 13:25:35,103 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 393
2023-04-26 13:25:35,103 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 377
2023-04-26 13:25:35,103 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 191
2023-04-26 13:25:35,104 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 255
2023-04-26 13:25:35,104 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 329
2023-04-26 13:25:35,104 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 388
2023-04-26 13:25:35,104 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 413
2023-04-26 13:25:35,104 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 464
2023-04-26 13:25:35,104 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 318
2023-04-26 13:25:35,104 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 359
2023-04-26 13:25:35,104 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 268
2023-04-26 13:25:35,104 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 337
2023-04-26 13:25:35,105 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 384
2023-04-26 13:25:35,105 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 335
2023-04-26 13:25:35,111 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-3] Removed broadcast_25_piece0 on 192.168.1.125:44401 in memory (size: 8.5 KB, free: 998.3 MB)
2023-04-26 13:25:35,115 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-2] Removed broadcast_26_piece0 on 192.168.1.125:44401 in memory (size: 20.9 KB, free: 998.3 MB)
2023-04-26 13:25:35,122 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 352
2023-04-26 13:25:35,122 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 422
2023-04-26 13:25:35,123 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 342
2023-04-26 13:25:35,124 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 320
2023-04-26 13:25:35,124 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 227
2023-04-26 13:25:35,124 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 351
2023-04-26 13:25:35,125 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 396
2023-04-26 13:25:35,125 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 313
2023-04-26 13:25:35,125 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 189
2023-04-26 13:25:35,125 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 232
2023-04-26 13:25:35,126 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 159
2023-04-26 13:25:35,126 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 192
2023-04-26 13:25:35,128 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-1] Removed broadcast_18_piece0 on 192.168.1.125:44401 in memory (size: 20.9 KB, free: 998.3 MB)
2023-04-26 13:25:35,132 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 278
2023-04-26 13:25:35,132 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 289
2023-04-26 13:25:35,132 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 307
2023-04-26 13:25:35,132 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 315
2023-04-26 13:25:35,132 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 167
2023-04-26 13:25:35,132 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 220
2023-04-26 13:25:35,132 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 265
2023-04-26 13:25:35,132 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 434
2023-04-26 13:25:35,132 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 222
2023-04-26 13:25:35,132 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 463
2023-04-26 13:25:35,132 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 433
2023-04-26 13:25:35,138 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-0] Removed broadcast_24_piece0 on 192.168.1.125:44401 in memory (size: 20.9 KB, free: 998.3 MB)
2023-04-26 13:25:35,146 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 304
2023-04-26 13:25:35,146 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 308
2023-04-26 13:25:35,146 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 297
2023-04-26 13:25:35,147 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 166
2023-04-26 13:25:35,147 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 248
2023-04-26 13:25:35,147 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 456
2023-04-26 13:25:35,147 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 466
2023-04-26 13:25:35,147 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 174
2023-04-26 13:25:35,147 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 173
2023-04-26 13:25:35,147 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 212
2023-04-26 13:25:35,148 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 344
2023-04-26 13:25:35,148 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 267
2023-04-26 13:25:35,148 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 327
2023-04-26 13:25:35,148 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 366
2023-04-26 13:25:35,149 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-3] Removed broadcast_22_piece0 on 192.168.1.125:44401 in memory (size: 3.5 KB, free: 998.3 MB)
2023-04-26 13:25:35,151 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 272
2023-04-26 13:25:35,151 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 204
2023-04-26 13:25:35,151 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 345
2023-04-26 13:25:35,152 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 303
2023-04-26 13:25:35,152 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 155
2023-04-26 13:25:35,152 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 427
2023-04-26 13:25:35,152 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 306
2023-04-26 13:25:35,152 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 439
2023-04-26 13:25:35,152 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 206
2023-04-26 13:25:35,152 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 198
2023-04-26 13:25:35,152 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 387
2023-04-26 13:25:35,153 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 339
2023-04-26 13:25:35,153 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 385
2023-04-26 13:25:35,153 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 386
2023-04-26 13:25:35,155 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-2] Removed broadcast_27_piece0 on 192.168.1.125:44401 in memory (size: 5.7 KB, free: 998.3 MB)
2023-04-26 13:25:35,161 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 156
2023-04-26 13:25:35,161 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 184
2023-04-26 13:25:35,162 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 287
2023-04-26 13:25:35,162 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 400
2023-04-26 13:25:35,162 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 152
2023-04-26 13:25:35,162 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 214
2023-04-26 13:25:35,162 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 459
2023-04-26 13:25:35,162 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 281
2023-04-26 13:25:35,162 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 449
2023-04-26 13:25:35,163 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 421
2023-04-26 13:25:35,163 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 158
2023-04-26 13:25:35,163 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 228
2023-04-26 13:25:35,163 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 375
2023-04-26 13:25:35,163 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 435
2023-04-26 13:25:35,163 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 370
2023-04-26 13:25:35,163 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 362
2023-04-26 13:25:35,163 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 404
2023-04-26 13:25:35,163 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 216
2023-04-26 13:25:35,164 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 246
2023-04-26 13:25:35,164 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 328
2023-04-26 13:25:35,164 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 462
2023-04-26 13:25:35,164 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 469
2023-04-26 13:25:35,164 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 251
2023-04-26 13:25:35,164 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 299
2023-04-26 13:25:35,164 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 381
2023-04-26 13:25:35,164 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 182
2023-04-26 13:25:35,164 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 273
2023-04-26 13:25:35,165 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 292
2023-04-26 13:25:35,165 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 365
2023-04-26 13:25:35,165 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 419
2023-04-26 13:25:35,172 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-1] Removed broadcast_16_piece0 on 192.168.1.125:44401 in memory (size: 20.7 KB, free: 998.4 MB)
2023-04-26 13:25:35,176 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 262
2023-04-26 13:25:35,176 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 249
2023-04-26 13:25:35,176 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 374
2023-04-26 13:25:35,176 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 302
2023-04-26 13:25:35,176 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 252
2023-04-26 13:25:35,177 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 177
2023-04-26 13:25:35,178 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 296
2023-04-26 13:25:35,178 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 196
2023-04-26 13:25:35,178 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 414
2023-04-26 13:25:35,178 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 291
2023-04-26 13:25:35,178 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 323
2023-04-26 13:25:35,178 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 263
2023-04-26 13:25:35,178 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 284
2023-04-26 13:25:35,178 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 390
2023-04-26 13:25:35,178 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 383
2023-04-26 13:25:35,179 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 270
2023-04-26 13:25:35,179 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 226
2023-04-26 13:25:35,179 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 343
2023-04-26 13:25:35,179 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 348
2023-04-26 13:25:35,179 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 465
2023-04-26 13:25:35,179 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 410
2023-04-26 13:25:35,179 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 172
2023-04-26 13:25:35,179 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 367
2023-04-26 13:25:35,179 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 259
2023-04-26 13:25:35,180 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 288
2023-04-26 13:25:35,180 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 426
2023-04-26 13:25:35,180 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 264
2023-04-26 13:25:35,180 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 361
2023-04-26 13:25:35,180 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 428
2023-04-26 13:25:35,180 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 454
2023-04-26 13:25:35,180 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 432
2023-04-26 13:25:35,180 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 382
2023-04-26 13:25:35,180 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 181
2023-04-26 13:25:35,182 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-0] Removed broadcast_11_piece0 on 192.168.1.125:44401 in memory (size: 6.3 KB, free: 998.4 MB)
2023-04-26 13:25:35,184 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 224
2023-04-26 13:25:35,184 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 187
2023-04-26 13:25:35,184 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 376
2023-04-26 13:25:35,184 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 441
2023-04-26 13:25:35,184 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 200
2023-04-26 13:25:35,185 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-3] Removed broadcast_19_piece0 on 192.168.1.125:44401 in memory (size: 3.6 KB, free: 998.4 MB)
2023-04-26 13:25:35,187 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 326
2023-04-26 13:25:35,188 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 401
2023-04-26 13:25:35,188 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 238
2023-04-26 13:25:35,188 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 203
2023-04-26 13:25:35,188 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 280
2023-04-26 13:25:35,188 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 275
2023-04-26 13:25:35,189 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 298
2023-04-26 13:25:35,706 INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy [http-nio-8080-exec-1] Pruning directories with: 
2023-04-26 13:25:35,706 INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy [http-nio-8080-exec-1] Post-Scan Filters: 
2023-04-26 13:25:35,708 INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy [http-nio-8080-exec-1] Output Data Schema: struct<Name: string, " ""Team""": string, " ""Position""": string, " ""Height(inches)""": int, " ""Weight(lbs)""": string ... 1 more field>
2023-04-26 13:25:35,709 INFO org.apache.spark.sql.execution.FileSourceScanExec [http-nio-8080-exec-1] Pushed Filters: 
2023-04-26 13:25:35,719 INFO org.apache.spark.storage.memory.MemoryStore [http-nio-8080-exec-1] Block broadcast_32 stored as values in memory (estimated size 167.8 KB, free 998.0 MB)
2023-04-26 13:25:35,734 INFO org.apache.spark.storage.memory.MemoryStore [http-nio-8080-exec-1] Block broadcast_32_piece0 stored as bytes in memory (estimated size 20.9 KB, free 998.0 MB)
2023-04-26 13:25:35,735 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-0] Added broadcast_32_piece0 in memory on 192.168.1.125:44401 (size: 20.9 KB, free: 998.4 MB)
2023-04-26 13:25:35,736 INFO org.apache.spark.SparkContext [http-nio-8080-exec-1] Created broadcast 32 from jdbc at SparkController.java:142
2023-04-26 13:25:35,737 INFO org.apache.spark.sql.execution.FileSourceScanExec [http-nio-8080-exec-1] Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2023-04-26 13:25:35,759 INFO org.apache.spark.SparkContext [http-nio-8080-exec-1] Starting job: jdbc at SparkController.java:142
2023-04-26 13:25:35,761 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Got job 17 (jdbc at SparkController.java:142) with 1 output partitions
2023-04-26 13:25:35,761 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Final stage: ResultStage 17 (jdbc at SparkController.java:142)
2023-04-26 13:25:35,761 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Parents of final stage: List()
2023-04-26 13:25:35,762 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Missing parents: List()
2023-04-26 13:25:35,762 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Submitting ResultStage 17 (MapPartitionsRDD[80] at jdbc at SparkController.java:142), which has no missing parents
2023-04-26 13:25:35,767 INFO org.apache.spark.storage.memory.MemoryStore [dag-scheduler-event-loop] Block broadcast_33 stored as values in memory (estimated size 16.0 KB, free 998.0 MB)
2023-04-26 13:25:35,769 INFO org.apache.spark.storage.memory.MemoryStore [dag-scheduler-event-loop] Block broadcast_33_piece0 stored as bytes in memory (estimated size 8.6 KB, free 998.0 MB)
2023-04-26 13:25:35,770 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-1] Added broadcast_33_piece0 in memory on 192.168.1.125:44401 (size: 8.6 KB, free: 998.3 MB)
2023-04-26 13:25:35,771 INFO org.apache.spark.SparkContext [dag-scheduler-event-loop] Created broadcast 33 from broadcast at DAGScheduler.scala:1163
2023-04-26 13:25:35,772 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[80] at jdbc at SparkController.java:142) (first 15 tasks are for partitions Vector(0))
2023-04-26 13:25:35,772 INFO org.apache.spark.scheduler.TaskSchedulerImpl [dag-scheduler-event-loop] Adding task set 17.0 with 1 tasks
2023-04-26 13:25:35,775 INFO org.apache.spark.scheduler.TaskSetManager [dispatcher-event-loop-2] Starting task 0.0 in stage 17.0 (TID 18, localhost, executor driver, partition 0, PROCESS_LOCAL, 8268 bytes)
2023-04-26 13:25:35,776 INFO org.apache.spark.executor.Executor [Executor task launch worker for task 18] Running task 0.0 in stage 17.0 (TID 18)
2023-04-26 13:25:35,804 INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [Executor task launch worker for task 18] Code generated in 14.945686 ms
2023-04-26 13:25:35,870 INFO org.apache.spark.sql.execution.datasources.FileScanRDD [Executor task launch worker for task 18] Reading File path: file:///home/inferyx/Documents/Files/mlb_players.tsv, range: 0-61049, partition values: [empty row]
2023-04-26 13:25:39,630 INFO org.apache.spark.executor.Executor [Executor task launch worker for task 18] Finished task 0.0 in stage 17.0 (TID 18). 1394 bytes result sent to driver
2023-04-26 13:25:39,631 INFO org.apache.spark.scheduler.TaskSetManager [task-result-getter-2] Finished task 0.0 in stage 17.0 (TID 18) in 3856 ms on localhost (executor driver) (1/1)
2023-04-26 13:25:39,632 INFO org.apache.spark.scheduler.TaskSchedulerImpl [task-result-getter-2] Removed TaskSet 17.0, whose tasks have all completed, from pool 
2023-04-26 13:25:39,633 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] ResultStage 17 (jdbc at SparkController.java:142) finished in 3.869 s
2023-04-26 13:25:39,634 INFO org.apache.spark.scheduler.DAGScheduler [http-nio-8080-exec-1] Job 17 finished: jdbc at SparkController.java:142, took 3.874018 s
2023-04-26 13:25:39,814 INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy [http-nio-8080-exec-1] Pruning directories with: 
2023-04-26 13:25:39,815 INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy [http-nio-8080-exec-1] Post-Scan Filters: 
2023-04-26 13:25:39,816 INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy [http-nio-8080-exec-1] Output Data Schema: struct<Name: string, " ""Team""": string, " ""Position""": string, " ""Height(inches)""": int, " ""Weight(lbs)""": string ... 1 more field>
2023-04-26 13:25:39,816 INFO org.apache.spark.sql.execution.FileSourceScanExec [http-nio-8080-exec-1] Pushed Filters: 
2023-04-26 13:25:39,853 INFO org.apache.spark.storage.memory.MemoryStore [http-nio-8080-exec-1] Block broadcast_34 stored as values in memory (estimated size 167.8 KB, free 997.8 MB)
2023-04-26 13:25:39,871 INFO org.apache.spark.storage.memory.MemoryStore [http-nio-8080-exec-1] Block broadcast_34_piece0 stored as bytes in memory (estimated size 20.9 KB, free 997.8 MB)
2023-04-26 13:25:39,872 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-2] Added broadcast_34_piece0 in memory on 192.168.1.125:44401 (size: 20.9 KB, free: 998.3 MB)
2023-04-26 13:25:39,874 INFO org.apache.spark.SparkContext [http-nio-8080-exec-1] Created broadcast 34 from show at SparkController.java:144
2023-04-26 13:25:39,875 INFO org.apache.spark.sql.execution.FileSourceScanExec [http-nio-8080-exec-1] Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2023-04-26 13:25:39,891 INFO org.apache.spark.SparkContext [http-nio-8080-exec-1] Starting job: show at SparkController.java:144
2023-04-26 13:25:39,893 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Got job 18 (show at SparkController.java:144) with 1 output partitions
2023-04-26 13:25:39,893 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Final stage: ResultStage 18 (show at SparkController.java:144)
2023-04-26 13:25:39,893 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Parents of final stage: List()
2023-04-26 13:25:39,893 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Missing parents: List()
2023-04-26 13:25:39,894 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Submitting ResultStage 18 (MapPartitionsRDD[86] at show at SparkController.java:144), which has no missing parents
2023-04-26 13:25:39,896 INFO org.apache.spark.storage.memory.MemoryStore [dag-scheduler-event-loop] Block broadcast_35 stored as values in memory (estimated size 12.8 KB, free 997.8 MB)
2023-04-26 13:25:39,898 INFO org.apache.spark.storage.memory.MemoryStore [dag-scheduler-event-loop] Block broadcast_35_piece0 stored as bytes in memory (estimated size 6.5 KB, free 997.8 MB)
2023-04-26 13:25:39,899 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-3] Added broadcast_35_piece0 in memory on 192.168.1.125:44401 (size: 6.5 KB, free: 998.3 MB)
2023-04-26 13:25:39,901 INFO org.apache.spark.SparkContext [dag-scheduler-event-loop] Created broadcast 35 from broadcast at DAGScheduler.scala:1163
2023-04-26 13:25:39,902 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[86] at show at SparkController.java:144) (first 15 tasks are for partitions Vector(0))
2023-04-26 13:25:39,903 INFO org.apache.spark.scheduler.TaskSchedulerImpl [dag-scheduler-event-loop] Adding task set 18.0 with 1 tasks
2023-04-26 13:25:39,905 INFO org.apache.spark.scheduler.TaskSetManager [dispatcher-event-loop-0] Starting task 0.0 in stage 18.0 (TID 19, localhost, executor driver, partition 0, PROCESS_LOCAL, 8268 bytes)
2023-04-26 13:25:39,906 INFO org.apache.spark.executor.Executor [Executor task launch worker for task 19] Running task 0.0 in stage 18.0 (TID 19)
2023-04-26 13:25:39,913 INFO org.apache.spark.sql.execution.datasources.FileScanRDD [Executor task launch worker for task 19] Reading File path: file:///home/inferyx/Documents/Files/mlb_players.tsv, range: 0-61049, partition values: [empty row]
2023-04-26 13:25:39,924 INFO org.apache.spark.executor.Executor [Executor task launch worker for task 19] Finished task 0.0 in stage 18.0 (TID 19). 2262 bytes result sent to driver
2023-04-26 13:25:39,925 INFO org.apache.spark.scheduler.TaskSetManager [task-result-getter-3] Finished task 0.0 in stage 18.0 (TID 19) in 20 ms on localhost (executor driver) (1/1)
2023-04-26 13:25:39,926 INFO org.apache.spark.scheduler.TaskSchedulerImpl [task-result-getter-3] Removed TaskSet 18.0, whose tasks have all completed, from pool 
2023-04-26 13:25:39,927 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] ResultStage 18 (show at SparkController.java:144) finished in 0.032 s
2023-04-26 13:25:39,929 INFO org.apache.spark.scheduler.DAGScheduler [http-nio-8080-exec-1] Job 18 finished: show at SparkController.java:144, took 0.036507 s
2023-04-26 13:25:39,931 INFO com.example.spring.jwt.mongodb.controllers.SparkController [http-nio-8080-exec-1] writting Tsv File
2023-04-26 13:25:40,196 INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy [http-nio-8080-exec-1] Pruning directories with: 
2023-04-26 13:25:40,197 INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy [http-nio-8080-exec-1] Post-Scan Filters: 
2023-04-26 13:25:40,198 INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy [http-nio-8080-exec-1] Output Data Schema: struct<address: struct<city: string, postalCode: string, state: string, streetAddress: string ... 2 more fields>, age: bigint, firstName: string, gender: string, lastName: string ... 1 more field>
2023-04-26 13:25:40,198 INFO org.apache.spark.sql.execution.FileSourceScanExec [http-nio-8080-exec-1] Pushed Filters: 
2023-04-26 13:25:40,235 INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [http-nio-8080-exec-1] Code generated in 9.08484 ms
2023-04-26 13:25:40,285 INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [http-nio-8080-exec-1] Code generated in 35.351487 ms
2023-04-26 13:25:40,294 INFO org.apache.spark.storage.memory.MemoryStore [http-nio-8080-exec-1] Block broadcast_36 stored as values in memory (estimated size 167.5 KB, free 997.6 MB)
2023-04-26 13:25:40,308 INFO org.apache.spark.storage.memory.MemoryStore [http-nio-8080-exec-1] Block broadcast_36_piece0 stored as bytes in memory (estimated size 20.9 KB, free 997.6 MB)
2023-04-26 13:25:40,309 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-3] Added broadcast_36_piece0 in memory on 192.168.1.125:44401 (size: 20.9 KB, free: 998.3 MB)
2023-04-26 13:25:40,311 INFO org.apache.spark.SparkContext [http-nio-8080-exec-1] Created broadcast 36 from show at SparkController.java:155
2023-04-26 13:25:40,312 INFO org.apache.spark.sql.execution.FileSourceScanExec [http-nio-8080-exec-1] Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2023-04-26 13:25:40,340 INFO org.apache.spark.SparkContext [http-nio-8080-exec-1] Starting job: show at SparkController.java:155
2023-04-26 13:25:40,342 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Got job 19 (show at SparkController.java:155) with 1 output partitions
2023-04-26 13:25:40,342 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Final stage: ResultStage 19 (show at SparkController.java:155)
2023-04-26 13:25:40,342 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Parents of final stage: List()
2023-04-26 13:25:40,343 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Missing parents: List()
2023-04-26 13:25:40,343 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Submitting ResultStage 19 (MapPartitionsRDD[93] at show at SparkController.java:155), which has no missing parents
2023-04-26 13:25:40,417 INFO org.apache.spark.storage.memory.MemoryStore [dag-scheduler-event-loop] Block broadcast_37 stored as values in memory (estimated size 27.6 KB, free 997.6 MB)
2023-04-26 13:25:40,419 INFO org.apache.spark.storage.memory.MemoryStore [dag-scheduler-event-loop] Block broadcast_37_piece0 stored as bytes in memory (estimated size 12.1 KB, free 997.6 MB)
2023-04-26 13:25:40,420 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-0] Added broadcast_37_piece0 in memory on 192.168.1.125:44401 (size: 12.1 KB, free: 998.3 MB)
2023-04-26 13:25:40,420 INFO org.apache.spark.SparkContext [dag-scheduler-event-loop] Created broadcast 37 from broadcast at DAGScheduler.scala:1163
2023-04-26 13:25:40,423 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Submitting 1 missing tasks from ResultStage 19 (MapPartitionsRDD[93] at show at SparkController.java:155) (first 15 tasks are for partitions Vector(0))
2023-04-26 13:25:40,423 INFO org.apache.spark.scheduler.TaskSchedulerImpl [dag-scheduler-event-loop] Adding task set 19.0 with 1 tasks
2023-04-26 13:25:40,424 INFO org.apache.spark.scheduler.TaskSetManager [dispatcher-event-loop-1] Starting task 0.0 in stage 19.0 (TID 20, localhost, executor driver, partition 0, PROCESS_LOCAL, 8264 bytes)
2023-04-26 13:25:40,425 INFO org.apache.spark.executor.Executor [Executor task launch worker for task 20] Running task 0.0 in stage 19.0 (TID 20)
2023-04-26 13:25:40,530 INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [Executor task launch worker for task 20] Code generated in 53.704476 ms
2023-04-26 13:25:40,552 INFO org.apache.spark.sql.execution.datasources.FileScanRDD [Executor task launch worker for task 20] Reading File path: file:///home/inferyx/Documents/Files/sample.json, range: 0-308, partition values: [empty row]
2023-04-26 13:25:40,712 INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [Executor task launch worker for task 20] Code generated in 99.943604 ms
2023-04-26 13:25:40,723 INFO org.apache.spark.executor.Executor [Executor task launch worker for task 20] Finished task 0.0 in stage 19.0 (TID 20). 1770 bytes result sent to driver
2023-04-26 13:25:40,724 INFO org.apache.spark.scheduler.TaskSetManager [task-result-getter-0] Finished task 0.0 in stage 19.0 (TID 20) in 300 ms on localhost (executor driver) (1/1)
2023-04-26 13:25:40,724 INFO org.apache.spark.scheduler.TaskSchedulerImpl [task-result-getter-0] Removed TaskSet 19.0, whose tasks have all completed, from pool 
2023-04-26 13:25:40,726 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] ResultStage 19 (show at SparkController.java:155) finished in 0.382 s
2023-04-26 13:25:40,727 INFO org.apache.spark.scheduler.DAGScheduler [http-nio-8080-exec-1] Job 19 finished: show at SparkController.java:155, took 0.385293 s
2023-04-26 13:25:40,978 INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy [http-nio-8080-exec-1] Pruning directories with: 
2023-04-26 13:25:40,979 INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy [http-nio-8080-exec-1] Post-Scan Filters: 
2023-04-26 13:25:40,980 INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy [http-nio-8080-exec-1] Output Data Schema: struct<address: struct<city: string, postalCode: string, state: string, streetAddress: string ... 2 more fields>, age: bigint, firstName: string, gender: string, lastName: string ... 1 more field>
2023-04-26 13:25:40,981 INFO org.apache.spark.sql.execution.FileSourceScanExec [http-nio-8080-exec-1] Pushed Filters: 
2023-04-26 13:25:40,995 INFO org.apache.spark.storage.memory.MemoryStore [http-nio-8080-exec-1] Block broadcast_38 stored as values in memory (estimated size 167.5 KB, free 997.4 MB)
2023-04-26 13:25:41,013 INFO org.apache.spark.storage.memory.MemoryStore [http-nio-8080-exec-1] Block broadcast_38_piece0 stored as bytes in memory (estimated size 20.9 KB, free 997.4 MB)
2023-04-26 13:25:41,014 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-0] Added broadcast_38_piece0 in memory on 192.168.1.125:44401 (size: 20.9 KB, free: 998.3 MB)
2023-04-26 13:25:41,016 INFO org.apache.spark.SparkContext [http-nio-8080-exec-1] Created broadcast 38 from jdbc at SparkController.java:160
2023-04-26 13:25:41,017 INFO org.apache.spark.sql.execution.FileSourceScanExec [http-nio-8080-exec-1] Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2023-04-26 13:25:41,053 INFO org.apache.spark.SparkContext [http-nio-8080-exec-1] Starting job: jdbc at SparkController.java:160
2023-04-26 13:25:41,054 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Got job 20 (jdbc at SparkController.java:160) with 1 output partitions
2023-04-26 13:25:41,055 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Final stage: ResultStage 20 (jdbc at SparkController.java:160)
2023-04-26 13:25:41,055 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Parents of final stage: List()
2023-04-26 13:25:41,055 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Missing parents: List()
2023-04-26 13:25:41,057 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Submitting ResultStage 20 (MapPartitionsRDD[101] at jdbc at SparkController.java:160), which has no missing parents
2023-04-26 13:25:41,064 INFO org.apache.spark.storage.memory.MemoryStore [dag-scheduler-event-loop] Block broadcast_39 stored as values in memory (estimated size 30.0 KB, free 997.3 MB)
2023-04-26 13:25:41,067 INFO org.apache.spark.storage.memory.MemoryStore [dag-scheduler-event-loop] Block broadcast_39_piece0 stored as bytes in memory (estimated size 13.5 KB, free 997.3 MB)
2023-04-26 13:25:41,068 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-1] Added broadcast_39_piece0 in memory on 192.168.1.125:44401 (size: 13.5 KB, free: 998.3 MB)
2023-04-26 13:25:41,069 INFO org.apache.spark.SparkContext [dag-scheduler-event-loop] Created broadcast 39 from broadcast at DAGScheduler.scala:1163
2023-04-26 13:25:41,070 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Submitting 1 missing tasks from ResultStage 20 (MapPartitionsRDD[101] at jdbc at SparkController.java:160) (first 15 tasks are for partitions Vector(0))
2023-04-26 13:25:41,070 INFO org.apache.spark.scheduler.TaskSchedulerImpl [dag-scheduler-event-loop] Adding task set 20.0 with 1 tasks
2023-04-26 13:25:41,073 INFO org.apache.spark.scheduler.TaskSetManager [dispatcher-event-loop-2] Starting task 0.0 in stage 20.0 (TID 21, localhost, executor driver, partition 0, PROCESS_LOCAL, 8264 bytes)
2023-04-26 13:25:41,074 INFO org.apache.spark.executor.Executor [Executor task launch worker for task 21] Running task 0.0 in stage 20.0 (TID 21)
2023-04-26 13:25:41,134 INFO org.apache.spark.sql.execution.datasources.FileScanRDD [Executor task launch worker for task 21] Reading File path: file:///home/inferyx/Documents/Files/sample.json, range: 0-308, partition values: [empty row]
2023-04-26 13:25:41,349 INFO org.apache.spark.executor.Executor [Executor task launch worker for task 21] Finished task 0.0 in stage 20.0 (TID 21). 1484 bytes result sent to driver
2023-04-26 13:25:41,350 INFO org.apache.spark.scheduler.TaskSetManager [task-result-getter-1] Finished task 0.0 in stage 20.0 (TID 21) in 278 ms on localhost (executor driver) (1/1)
2023-04-26 13:25:41,350 INFO org.apache.spark.scheduler.TaskSchedulerImpl [task-result-getter-1] Removed TaskSet 20.0, whose tasks have all completed, from pool 
2023-04-26 13:25:41,351 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] ResultStage 20 (jdbc at SparkController.java:160) finished in 0.293 s
2023-04-26 13:25:41,352 INFO org.apache.spark.scheduler.DAGScheduler [http-nio-8080-exec-1] Job 20 finished: jdbc at SparkController.java:160, took 0.298232 s
2023-04-26 13:25:41,562 INFO org.apache.spark.SparkContext [http-nio-8080-exec-1] Starting job: show at SparkController.java:162
2023-04-26 13:25:41,563 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Got job 21 (show at SparkController.java:162) with 1 output partitions
2023-04-26 13:25:41,563 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Final stage: ResultStage 21 (show at SparkController.java:162)
2023-04-26 13:25:41,564 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Parents of final stage: List()
2023-04-26 13:25:41,564 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Missing parents: List()
2023-04-26 13:25:41,564 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Submitting ResultStage 21 (MapPartitionsRDD[108] at show at SparkController.java:162), which has no missing parents
2023-04-26 13:25:41,567 INFO org.apache.spark.storage.memory.MemoryStore [dag-scheduler-event-loop] Block broadcast_40 stored as values in memory (estimated size 7.5 KB, free 997.3 MB)
2023-04-26 13:25:41,568 INFO org.apache.spark.storage.memory.MemoryStore [dag-scheduler-event-loop] Block broadcast_40_piece0 stored as bytes in memory (estimated size 3.5 KB, free 997.3 MB)
2023-04-26 13:25:41,569 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-1] Added broadcast_40_piece0 in memory on 192.168.1.125:44401 (size: 3.5 KB, free: 998.2 MB)
2023-04-26 13:25:41,570 INFO org.apache.spark.SparkContext [dag-scheduler-event-loop] Created broadcast 40 from broadcast at DAGScheduler.scala:1163
2023-04-26 13:25:41,571 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[108] at show at SparkController.java:162) (first 15 tasks are for partitions Vector(0))
2023-04-26 13:25:41,572 INFO org.apache.spark.scheduler.TaskSchedulerImpl [dag-scheduler-event-loop] Adding task set 21.0 with 1 tasks
2023-04-26 13:25:41,573 INFO org.apache.spark.scheduler.TaskSetManager [dispatcher-event-loop-2] Starting task 0.0 in stage 21.0 (TID 22, localhost, executor driver, partition 0, PROCESS_LOCAL, 8705 bytes)
2023-04-26 13:25:41,574 INFO org.apache.spark.executor.Executor [Executor task launch worker for task 22] Running task 0.0 in stage 21.0 (TID 22)
2023-04-26 13:25:41,580 INFO org.apache.spark.executor.Executor [Executor task launch worker for task 22] Finished task 0.0 in stage 21.0 (TID 22). 1841 bytes result sent to driver
2023-04-26 13:25:41,581 INFO org.apache.spark.scheduler.TaskSetManager [task-result-getter-2] Finished task 0.0 in stage 21.0 (TID 22) in 8 ms on localhost (executor driver) (1/1)
2023-04-26 13:25:41,582 INFO org.apache.spark.scheduler.TaskSchedulerImpl [task-result-getter-2] Removed TaskSet 21.0, whose tasks have all completed, from pool 
2023-04-26 13:25:41,583 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] ResultStage 21 (show at SparkController.java:162) finished in 0.016 s
2023-04-26 13:25:41,584 INFO org.apache.spark.scheduler.DAGScheduler [http-nio-8080-exec-1] Job 21 finished: show at SparkController.java:162, took 0.021528 s
2023-04-26 13:25:41,588 INFO org.apache.spark.SparkContext [http-nio-8080-exec-1] Starting job: show at SparkController.java:162
2023-04-26 13:25:41,590 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Got job 22 (show at SparkController.java:162) with 2 output partitions
2023-04-26 13:25:41,590 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Final stage: ResultStage 22 (show at SparkController.java:162)
2023-04-26 13:25:41,590 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Parents of final stage: List()
2023-04-26 13:25:41,591 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Missing parents: List()
2023-04-26 13:25:41,592 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Submitting ResultStage 22 (MapPartitionsRDD[108] at show at SparkController.java:162), which has no missing parents
2023-04-26 13:25:41,594 INFO org.apache.spark.storage.memory.MemoryStore [dag-scheduler-event-loop] Block broadcast_41 stored as values in memory (estimated size 7.5 KB, free 997.3 MB)
2023-04-26 13:25:41,596 INFO org.apache.spark.storage.memory.MemoryStore [dag-scheduler-event-loop] Block broadcast_41_piece0 stored as bytes in memory (estimated size 3.5 KB, free 997.3 MB)
2023-04-26 13:25:41,597 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-1] Added broadcast_41_piece0 in memory on 192.168.1.125:44401 (size: 3.5 KB, free: 998.2 MB)
2023-04-26 13:25:41,597 INFO org.apache.spark.SparkContext [dag-scheduler-event-loop] Created broadcast 41 from broadcast at DAGScheduler.scala:1163
2023-04-26 13:25:41,598 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Submitting 2 missing tasks from ResultStage 22 (MapPartitionsRDD[108] at show at SparkController.java:162) (first 15 tasks are for partitions Vector(1, 2))
2023-04-26 13:25:41,598 INFO org.apache.spark.scheduler.TaskSchedulerImpl [dag-scheduler-event-loop] Adding task set 22.0 with 2 tasks
2023-04-26 13:25:41,600 INFO org.apache.spark.scheduler.TaskSetManager [dispatcher-event-loop-2] Starting task 0.0 in stage 22.0 (TID 23, localhost, executor driver, partition 1, PROCESS_LOCAL, 8334 bytes)
2023-04-26 13:25:41,600 INFO org.apache.spark.scheduler.TaskSetManager [dispatcher-event-loop-2] Starting task 1.0 in stage 22.0 (TID 24, localhost, executor driver, partition 2, PROCESS_LOCAL, 8271 bytes)
2023-04-26 13:25:41,601 INFO org.apache.spark.executor.Executor [Executor task launch worker for task 23] Running task 0.0 in stage 22.0 (TID 23)
2023-04-26 13:25:41,601 INFO org.apache.spark.executor.Executor [Executor task launch worker for task 24] Running task 1.0 in stage 22.0 (TID 24)
2023-04-26 13:25:41,605 INFO org.apache.spark.executor.Executor [Executor task launch worker for task 23] Finished task 0.0 in stage 22.0 (TID 23). 1186 bytes result sent to driver
2023-04-26 13:25:41,605 INFO org.apache.spark.executor.Executor [Executor task launch worker for task 24] Finished task 1.0 in stage 22.0 (TID 24). 1112 bytes result sent to driver
2023-04-26 13:25:41,606 INFO org.apache.spark.scheduler.TaskSetManager [task-result-getter-3] Finished task 0.0 in stage 22.0 (TID 23) in 7 ms on localhost (executor driver) (1/2)
2023-04-26 13:25:41,607 INFO org.apache.spark.scheduler.TaskSetManager [task-result-getter-0] Finished task 1.0 in stage 22.0 (TID 24) in 7 ms on localhost (executor driver) (2/2)
2023-04-26 13:25:41,607 INFO org.apache.spark.scheduler.TaskSchedulerImpl [task-result-getter-0] Removed TaskSet 22.0, whose tasks have all completed, from pool 
2023-04-26 13:25:41,609 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] ResultStage 22 (show at SparkController.java:162) finished in 0.016 s
2023-04-26 13:25:41,610 INFO org.apache.spark.scheduler.DAGScheduler [http-nio-8080-exec-1] Job 22 finished: show at SparkController.java:162, took 0.021797 s
2023-04-26 13:25:41,612 INFO com.example.spring.jwt.mongodb.controllers.SparkController [http-nio-8080-exec-1] writting Json File
2023-04-26 13:25:42,061 INFO org.apache.spark.SparkContext [http-nio-8080-exec-1] Starting job: jdbc at SparkController.java:170
2023-04-26 13:25:42,063 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Got job 23 (jdbc at SparkController.java:170) with 4 output partitions
2023-04-26 13:25:42,063 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Final stage: ResultStage 23 (jdbc at SparkController.java:170)
2023-04-26 13:25:42,063 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Parents of final stage: List()
2023-04-26 13:25:42,063 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Missing parents: List()
2023-04-26 13:25:42,064 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Submitting ResultStage 23 (MapPartitionsRDD[114] at jdbc at SparkController.java:170), which has no missing parents
2023-04-26 13:25:42,074 INFO org.apache.spark.storage.memory.MemoryStore [dag-scheduler-event-loop] Block broadcast_42 stored as values in memory (estimated size 13.9 KB, free 997.3 MB)
2023-04-26 13:25:42,076 INFO org.apache.spark.storage.memory.MemoryStore [dag-scheduler-event-loop] Block broadcast_42_piece0 stored as bytes in memory (estimated size 6.8 KB, free 997.3 MB)
2023-04-26 13:25:42,077 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-3] Added broadcast_42_piece0 in memory on 192.168.1.125:44401 (size: 6.8 KB, free: 998.2 MB)
2023-04-26 13:25:42,078 INFO org.apache.spark.SparkContext [dag-scheduler-event-loop] Created broadcast 42 from broadcast at DAGScheduler.scala:1163
2023-04-26 13:25:42,080 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Submitting 4 missing tasks from ResultStage 23 (MapPartitionsRDD[114] at jdbc at SparkController.java:170) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
2023-04-26 13:25:42,080 INFO org.apache.spark.scheduler.TaskSchedulerImpl [dag-scheduler-event-loop] Adding task set 23.0 with 4 tasks
2023-04-26 13:25:42,081 INFO org.apache.spark.scheduler.TaskSetManager [dispatcher-event-loop-0] Starting task 0.0 in stage 23.0 (TID 25, localhost, executor driver, partition 0, PROCESS_LOCAL, 8705 bytes)
2023-04-26 13:25:42,082 INFO org.apache.spark.scheduler.TaskSetManager [dispatcher-event-loop-0] Starting task 1.0 in stage 23.0 (TID 26, localhost, executor driver, partition 1, PROCESS_LOCAL, 8334 bytes)
2023-04-26 13:25:42,082 INFO org.apache.spark.scheduler.TaskSetManager [dispatcher-event-loop-0] Starting task 2.0 in stage 23.0 (TID 27, localhost, executor driver, partition 2, PROCESS_LOCAL, 8271 bytes)
2023-04-26 13:25:42,083 INFO org.apache.spark.scheduler.TaskSetManager [dispatcher-event-loop-0] Starting task 3.0 in stage 23.0 (TID 28, localhost, executor driver, partition 3, PROCESS_LOCAL, 8295 bytes)
2023-04-26 13:25:42,083 INFO org.apache.spark.executor.Executor [Executor task launch worker for task 25] Running task 0.0 in stage 23.0 (TID 25)
2023-04-26 13:25:42,083 INFO org.apache.spark.executor.Executor [Executor task launch worker for task 26] Running task 1.0 in stage 23.0 (TID 26)
2023-04-26 13:25:42,085 INFO org.apache.spark.executor.Executor [Executor task launch worker for task 27] Running task 2.0 in stage 23.0 (TID 27)
2023-04-26 13:25:42,101 INFO org.apache.spark.executor.Executor [Executor task launch worker for task 28] Running task 3.0 in stage 23.0 (TID 28)
2023-04-26 13:25:42,573 INFO org.apache.spark.executor.Executor [Executor task launch worker for task 26] Finished task 1.0 in stage 23.0 (TID 26). 1064 bytes result sent to driver
2023-04-26 13:25:42,574 INFO org.apache.spark.scheduler.TaskSetManager [task-result-getter-1] Finished task 1.0 in stage 23.0 (TID 26) in 492 ms on localhost (executor driver) (1/4)
2023-04-26 13:25:42,741 INFO org.apache.spark.executor.Executor [Executor task launch worker for task 28] Finished task 3.0 in stage 23.0 (TID 28). 1064 bytes result sent to driver
2023-04-26 13:25:42,741 INFO org.apache.spark.executor.Executor [Executor task launch worker for task 27] Finished task 2.0 in stage 23.0 (TID 27). 1064 bytes result sent to driver
2023-04-26 13:25:42,743 INFO org.apache.spark.scheduler.TaskSetManager [task-result-getter-2] Finished task 3.0 in stage 23.0 (TID 28) in 660 ms on localhost (executor driver) (2/4)
2023-04-26 13:25:42,743 INFO org.apache.spark.scheduler.TaskSetManager [task-result-getter-3] Finished task 2.0 in stage 23.0 (TID 27) in 661 ms on localhost (executor driver) (3/4)
2023-04-26 13:25:42,798 INFO org.apache.spark.executor.Executor [Executor task launch worker for task 25] Finished task 0.0 in stage 23.0 (TID 25). 1064 bytes result sent to driver
2023-04-26 13:25:42,799 INFO org.apache.spark.scheduler.TaskSetManager [task-result-getter-0] Finished task 0.0 in stage 23.0 (TID 25) in 718 ms on localhost (executor driver) (4/4)
2023-04-26 13:25:42,800 INFO org.apache.spark.scheduler.TaskSchedulerImpl [task-result-getter-0] Removed TaskSet 23.0, whose tasks have all completed, from pool 
2023-04-26 13:25:42,801 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] ResultStage 23 (jdbc at SparkController.java:170) finished in 0.734 s
2023-04-26 13:25:42,801 INFO org.apache.spark.scheduler.DAGScheduler [http-nio-8080-exec-1] Job 23 finished: jdbc at SparkController.java:170, took 0.739599 s
2023-04-26 13:25:42,969 INFO org.apache.spark.SparkContext [http-nio-8080-exec-1] Starting job: show at SparkController.java:172
2023-04-26 13:25:42,970 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Got job 24 (show at SparkController.java:172) with 1 output partitions
2023-04-26 13:25:42,971 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Final stage: ResultStage 24 (show at SparkController.java:172)
2023-04-26 13:25:42,971 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Parents of final stage: List()
2023-04-26 13:25:42,972 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Missing parents: List()
2023-04-26 13:25:42,973 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Submitting ResultStage 24 (MapPartitionsRDD[121] at show at SparkController.java:172), which has no missing parents
2023-04-26 13:25:42,976 INFO org.apache.spark.storage.memory.MemoryStore [dag-scheduler-event-loop] Block broadcast_43 stored as values in memory (estimated size 7.5 KB, free 997.3 MB)
2023-04-26 13:25:42,978 INFO org.apache.spark.storage.memory.MemoryStore [dag-scheduler-event-loop] Block broadcast_43_piece0 stored as bytes in memory (estimated size 3.5 KB, free 997.3 MB)
2023-04-26 13:25:42,979 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-2] Added broadcast_43_piece0 in memory on 192.168.1.125:44401 (size: 3.5 KB, free: 998.2 MB)
2023-04-26 13:25:42,980 INFO org.apache.spark.SparkContext [dag-scheduler-event-loop] Created broadcast 43 from broadcast at DAGScheduler.scala:1163
2023-04-26 13:25:42,980 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Submitting 1 missing tasks from ResultStage 24 (MapPartitionsRDD[121] at show at SparkController.java:172) (first 15 tasks are for partitions Vector(0))
2023-04-26 13:25:42,981 INFO org.apache.spark.scheduler.TaskSchedulerImpl [dag-scheduler-event-loop] Adding task set 24.0 with 1 tasks
2023-04-26 13:25:42,982 INFO org.apache.spark.scheduler.TaskSetManager [dispatcher-event-loop-0] Starting task 0.0 in stage 24.0 (TID 29, localhost, executor driver, partition 0, PROCESS_LOCAL, 8705 bytes)
2023-04-26 13:25:42,982 INFO org.apache.spark.executor.Executor [Executor task launch worker for task 29] Running task 0.0 in stage 24.0 (TID 29)
2023-04-26 13:25:42,987 INFO org.apache.spark.executor.Executor [Executor task launch worker for task 29] Finished task 0.0 in stage 24.0 (TID 29). 1841 bytes result sent to driver
2023-04-26 13:25:42,988 INFO org.apache.spark.scheduler.TaskSetManager [task-result-getter-1] Finished task 0.0 in stage 24.0 (TID 29) in 6 ms on localhost (executor driver) (1/1)
2023-04-26 13:25:42,989 INFO org.apache.spark.scheduler.TaskSchedulerImpl [task-result-getter-1] Removed TaskSet 24.0, whose tasks have all completed, from pool 
2023-04-26 13:25:42,994 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] ResultStage 24 (show at SparkController.java:172) finished in 0.020 s
2023-04-26 13:25:42,995 INFO org.apache.spark.scheduler.DAGScheduler [http-nio-8080-exec-1] Job 24 finished: show at SparkController.java:172, took 0.025344 s
2023-04-26 13:25:42,999 INFO org.apache.spark.SparkContext [http-nio-8080-exec-1] Starting job: show at SparkController.java:172
2023-04-26 13:25:43,001 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Got job 25 (show at SparkController.java:172) with 2 output partitions
2023-04-26 13:25:43,001 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Final stage: ResultStage 25 (show at SparkController.java:172)
2023-04-26 13:25:43,001 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Parents of final stage: List()
2023-04-26 13:25:43,002 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Missing parents: List()
2023-04-26 13:25:43,002 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Submitting ResultStage 25 (MapPartitionsRDD[121] at show at SparkController.java:172), which has no missing parents
2023-04-26 13:25:43,005 INFO org.apache.spark.storage.memory.MemoryStore [dag-scheduler-event-loop] Block broadcast_44 stored as values in memory (estimated size 7.5 KB, free 997.3 MB)
2023-04-26 13:25:43,007 INFO org.apache.spark.storage.memory.MemoryStore [dag-scheduler-event-loop] Block broadcast_44_piece0 stored as bytes in memory (estimated size 3.5 KB, free 997.3 MB)
2023-04-26 13:25:43,008 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-2] Added broadcast_44_piece0 in memory on 192.168.1.125:44401 (size: 3.5 KB, free: 998.2 MB)
2023-04-26 13:25:43,009 INFO org.apache.spark.SparkContext [dag-scheduler-event-loop] Created broadcast 44 from broadcast at DAGScheduler.scala:1163
2023-04-26 13:25:43,010 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] Submitting 2 missing tasks from ResultStage 25 (MapPartitionsRDD[121] at show at SparkController.java:172) (first 15 tasks are for partitions Vector(1, 2))
2023-04-26 13:25:43,010 INFO org.apache.spark.scheduler.TaskSchedulerImpl [dag-scheduler-event-loop] Adding task set 25.0 with 2 tasks
2023-04-26 13:25:43,011 INFO org.apache.spark.scheduler.TaskSetManager [dispatcher-event-loop-0] Starting task 0.0 in stage 25.0 (TID 30, localhost, executor driver, partition 1, PROCESS_LOCAL, 8334 bytes)
2023-04-26 13:25:43,012 INFO org.apache.spark.scheduler.TaskSetManager [dispatcher-event-loop-0] Starting task 1.0 in stage 25.0 (TID 31, localhost, executor driver, partition 2, PROCESS_LOCAL, 8271 bytes)
2023-04-26 13:25:43,013 INFO org.apache.spark.executor.Executor [Executor task launch worker for task 30] Running task 0.0 in stage 25.0 (TID 30)
2023-04-26 13:25:43,014 INFO org.apache.spark.executor.Executor [Executor task launch worker for task 31] Running task 1.0 in stage 25.0 (TID 31)
2023-04-26 13:25:43,017 INFO org.apache.spark.executor.Executor [Executor task launch worker for task 30] Finished task 0.0 in stage 25.0 (TID 30). 1186 bytes result sent to driver
2023-04-26 13:25:43,018 INFO org.apache.spark.scheduler.TaskSetManager [task-result-getter-2] Finished task 0.0 in stage 25.0 (TID 30) in 7 ms on localhost (executor driver) (1/2)
2023-04-26 13:25:43,020 INFO org.apache.spark.executor.Executor [Executor task launch worker for task 31] Finished task 1.0 in stage 25.0 (TID 31). 1112 bytes result sent to driver
2023-04-26 13:25:43,021 INFO org.apache.spark.scheduler.TaskSetManager [task-result-getter-3] Finished task 1.0 in stage 25.0 (TID 31) in 9 ms on localhost (executor driver) (2/2)
2023-04-26 13:25:43,023 INFO org.apache.spark.scheduler.TaskSchedulerImpl [task-result-getter-3] Removed TaskSet 25.0, whose tasks have all completed, from pool 
2023-04-26 13:25:43,024 INFO org.apache.spark.scheduler.DAGScheduler [dag-scheduler-event-loop] ResultStage 25 (show at SparkController.java:172) finished in 0.021 s
2023-04-26 13:25:43,025 INFO org.apache.spark.scheduler.DAGScheduler [http-nio-8080-exec-1] Job 25 finished: show at SparkController.java:172, took 0.024543 s
2023-04-26 13:25:43,029 INFO com.example.spring.jwt.mongodb.controllers.SparkController [http-nio-8080-exec-1] writting Excel File
2023-04-26 13:34:39,820 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 659
2023-04-26 13:34:39,824 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 728
2023-04-26 13:34:39,824 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 741
2023-04-26 13:34:39,824 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 740
2023-04-26 13:34:39,824 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 478
2023-04-26 13:34:39,824 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 717
2023-04-26 13:34:39,824 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 590
2023-04-26 13:34:39,824 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 480
2023-04-26 13:34:39,825 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 675
2023-04-26 13:34:39,825 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 696
2023-04-26 13:34:39,825 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 568
2023-04-26 13:34:39,825 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 491
2023-04-26 13:34:39,825 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 513
2023-04-26 13:34:39,825 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 730
2023-04-26 13:34:39,832 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-0] Removed broadcast_34_piece0 on 192.168.1.125:44401 in memory (size: 20.9 KB, free: 998.3 MB)
2023-04-26 13:34:39,873 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 635
2023-04-26 13:34:39,874 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 576
2023-04-26 13:34:39,874 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 477
2023-04-26 13:34:39,874 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 570
2023-04-26 13:34:39,884 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-2] Removed broadcast_42_piece0 on 192.168.1.125:44401 in memory (size: 6.8 KB, free: 998.3 MB)
2023-04-26 13:34:39,886 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 693
2023-04-26 13:34:39,887 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 708
2023-04-26 13:34:39,887 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 704
2023-04-26 13:34:39,887 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 561
2023-04-26 13:34:39,887 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 560
2023-04-26 13:34:39,887 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 505
2023-04-26 13:34:39,888 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 567
2023-04-26 13:34:39,888 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 641
2023-04-26 13:34:39,888 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 674
2023-04-26 13:34:39,888 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 571
2023-04-26 13:34:39,888 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 526
2023-04-26 13:34:39,888 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 523
2023-04-26 13:34:39,888 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 592
2023-04-26 13:34:39,893 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-1] Removed broadcast_33_piece0 on 192.168.1.125:44401 in memory (size: 8.6 KB, free: 998.3 MB)
2023-04-26 13:34:39,914 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 504
2023-04-26 13:34:39,914 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 682
2023-04-26 13:34:39,914 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 688
2023-04-26 13:34:39,914 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 521
2023-04-26 13:34:39,914 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 573
2023-04-26 13:34:39,914 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 657
2023-04-26 13:34:39,915 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 631
2023-04-26 13:34:39,915 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 627
2023-04-26 13:34:39,915 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 492
2023-04-26 13:34:39,915 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 679
2023-04-26 13:34:39,915 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 606
2023-04-26 13:34:39,915 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 612
2023-04-26 13:34:39,915 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 528
2023-04-26 13:34:39,916 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 542
2023-04-26 13:34:39,916 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 667
2023-04-26 13:34:39,916 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 698
2023-04-26 13:34:39,916 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 686
2023-04-26 13:34:39,916 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 474
2023-04-26 13:34:39,916 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 725
2023-04-26 13:34:39,916 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 699
2023-04-26 13:34:39,916 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 508
2023-04-26 13:34:39,917 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 608
2023-04-26 13:34:39,917 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 707
2023-04-26 13:34:39,917 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 563
2023-04-26 13:34:39,917 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 678
2023-04-26 13:34:39,917 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 654
2023-04-26 13:34:39,919 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-3] Removed broadcast_30_piece0 on 192.168.1.125:44401 in memory (size: 20.9 KB, free: 998.3 MB)
2023-04-26 13:34:39,921 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 715
2023-04-26 13:34:39,921 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 564
2023-04-26 13:34:39,921 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 549
2023-04-26 13:34:39,921 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 533
2023-04-26 13:34:39,922 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 621
2023-04-26 13:34:39,922 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 517
2023-04-26 13:34:39,922 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 729
2023-04-26 13:34:39,922 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 643
2023-04-26 13:34:39,922 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 546
2023-04-26 13:34:39,922 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 684
2023-04-26 13:34:39,922 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 595
2023-04-26 13:34:39,922 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 518
2023-04-26 13:34:39,922 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 669
2023-04-26 13:34:39,922 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 599
2023-04-26 13:34:39,922 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 516
2023-04-26 13:34:39,922 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 637
2023-04-26 13:34:39,922 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 519
2023-04-26 13:34:39,922 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 578
2023-04-26 13:34:39,922 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 605
2023-04-26 13:34:39,923 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 531
2023-04-26 13:34:39,923 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 744
2023-04-26 13:34:39,923 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 632
2023-04-26 13:34:39,923 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 574
2023-04-26 13:34:39,923 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 603
2023-04-26 13:34:39,933 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-0] Removed broadcast_43_piece0 on 192.168.1.125:44401 in memory (size: 3.5 KB, free: 998.3 MB)
2023-04-26 13:34:39,945 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 748
2023-04-26 13:34:39,945 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 604
2023-04-26 13:34:39,946 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-1] Removed broadcast_37_piece0 on 192.168.1.125:44401 in memory (size: 12.1 KB, free: 998.3 MB)
2023-04-26 13:34:39,948 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 587
2023-04-26 13:34:39,948 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 557
2023-04-26 13:34:39,948 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 548
2023-04-26 13:34:39,948 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 470
2023-04-26 13:34:39,948 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 615
2023-04-26 13:34:39,948 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 551
2023-04-26 13:34:39,948 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 747
2023-04-26 13:34:39,948 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 701
2023-04-26 13:34:39,948 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 653
2023-04-26 13:34:39,949 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 745
2023-04-26 13:34:39,949 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 719
2023-04-26 13:34:39,949 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 645
2023-04-26 13:34:39,949 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 585
2023-04-26 13:34:39,949 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 575
2023-04-26 13:34:39,949 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 485
2023-04-26 13:34:39,949 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 673
2023-04-26 13:34:39,949 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 581
2023-04-26 13:34:39,949 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 498
2023-04-26 13:34:39,949 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 600
2023-04-26 13:34:39,949 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 547
2023-04-26 13:34:39,949 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 507
2023-04-26 13:34:39,949 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 720
2023-04-26 13:34:39,949 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 709
2023-04-26 13:34:39,950 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 593
2023-04-26 13:34:39,950 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 718
2023-04-26 13:34:39,950 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 737
2023-04-26 13:34:39,950 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 614
2023-04-26 13:34:39,950 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 749
2023-04-26 13:34:39,950 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 554
2023-04-26 13:34:39,950 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 482
2023-04-26 13:34:39,950 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 690
2023-04-26 13:34:39,950 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 691
2023-04-26 13:34:39,950 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 589
2023-04-26 13:34:39,950 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 586
2023-04-26 13:34:39,950 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 588
2023-04-26 13:34:39,950 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 509
2023-04-26 13:34:39,950 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 734
2023-04-26 13:34:39,952 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-2] Removed broadcast_39_piece0 on 192.168.1.125:44401 in memory (size: 13.5 KB, free: 998.3 MB)
2023-04-26 13:34:39,981 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 487
2023-04-26 13:34:39,981 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 726
2023-04-26 13:34:39,981 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 694
2023-04-26 13:34:39,982 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 743
2023-04-26 13:34:39,982 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 670
2023-04-26 13:34:39,982 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 479
2023-04-26 13:34:39,982 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 529
2023-04-26 13:34:39,982 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 489
2023-04-26 13:34:39,982 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 665
2023-04-26 13:34:39,982 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 619
2023-04-26 13:34:39,982 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 536
2023-04-26 13:34:39,982 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 640
2023-04-26 13:34:39,982 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 646
2023-04-26 13:34:39,982 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 565
2023-04-26 13:34:39,982 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 736
2023-04-26 13:34:39,982 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 503
2023-04-26 13:34:39,982 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 666
2023-04-26 13:34:39,982 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 697
2023-04-26 13:34:39,982 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 540
2023-04-26 13:34:39,982 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 512
2023-04-26 13:34:39,983 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 636
2023-04-26 13:34:39,983 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 739
2023-04-26 13:34:39,983 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 648
2023-04-26 13:34:39,983 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 490
2023-04-26 13:34:39,983 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 672
2023-04-26 13:34:39,983 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 702
2023-04-26 13:34:39,983 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 537
2023-04-26 13:34:39,983 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 494
2023-04-26 13:34:39,983 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 721
2023-04-26 13:34:39,984 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-3] Removed broadcast_32_piece0 on 192.168.1.125:44401 in memory (size: 20.9 KB, free: 998.3 MB)
2023-04-26 13:34:39,987 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 731
2023-04-26 13:34:39,987 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 497
2023-04-26 13:34:39,987 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 626
2023-04-26 13:34:39,987 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 655
2023-04-26 13:34:39,989 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-0] Removed broadcast_36_piece0 on 192.168.1.125:44401 in memory (size: 20.9 KB, free: 998.4 MB)
2023-04-26 13:34:39,991 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 545
2023-04-26 13:34:39,991 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 579
2023-04-26 13:34:39,991 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 488
2023-04-26 13:34:39,991 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 525
2023-04-26 13:34:39,991 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 746
2023-04-26 13:34:39,992 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 687
2023-04-26 13:34:39,992 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 661
2023-04-26 13:34:39,992 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 624
2023-04-26 13:34:39,992 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 558
2023-04-26 13:34:39,992 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 566
2023-04-26 13:34:39,992 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 597
2023-04-26 13:34:39,992 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 582
2023-04-26 13:34:39,992 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 559
2023-04-26 13:34:39,992 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 727
2023-04-26 13:34:39,992 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 530
2023-04-26 13:34:39,993 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 623
2023-04-26 13:34:39,993 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 706
2023-04-26 13:34:39,993 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 572
2023-04-26 13:34:39,993 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 555
2023-04-26 13:34:39,993 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 506
2023-04-26 13:34:39,993 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 493
2023-04-26 13:34:39,993 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 510
2023-04-26 13:34:39,993 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 703
2023-04-26 13:34:39,993 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 618
2023-04-26 13:34:39,993 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 628
2023-04-26 13:34:39,994 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 472
2023-04-26 13:34:39,994 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 722
2023-04-26 13:34:39,994 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 584
2023-04-26 13:34:39,994 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 596
2023-04-26 13:34:39,994 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 716
2023-04-26 13:34:39,994 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 622
2023-04-26 13:34:39,994 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 629
2023-04-26 13:34:39,994 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 712
2023-04-26 13:34:39,994 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 550
2023-04-26 13:34:39,995 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 583
2023-04-26 13:34:39,995 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 617
2023-04-26 13:34:40,000 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-1] Removed broadcast_38_piece0 on 192.168.1.125:44401 in memory (size: 20.9 KB, free: 998.4 MB)
2023-04-26 13:34:40,014 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 502
2023-04-26 13:34:40,014 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 677
2023-04-26 13:34:40,014 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 473
2023-04-26 13:34:40,014 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 750
2023-04-26 13:34:40,017 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-2] Removed broadcast_41_piece0 on 192.168.1.125:44401 in memory (size: 3.5 KB, free: 998.4 MB)
2023-04-26 13:34:40,019 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 651
2023-04-26 13:34:40,023 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-3] Removed broadcast_40_piece0 on 192.168.1.125:44401 in memory (size: 3.5 KB, free: 998.4 MB)
2023-04-26 13:34:40,029 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 484
2023-04-26 13:34:40,030 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 486
2023-04-26 13:34:40,030 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 639
2023-04-26 13:34:40,030 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 515
2023-04-26 13:34:40,030 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 683
2023-04-26 13:34:40,037 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-0] Removed broadcast_35_piece0 on 192.168.1.125:44401 in memory (size: 6.5 KB, free: 998.4 MB)
2023-04-26 13:34:40,048 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 724
2023-04-26 13:34:40,049 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 553
2023-04-26 13:34:40,049 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 522
2023-04-26 13:34:40,049 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 662
2023-04-26 13:34:40,049 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 520
2023-04-26 13:34:40,050 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 634
2023-04-26 13:34:40,050 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 607
2023-04-26 13:34:40,050 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 752
2023-04-26 13:34:40,050 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 501
2023-04-26 13:34:40,050 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 685
2023-04-26 13:34:40,050 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 668
2023-04-26 13:34:40,050 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 689
2023-04-26 13:34:40,050 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 475
2023-04-26 13:34:40,050 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 663
2023-04-26 13:34:40,051 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 500
2023-04-26 13:34:40,051 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 650
2023-04-26 13:34:40,051 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 695
2023-04-26 13:34:40,051 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 613
2023-04-26 13:34:40,051 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 541
2023-04-26 13:34:40,051 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 511
2023-04-26 13:34:40,051 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 609
2023-04-26 13:34:40,051 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 710
2023-04-26 13:34:40,051 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 496
2023-04-26 13:34:40,052 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 647
2023-04-26 13:34:40,052 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 591
2023-04-26 13:34:40,052 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 649
2023-04-26 13:34:40,052 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 642
2023-04-26 13:34:40,052 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 633
2023-04-26 13:34:40,053 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 692
2023-04-26 13:34:40,053 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 638
2023-04-26 13:34:40,053 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 538
2023-04-26 13:34:40,054 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 660
2023-04-26 13:34:40,054 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 562
2023-04-26 13:34:40,054 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 594
2023-04-26 13:34:40,055 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 476
2023-04-26 13:34:40,055 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 611
2023-04-26 13:34:40,055 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 532
2023-04-26 13:34:40,056 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 738
2023-04-26 13:34:40,056 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 610
2023-04-26 13:34:40,056 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 713
2023-04-26 13:34:40,057 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 598
2023-04-26 13:34:40,057 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 514
2023-04-26 13:34:40,057 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 644
2023-04-26 13:34:40,057 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 580
2023-04-26 13:34:40,058 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 711
2023-04-26 13:34:40,058 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 620
2023-04-26 13:34:40,058 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 601
2023-04-26 13:34:40,060 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 680
2023-04-26 13:34:40,060 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 652
2023-04-26 13:34:40,061 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 735
2023-04-26 13:34:40,061 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 705
2023-04-26 13:34:40,064 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-1] Removed broadcast_44_piece0 on 192.168.1.125:44401 in memory (size: 3.5 KB, free: 998.4 MB)
2023-04-26 13:34:40,069 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 556
2023-04-26 13:34:40,069 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 625
2023-04-26 13:34:40,070 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 732
2023-04-26 13:34:40,070 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 656
2023-04-26 13:34:40,070 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 751
2023-04-26 13:34:40,070 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 524
2023-04-26 13:34:40,071 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 700
2023-04-26 13:34:40,073 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 543
2023-04-26 13:34:40,074 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 527
2023-04-26 13:34:40,074 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 534
2023-04-26 13:34:40,074 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 616
2023-04-26 13:34:40,074 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 471
2023-04-26 13:34:40,074 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 723
2023-04-26 13:34:40,074 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 681
2023-04-26 13:34:40,075 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 630
2023-04-26 13:34:40,075 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 495
2023-04-26 13:34:40,075 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 499
2023-04-26 13:34:40,075 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 569
2023-04-26 13:34:40,075 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 552
2023-04-26 13:34:40,075 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 577
2023-04-26 13:34:40,076 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 539
2023-04-26 13:34:40,076 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 714
2023-04-26 13:34:40,076 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 535
2023-04-26 13:34:40,076 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 664
2023-04-26 13:34:40,076 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 742
2023-04-26 13:34:40,077 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 733
2023-04-26 13:34:40,077 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 483
2023-04-26 13:34:40,080 INFO org.apache.spark.storage.BlockManagerInfo [dispatcher-event-loop-2] Removed broadcast_31_piece0 on 192.168.1.125:44401 in memory (size: 6.3 KB, free: 998.4 MB)
2023-04-26 13:34:40,083 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 658
2023-04-26 13:34:40,083 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 481
2023-04-26 13:34:40,083 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 544
2023-04-26 13:34:40,083 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 676
2023-04-26 13:34:40,083 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 671
2023-04-26 13:34:40,083 INFO org.apache.spark.ContextCleaner [Spark Context Cleaner] Cleaned accumulator 602
